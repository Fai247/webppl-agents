var makeDonutUtility = function (world, rewards) { 
  return function(state, action) {
    var getFeature = world.feature;
    var feature = getFeature(state);

    if (feature.name) { return rewards[feature.name][state.timeAtRestaurant]; }
    return -0.01;
  };
};


var makeRiskAgent = function (agentParams, world) {
  map(function(s){assert.ok(agentParams.hasOwnProperty(s),'makeAgent args: ' + s + '; ' + JSON.stringify(agentParams));}, 
      ['utility','alpha','discount']);

  var stateToActions = world.stateToActions;
  var transition = world.transition;
  var utility = agentParams.utility;


  var probStick = agentParams.probStick;
  
  
  var agent = dp.cache( 
    function(state, delay){
      return Infer({ method: 'enumerate' }, function(){
        var possibleActions = stateToActions(state);
        var action = uniformDraw(possibleActions);
        var eu = expectedUtility(state, action, delay);    
        factor(agentParams.alpha * eu);
        return action;
      });      
    });

  
  var expectedUtility = dp.cache(
    function(state, action, delay){
      var u = 1.0/(1 + agentParams.discount*delay) * utility(state, action);
      
      assert.ok(u === u,"utility not valid " + u + " " + JSON.stringify(state) + " " + action + " " + delay + ' ' + utility(state, action));
      if (state.terminateAfterAction){
        return u; 
      } else {                     
        return u + expectation( Infer({ method: 'enumerate' }, function(){
          var nextState = transition(state, action); 

          var perceivedDelay = flip(probStick) ? delay + 1 : 0;
          var nextAction = sample(agent(nextState, perceivedDelay));
          return expectedUtility(nextState, nextAction, delay+1);  
        }));
      }                      
    });


  // change action function to single argument
  var agent2 = dp.cache( 
    function(state, evaluateDelay, simulateActionDelay){
      return Infer({ method: 'enumerate' }, function(){
        var possibleActions = stateToActions(state);
        var action = uniformDraw(possibleActions);
        var eu = expectedUtility2(state, action, evaluateDelay, simulateActionDelay);    
        factor(agentParams.alpha * eu);
        return action;
      });      
    });
  
  var expectedUtility2 = dp.cache(
    function(state, action, evaluateDelay, simulateActionDelay){
      var u = 1.0/(1 + agentParams.discount*evaluateDelay) * utility(state, action);
      
      if (state.terminateAfterAction){
        return u; 
      } else {                     
        return u + expectation( Infer({ method: 'enumerate' }, function(){
          var nextState = transition(state, action); 

          var stickWithPlan = flip(probStick);
          var nextSimulateActionDelay = stickWithPlan ? simulateActionDelay + 1 : 0;
          var nextAction = sample(agent(nextState, nextSimulateActionDelay, nextSimulateActionDelay));
          return expectedUtility2(nextState, nextAction, evaluateDelay + 1, nextSimulateActionDelay);  
        }));
      }                      
    });

  // simulate agent2 with agent(start, 0, 0). if no flips, both rise together
  // if always flip, perceived delay always zero. 

  return {
    agentParams : agentParams,
    expectedUtility : expectedUtility,
    agent : agent,
  };
};

var simulateRisk = function(state, world, agent, actualTotalTime, outputType) { 
  var perceivedTotalTime = state.timeLeft;

  var agentAction = agent.agent;
  var expectedUtility = agent.expectedUtility;
  var transition = world.transition;

  var sampleSequence = function (state, actualTimeLeft, delay) {

    var simulateDelay = flip(agent.agentParams.probStick) ? delay : 0;
    var action = sample(agentAction(state, simulateDelay));
    
    var nextState = transition(state, action); 
    var out = {states:state, actions:action, both:[state,action],
               all: [state,action,simulateDelay]}[outputType];
    if (actualTimeLeft==0 || state.terminateAfterAction){
      return [out];
    } else {
      return [ out ].concat( sampleSequence(nextState, actualTimeLeft-1, delay + 1));
    }
  };
  return sampleSequence(state, actualTotalTime, 0);
};

var start = { 
  loc : [3,0],
  terminateAfterAction : false,
  timeLeft : 14
};

var world = makeDonutWorld2({ big : true, maxTimeAtRestaurant : 2});

var donutUtility = makeDonutUtility(world, {
    'Donut N' : [10, -10],
    'Donut S' : [10, -10],
    'Veg'   : [-10, 20],
    'Noodle': [0, 0]
});

var locToFeature = function(loc){
  if (arraysEqual(loc,[4,7])){return 'veg'};
  if (arraysEqual(loc,[2,5])){return 'donut North'};
  if (arraysEqual(loc,[0,0])){return 'donut south'};
};


var hd = false;
var makeAgent = hd ? makeHyperbolicDiscounter : makeRiskAgent;
var simulate = hd? simulateHyperbolic : simulateRisk;



var runRiskAgent = function(probStick){
  var agent = makeAgent(
    { utility : donutUtility,
      alpha : 500, 
      discount : 1, 
      probStick: probStick,
    }, world);
  var out = simulate(start, world, agent, 14,'all');
  var locs = _.map(_.map(out,0), 'loc');
  var actions = _.map(out, 1);
  var delays = _.map(out,2);
  var lastState = locToFeature(last(locs));
  console.log( 'simulateHyperbolic(start, world, agent, 13);', 
               zip(actions, delays));
  console.log('locs: ', locs);
  console.log('ended at:', lastState);
  if (out.length==12){console.log('length of long route')};
  if (out.length==8){console.log('length of temptation route')};
  if (out.length==10){console.log('length of direct route')};

  //if (out.lenght
  return {lastState:lastState};
};

runRiskAgent(.99);

var out =  Infer({ method: 'rejection', samples: 2 }, function(){return runRiskAgent(.8).lastState;});
printDist(out);

  
ash();

var runAgent = function(sophisticatedOrNaive){
  var agent = makeHyperbolicDiscounter(
    { utility : donutUtility,
      alpha : 500, 
      discount : 1, 
      sophisticatedOrNaive  : sophisticatedOrNaive,
    }, world);
  var locs = _.map(simulateHyperbolic(start, world, agent, 13,'states'),'loc');
  console.log( 'simulateHyperbolic(start, world, agent, 13)  ' + sophisticatedOrNaive, locs);
  console.log('ended at: ', locToFeature(last(locs)));
               
};
runAgent('sophisticated');
runAgent('naive');





ash();









// TODO merge with simulate, use a more descriptive name. It seems that
// ideally we'd use the MAP path, and the 
var MAPActionPath = function(state, world, agent, actualTotalTime, statesOrActions) { 
  var perceivedTotalTime = state.timeLeft;
  assert.ok( perceivedTotalTime  > 1, 'perceivedTime<=1. If=1 then should have state.terminateAfterAction, but then simulate wont work' + JSON.stringify(state));

  var agentAction = agent.agent;
  var expectedUtility = agent.expectedUtility;
  var transition = world.transition;

  var sampleSequence = function (state, actualTimeLeft) {
      var action = agentAction(state, actualTotalTime-actualTimeLeft).MAP().val;
      var nextState = transition(state, action); 
      var out = {states:state, actions:action, both:[state,action]}[statesOrActions];
    if (actualTimeLeft==0 || state.terminateAfterAction){
      return [out];
    } else {
      return [ out ].concat( sampleSequence(nextState, actualTimeLeft-1));
    }
  };
  return sampleSequence(state, actualTotalTime);
};

var getExpectedUtilities = function(trajectory, agent, actions) { 
  var expectedUtility = agent.expectedUtility;

  var v = mapIndexed(function(i, state) {
    return [state.loc, map(function (a) { return  expectedUtility(state, a, i); }, actions)];
  }, trajectory)
  return v;
};

// TODO more descriptive name
var mdpSim = function(start, world, agent, actualTotalTime) { 
  var trajectory = simulateHyperbolic(start, world, agent, actualTotalTime, 'states');

  var trajectoryPlans = map(function (state) {
    var currentPlan = MAPActionPath(state, world, agent, state.timeLeft, 'states');
    return getExpectedUtilities(currentPlan, agent, world.actions);
  }, trajectory);

  GridWorld.draw(world, {trajectory : trajectory, paths : trajectoryPlans });
}

null
