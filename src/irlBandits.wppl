// IRL Bandits

// Functions for running agents and doing inference depend on beliefDelayAgent.wppl


// Multi-arm deterministic bandits.
// Rewards of arms are fixed by the latent state. 
var makeIRLBanditWorld = function (numArms) {
  var actions = _.range(numArms);
  
  var mdpTransition = function (state, action) {
    var dead = state.timeLeft - 1 == 1 ? true : false;
    return update(state, { loc:action, timeLeft:state.timeLeft - 1, dead:dead });
  };
  
  var manifestStateToActions = function (manifestState) {return actions;};

  var transition = function(state, action){
    return buildState( mdpTransition(state.manifestState, action), state.latentState);
  };

  var observe = function(state){ // agent observes the reward of arm when he chooses it
    if (state.manifestState.loc=='start'){return 'noObservation';}
    return state.latentState[state.manifestState.loc];
  };
  
  return {manifestStateToActions: manifestStateToActions, transition:transition, observe:observe};
};



// We have a set of prizes ['a','b','c']. The agent is unsure of the prize
// for some of the arms. We (doing inference) are unsure of some of the
// utilities for prizes. 

// prizeToUtility = {a: 0, b: 5, ... }
// the true mapping from arms to prizes must be in agent's prior
// (for convenience, we can often let the true mapping be 0:a, 1:b, ...
// i.e. zip( _.range(numArms), ['a','b','c', ...].slice(...))

// form of latentState: {arm:prize}, usually, {0:'a', ....}

// we could implement stochastic version with stochastic utilities
// (where the utility is then baked into the observation). 


var noDiscountBaseAgentParams = { // doesn't contain priorBelief or utility
  alpha: 100,
  noDelays: true,
  discount: 0,
  sophisticatedOrNaive: 'naive',
  myopiaCutoff: false
};


var makeIRLBanditAgent = function(prizeToUtility, agentParams, worldAndStart){
  var priorBelief = agentParams.priorBelief;
  assert.ok( _.isFinite(priorBelief.score([],worldAndStart.startState.latentState)), 
             'makeIRLBanditAgent does not have true latent in support');
  
  var utility = function(state,action){
    var loc = state.manifestState.loc;
    if (loc=='start'){return 0;};
    return prizeToUtility[state.latentState[loc]];
  };
  return makeBeliefDelayAgent(update(agentParams,{utility:utility}), worldAndStart.world);
};


var makeIRLBanditWorldAndStart = function(numArms, armToPrize, perceivedTotalTime){
  map( function(i){assert.ok( _.isString(armToPrize[i]), 'makeIRLBanditWorld args' );},
       _.range(numArms));
  
  var actualTotalTime = perceivedTotalTime;
  var world = makeIRLBanditWorld(numArms);
  var startState = {manifestState:{loc:'start', timeLeft: perceivedTotalTime, dead:false}, 
                    latentState: armToPrize};

  return {world:world, startState: startState, actualTotalTime:actualTotalTime};
};


// Inference by sampling full trajectories or by doing 'off policy' inference

var inferIRLBandit = function(worldAndStart, baseAgentParams, prior, observedStateAction, trajectoryOrOffPolicy, numRejectionSamples){
  var world = worldAndStart.world;
  var startState = worldAndStart.startState;
  var actualTotalTime = worldAndStart.actualTotalTime;
  var observe = world.observe;

  var priorPrizeToUtility = prior.priorPrizeToUtility;
  var priorAgentPrior = prior.priorAgentPrior;

  assert.ok(isWorld(world) && isState(startState) && isERP(priorPrizeToUtility) && isERP(priorAgentPrior), 'inferirlbandit args');
  assert.ok(trajectoryOrOffPolicy == 'trajectory' || trajectoryOrOffPolicy=='offPolicy', 'trajectoryOrOffPolicy bad');
  if (actualTotalTime != observedStateAction.length){console.log( 'Warning: actualTotalTime is not observed trajectory length');}
  assert.ok( isState(observedStateAction[0][0]), 'fullstate in trajectory for inferirlbandit');

  
  return Enumerate(function(){
    // priors and makeAgent are specific to IRL Bandits
    var prizeToUtility = sample(priorPrizeToUtility);
    var priorBelief = sample(priorAgentPrior);
    
    var agent = makeIRLBanditAgent(prizeToUtility, update(baseAgentParams, {priorBelief:priorBelief}), worldAndStart);
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;

    // Factor on whole sampled trajectory (SLOW IF NOT DETERMINISTIC AND NUM SAMPLES HIGH)
    var factorOnTrajectory = function(){
      var trajectoryERP = Rejection( function(){
        return simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'states')}, numRejectionSamples);
      factor(trajectoryERP.score([], map(first, observedStateAction)));
    };

    // Move agent through observed sequence 
    var factorSequenceOffPolicy = function(currentBelief, timeIndex){
      if (timeIndex < observedStateAction.length) { 
        var observedAction = observedStateAction[timeIndex][1];
        
        // Go to next world state and sample observation from that state
        var state = observedStateAction[timeIndex][0];
        var observation = observe(state);  // assuming that observations are deterministic?

        // Update agent's internal state and get action ERP
        var delay = 0;
        var nextBelief = agentUpdateBelief(state.manifestState, currentBelief, observation, delay);
        var nextActionERP = agentAct(state.manifestState, nextBelief, delay);

        factor(nextActionERP.score([], observedAction));

        // condition on next world state, passing through updated internal state
        factorSequenceOffPolicy(nextBelief, timeIndex + 1);
      }
    };

    var doInfer = (trajectoryOrOffPolicy=='trajectory') ? factorOnTrajectory() : factorSequenceOffPolicy(priorBelief, 0);
    
    return {prizeToUtility: prizeToUtility, priorBelief:priorBelief};
  });
};
