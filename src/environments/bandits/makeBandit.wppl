// Stochastic bandit pomdp constructor: outputs Bandit environments whose prizes are
// strings or numbers.

// The latent state stores each arm's ERP over prizes, and the manifest state
// location contains the reward gained after the previous arm pull.

// main constructor function is makeBandit

var makeBanditWorld = function(numberOfArms) {
  var actions = range(numberOfArms);

  var advanceTime = function(manifestState) {
    var newTimeLeft = manifestState.timeLeft - 1;
    var terminateAfterAction = (newTimeLeft === 1);
    return update(manifestState, {timeLeft: newTimeLeft,
  				  terminateAfterAction: terminateAfterAction});
  };
  
  var manifestStateToActions = function(manifestState) {return actions;};

  var transition = function(state, action){
    
    assert.ok(hasSample(state.latentState[action]), 'bandit transition: latent state has no ERP for given action');
    // figure out what the prize is
    var prize = sample(state.latentState[action]);

    // make the new location the prize, advance the time
    var manifestStateWithReward = update(state.manifestState, {loc: prize});
    var newManifestState = advanceTime(manifestStateWithReward);

    return buildState( newManifestState, state.latentState);
  };

  var observe = function(state){
    // in beliefAgent, the observation here will be augmented with an 
    // observation of the manifest state, and belief updating will depend on
    // both that full observation and the action the agent took. This is all the
    // agent needs to update, so there is no need to have an observation here.
    // However, if we make the observation 'noObservation', the agent will not
    // update its beliefs, which is bad, so we arbitrarily set the observation
    // to always be 0.

    return 0;

  };

  return {
    manifestStateToActions: manifestStateToActions,
    transition: transition,
    observe: observe
  };
};

var makeBanditStartState = function(numberOfTrials, armToPrizeERP) {
  return {manifestState: {loc: 'start',
			  timeLeft: numberOfTrials,
			  terminateAfterAction: false},
	  latentState: armToPrizeERP};
};

var makeBandit = function(options) {

  assert.ok(_.has(options, 'numberOfArms') && _.has(options, 'armToPrizeERP')
	    && _.has(options, 'numberOfTrials'),
	   'makeBandit args: options does not contain one or more of numberOfArms, armToPrizeERP, and numberOfTrials');

  var numberOfArms = options.numberOfArms;
  var armToPrizeERP = options.armToPrizeERP;
  var numberOfTrials = options.numberOfTrials;
  var numericalPrizes = options.numericalPrizes;
  var prizeToUtility = options.prizeToUtility;
  
  // ensure that armToPrizeERP has an entry for every arm
  mapN(function(arm){
    assert.ok(_.has(armToPrizeERP, arm.toString()),
	      'makeBandit: arm ' + arm + ' has no entry in armToPrizeERP');
  }, numberOfArms);
  
  // ensure that armToPrizeERP has an ERP for every arm
  mapN(function(arm){
    assert.ok(hasSample(armToPrizeERP[arm.toString()]),
	      'makeBandit: arm ' + arm + ' has no ERP in armToPrizeERP');
  }, numberOfArms);

  // ensure that prizes are numerical iff numericalPrizes is true,
  // and strings iff numericalPrizes is false
  // also, if numericalPrizes is false, and prizeToUtility is defined, ensure
  // that every prize has a utility
  if (numericalPrizes) {
    var supportIsNumeric = function(arm) {
      var erp = armToPrizeERP[arm];
      map(function(x){
	assert.ok(_.isFinite(x) && inSupport(x, erp),
		  'makeBandit: some prizes are non-numeric but numericalPrizes is true');
      }, erp.support());
    };

    mapN(supportIsNumeric, numberOfArms);
    
  } else {
    var supportIsStringy = function(arm) {
      var erp = armToPrizeERP[arm];
      map(function(x){
	assert.ok(_.isString(x) && inSupport(x, erp),
		  'makeBandit: some prizes are not strings but numericalPrizes is false');
      }, erp.support());
    };

    mapN(supportIsStringy, numberOfArms);
  }
  
  var world = makeBanditWorld(numberOfArms);
  var startState = makeBanditStartState(numberOfTrials, armToPrizeERP);

  return {
    world: world,
    startState: startState,
    armToPrizeERP: armToPrizeERP,
    numericalPrizes: numericalPrizes,
    numberOfArms: numberOfArms
  };
};


var numericBanditUtility = function(state, action) {
  var prize = state.manifestState.loc;
  return prize === 'start' ? 0 : prize;
};

var makeStringBanditUtility = function(prizeToUtility) {
  return function(state, action) {
    var prize = state.manifestState.loc;
    return prize === 'start' ? 0 : prizeToUtility[prize];
  };
};

var makeBanditUtility = function(bandit, prizeToUtility) {
  if (bandit.numericalPrizes) {
    return numericBanditUtility;
  } else {
    var prizesHaveUtilities = function(arm) {
      var armToPrizeERP = bandit.armToPrizeERP;
      var erp = armToPrizeERP[arm];
      map(function(x){
	assert.ok(_.isFinite(prizeToUtility[x]) && inSupport(x, erp),
		  'makeBandit: some prizes do not have utilities');
      }, erp.support());
    };

    mapN(prizesHaveUtilities, bandit.numberOfArms);
    
    return makeStringBanditUtility(prizeToUtility);
  }
};

var makeBanditAgent = function(agentParams, bandit, beliefOrBeliefDelay,
			       prizeToUtility) {
  
  var priorBelief = agentParams.priorBelief;
  var numericalPrizes = bandit.numericalPrizes;

  if (isERPOverLatent(priorBelief)) {
    assert.ok(_.isEqual(priorBelief.manifestState,
			bandit.startState.manifestState),
	      'makeBanditAgent: priorBelief has wrong start manifestState');
  } else {
    assert.ok(hasSample(priorBelief), 'makeBanditAgent: priorBelief is neither an ERP nor an object containing a manifest state and a latentStateERP');
    assert.ok(_.isEqual(sample(priorBelief).manifestState,
			bandit.startState.manifestState),
	      'makeBanditAgent: priorBelief has wrong start manifestState');
  }

  var utility = makeBanditUtility(bandit, prizeToUtility);
  
  var makeAgent = getMakeAgentFunction(beliefOrBeliefDelay);
  
  return makeAgent(update(agentParams, {utility: utility}),
		   bandit.world);
};

// Inference by sampling full trajectories or by doing 'off policy' inference

var inferBandit = function(bandit, baseAgentParams, prior, observedStateAction,
			   trajectoryOrOffPolicy, numRejectionSamples,
			   beliefOrBeliefDelay){
  var world = bandit.world;
  var startState = bandit.startState;
  var numericPrizes = bandit.numericPrizes;

  var simulate = getSimulateFunction(beliefOrBeliefDelay);

  var priorPrizeToUtility = prior.priorPrizeToUtility || false;
  var priorAgentPrior = prior.priorAgentPrior;
  var priorMyopia = prior.priorMyopia || false;


  assert.ok(isPOMDPWithManifestLatent(world) && stateHasManifestLatent(startState)
	    && hasSample(priorAgentPrior),
	    'inferBandit args: \n\n startState ' + startState
	    + ' \n\n priorAgentPrior' + priorAgentPrior );
  assert.ok(trajectoryOrOffPolicy === 'trajectory'
	    || trajectoryOrOffPolicy === 'offPolicy',
	    'inferBandit args: trajectoryOrOffPolicy bad');
  assert.ok( stateHasManifestLatent(observedStateAction[0][0]),
	     'inferBandit args: observedStateAction first entry of first entry not a POMDP state');

  if (priorPrizeToUtility) {
    assert.ok(hasSample(priorPrizeToUtility), 'inferBandit args: priorPrizeToUtility defined but not an ERP');
  }

  if (priorMyopia) {
    assert.ok(hasSample(priorMyopia), 'inferBandit args: priorMyopia defined but not an ERP');
  }

  
  return Enumerate(function(){
    // priors and makeAgent are specific to bandits
    var prizeToUtility = priorPrizeToUtility ? sample(priorPrizeToUtility)
	  : undefined;
    var priorBelief = sample(priorAgentPrior);
    var myopiaUpdate = priorMyopia? {myopia: sample(priorMyopia) } : {};
    var newAgentParams = update(baseAgentParams, update({priorBelief:priorBelief}, myopiaUpdate) );
    
    var agent = makeBanditAgent(newAgentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    var observe = agent.POMDPFunctions.observe;

    // Factor on whole sampled trajectory (SLOW IF NOT DETERMINISTIC AND NUM SAMPLES HIGH)
    var factorOnTrajectory = function(){
      var trajectoryERP = Rejection( function(){
        return simulate(startState, world, agent, 'states');}, numRejectionSamples);
      factor(trajectoryERP.score([], map(first, observedStateAction)));
    };

    // Move agent through observed sequence 
    var factorSequenceOffPolicy = function(currentBelief, previousAction, timeIndex){
      if (timeIndex < observedStateAction.length) { 

        // Go to next world state and sample observation from that state
        var state = observedStateAction[timeIndex][0];
        var observation = observe(state);

        // Update agent's internal state and get action ERP
        var delay = 0;     
        var nextBelief = beliefOrBeliefDelay == 'belief' ?
            agentUpdateBelief(currentBelief, observation, previousAction) :
            agentUpdateBelief(currentBelief, observation, previousAction, delay);

        
        var nextActionERP = beliefOrBeliefDelay == 'belief' ? 
            agentAct(nextBelief) : agentAct(nextBelief, delay); 

        var observedAction = observedStateAction[timeIndex][1];
        factor(nextActionERP.score([], observedAction));

        // condition on next world state, passing through updated internal state
        factorSequenceOffPolicy(nextBelief, observedAction, timeIndex + 1);
      }
    };

    var doInfer = (trajectoryOrOffPolicy=='trajectory') ? factorOnTrajectory() : 
        factorSequenceOffPolicy(priorBelief,'noAction', 0);
    
    return {prizeToUtility: prizeToUtility, priorBelief: priorBelief, myopiaBound: newAgentParams.myopia.bound};
  });
};

var agentModelsBanditInfer = function(baseAgentParams, priorPrizeToUtility,
				      priorInitialBelief, bandit,
				      observedSequence) {

  return Enumerate(function(){
    var prizeToUtility = priorPrizeToUtility ? sample(priorPrizeToUtility)
	  : undefined;
    var initialBelief = sample(priorInitialBelief);

    var newAgentParams = update(baseAgentParams, {priorBelief:initialBelief});
    
    var agent = makeBanditAgent(newAgentParams, bandit, 'belief',
				prizeToUtility);
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    
    var factorSequence = function(currentBelief, previousAction, timeIndex){
      if (timeIndex < observedSequence.length) { 
        var state = observedSequence[timeIndex].state;
        var observation = observedSequence[timeIndex].observation;
        var nextBelief = agentUpdateBelief(currentBelief, observation, previousAction);
        var nextActionERP = agentAct(nextBelief);
        var observedAction = observedSequence[timeIndex].action;
        
        factor(nextActionERP.score([], observedAction));
        
        factorSequence(nextBelief, observedAction, timeIndex + 1);
      }
    };
    factorSequence(initialBelief,'noAction', 0);
    
    return {prizeToUtility: prizeToUtility, priorBelief:initialBelief};
  });
};

var noDiscountBaseAgentParams = { // doesn't contain priorBelief or utility
  alpha: 100,
  noDelays: true,
  discount: 0,
  sophisticatedOrNaive: 'naive',
  myopia: {on: false, bound:0},
  boundVOI: {on: false, bound:0}
};
