// Stochastic bandit POMDP constructor: outputs Bandit environments whose prizes are
// strings or numbers.

// The latent state stores each arm's Dist over prizes, and the manifest state
// location contains the reward gained after the previous arm pull.



// helper function
var inSupport = function(x, dist){
  return _.isFinite(dist.score(x));
};


// main constructor function is makeBandit
var makeBanditWorld = function(numberOfArms) {
  var actions = _.range(numberOfArms);

  var advanceTime = function(manifestState) {
    var newTimeLeft = manifestState.timeLeft - 1;
    var terminateAfterAction = (newTimeLeft === 1);
    return update(manifestState, {timeLeft: newTimeLeft,
  				  terminateAfterAction: terminateAfterAction});
  };
  
  var manifestStateToActions = function(manifestState) {return actions;};

  var transition = function(state, action){
    
    assert.ok(hasSample(state.latentState[action]), 'bandit transition: latent state has no Dist for given action');
    // figure out what the prize is
    var prize = sample(state.latentState[action]);

    // make the new location the prize, advance the time
    var manifestStateWithReward = update(state.manifestState, {loc: prize});
    var newManifestState = advanceTime(manifestStateWithReward);

    return buildState( newManifestState, state.latentState);
  };

  var observe = function(state){
    // in beliefAgent, the observation here will be augmented with an 
    // observation of the manifest state, and belief updating will depend on
    // both that full observation and the action the agent took. This is all the
    // agent needs to update, so there is no need to have an observation here.
    // However, if we make the observation 'noObservation', the agent will not
    // update its beliefs, which is bad, so we arbitrarily set the observation
    // to always be 0.

    return 0;

  };

  return {
    manifestStateToActions: manifestStateToActions,
    transition: transition,
    observe: observe
  };
};

var makeBanditStartState = function(numberOfTrials, armToPrizeDist) {
  return {manifestState: {loc: 'start',
			  timeLeft: numberOfTrials,
			  terminateAfterAction: false},
	  latentState: armToPrizeDist};
};

var makeBanditPOMDP = function(options) {

  assert.ok(_.has(options, 'numberOfArms') && _.has(options, 'armToPrizeDist')
	    && _.has(options, 'numberOfTrials'),
	   'makeBanditPOMDP args: options does not contain one or more of numberOfArms, armToPrizeDist, and numberOfTrials');

  var numberOfArms = options.numberOfArms;
  var armToPrizeDist = options.armToPrizeDist;
  var numberOfTrials = options.numberOfTrials;
  var numericalPrizes = options.numericalPrizes;
  var prizeToUtility = options.prizeToUtility;
  
  // ensure that armToPrizeDist has an entry for every arm
  mapN(function(arm){
    assert.ok(_.has(armToPrizeDist, arm.toString()),
	      'makeBandit: arm ' + arm + ' has no entry in armToPrizeDist');
  }, numberOfArms);
  
  // ensure that armToPrizeDist has an Dist for every arm
  mapN(function(arm){
    assert.ok(hasSample(armToPrizeDist[arm.toString()]),
	      'makeBandit: arm ' + arm + ' has no Dist in armToPrizeDist');
  }, numberOfArms);

  // ensure that prizes are numerical iff numericalPrizes is true,
  // and strings iff numericalPrizes is false
  // also, if numericalPrizes is false, and prizeToUtility is defined, ensure
  // that every prize has a utility
  if (numericalPrizes) {
    var supportIsNumeric = function(arm) {
      var dist = armToPrizeDist[arm];
      map(function(x){
	assert.ok(_.isFinite(x) && inSupport(x, dist),
		  'makeBandit: some prizes are non-numeric but numericalPrizes is true');
      }, dist.support());
    };

    mapN(supportIsNumeric, numberOfArms);
    
  } else {
    var supportIsStringy = function(arm) {
      var dist = armToPrizeDist[arm];
      map(function(x){
	assert.ok(_.isString(x) && inSupport(x, dist),
		  'makeBandit: some prizes are not strings but numericalPrizes is false');
      }, dist.support());
    };

    mapN(supportIsStringy, numberOfArms);
  }
  
  var world = makeBanditWorld(numberOfArms);
  var startState = makeBanditStartState(numberOfTrials, armToPrizeDist);

  return {
    world: world,
    startState: startState,
    armToPrizeDist: armToPrizeDist,
    numericalPrizes: numericalPrizes,
    numberOfArms: numberOfArms
  };
};


var numericBanditUtility = function(state, action) {
  var prize = state.manifestState.loc;
  return prize === 'start' ? 0 : prize;
};

var makeStringBanditUtility = function(prizeToUtility) {
  return function(state, action) {
    var prize = state.manifestState.loc;
    return prize === 'start' ? 0 : prizeToUtility[prize];
  };
};

var makeBanditUtility = function(bandit, prizeToUtility) {
  if (bandit.numericalPrizes) {
    return numericBanditUtility;
  } else {
    var prizesHaveUtilities = function(arm) {
      var armToPrizeDist = bandit.armToPrizeDist;
      var dist = armToPrizeDist[arm];
      map(function(x){
	assert.ok(_.isFinite(prizeToUtility[x]) && inSupport(x, dist),
		  'makeBandit: some prizes do not have utilities');
      }, dist.support());
    };

    mapN(prizesHaveUtilities, bandit.numberOfArms);
    
    return makeStringBanditUtility(prizeToUtility);
  }
};

var makeBanditAgent = function(agentParams, bandit, beliefOrBeliefDelay,
			       prizeToUtility) {
  
  var priorBelief = agentParams.priorBelief;
  var numericalPrizes = bandit.numericalPrizes;

  if (isDistOverLatent(priorBelief)) {
    assert.ok(_.isEqual(priorBelief.manifestState,
			bandit.startState.manifestState),
	      'makeBanditAgent: priorBelief has wrong start manifestState');
  } else {
    assert.ok(hasSample(priorBelief), 'makeBanditAgent: priorBelief is neither an Dist nor an object containing a manifest state and a latentStateDist');
    assert.ok(_.isEqual(sample(priorBelief).manifestState,
			bandit.startState.manifestState),
	      'makeBanditAgent: priorBelief has wrong start manifestState');
  }

  var utility = makeBanditUtility(bandit, prizeToUtility);
  
  var makeAgent = getMakeAgentFunction(beliefOrBeliefDelay);
  
  return makeAgent(update(agentParams, {utility: utility}),
		   bandit.world);
};

// Inference by sampling full trajectories or by doing 'off policy' inference

var inferBandit = function(bandit, baseAgentParams, prior, observedStateAction,
			   trajectoryOrOffPolicy, numRejectionSamples,
			   beliefOrBeliefDelay){
  var world = bandit.world;
  var startState = bandit.startState;
  var numericPrizes = bandit.numericPrizes;

  //var simulate = getSimulateFunction(beliefOrBeliefDelay);

  var priorPrizeToUtility = prior.priorPrizeToUtility || false;
  var priorAgentPrior = prior.priorAgentPrior;
  var priorRewardMyopic = prior.priorRewardMyopic || false;


  assert.ok(isPOMDPWithManifestLatent(world) && stateHasManifestLatent(startState)
	    && hasSample(priorAgentPrior),
	    'inferBandit args: \n\n startState ' + startState
	    + ' \n\n priorAgentPrior' + priorAgentPrior );
  assert.ok(trajectoryOrOffPolicy === 'trajectory'
	    || trajectoryOrOffPolicy === 'offPolicy',
	    'inferBandit args: trajectoryOrOffPolicy bad');
  assert.ok( stateHasManifestLatent(observedStateAction[0][0]),
	     'inferBandit args: observedStateAction first entry of first entry not a POMDP state');

  if (priorPrizeToUtility) {
    assert.ok(hasSample(priorPrizeToUtility), 'inferBandit args: priorPrizeToUtility defined but not an Dist');
  }

  if (priorRewardMyopic) {
    assert.ok(hasSample(priorRewardMyopic), 'inferBandit args: priorMyopia defined but not an Dist');
  }

  
  return Infer({ method: 'enumerate' }, function(){
    // priors and makeAgent are specific to bandits
    var prizeToUtility = priorPrizeToUtility ? sample(priorPrizeToUtility)
	  : undefined;
    var priorBelief = sample(priorAgentPrior);
    var rewardMyopicUpdate = priorRewardMyopic ? {rewardMyopic: sample(priorRewardMyopic) } : {};
    var newAgentParams = update(baseAgentParams, update({priorBelief:priorBelief}, 
                                                        rewardMyopicUpdate) );
    
    var agent = makeBanditAgent(newAgentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    var observe = agent.POMDPFunctions.observe;

    // Factor on whole sampled trajectory (SLOW IF NOT DETERMINISTIC AND NUM SAMPLES HIGH)
    var factorOnTrajectory = function(){
      var trajectoryDist = Infer({ method: 'rejection', samples: numRejectionSamples }, function(){
        return simulatePOMDP(startState, world, agent, 'states');});
      factor(trajectoryDist.score(map(first, observedStateAction)));
    };

    // Move agent through observed sequence 
    var factorSequenceOffPolicy = function(currentBelief, previousAction, timeIndex){
      if (timeIndex < observedStateAction.length) { 

        // Go to next world state and sample observation from that state
        var state = observedStateAction[timeIndex][0];
        var observation = observe(state);

        // Update agent's internal state and get action Dist
        var delay = 0;     
        var nextBelief = beliefOrBeliefDelay == 'belief' ?
            agentUpdateBelief(currentBelief, observation, previousAction) :
            agentUpdateBelief(currentBelief, observation, previousAction, delay);

        
        var nextActionDist = beliefOrBeliefDelay == 'belief' ? 
            agentAct(nextBelief) : agentAct(nextBelief, delay); 

        var observedAction = observedStateAction[timeIndex][1];
        factor(nextActionDist.score(observedAction));

        // condition on next world state, passing through updated internal state
        factorSequenceOffPolicy(nextBelief, observedAction, timeIndex + 1);
      }
    };

    var doInfer = (trajectoryOrOffPolicy=='trajectory') ? factorOnTrajectory() : 
        factorSequenceOffPolicy(priorBelief,'noAction', 0);

    var bound = newAgentParams.rewardMyopic ? newAgentParams.rewardMyopic.bound : 0;
    return {prizeToUtility: prizeToUtility, 
            priorBelief: priorBelief, 
            rewardMyopicBound: bound};
  });
};

var noDiscountBaseAgentParams = { // doesn't contain priorBelief or utility
  alpha: 100,
  noDelays: true,
  discount: 0,
  sophisticatedOrNaive: 'naive'
};
