// Stochastic bandit pomdp constructor. Can output bandits whose prizes are
// strings or numbers.

// The latent state stores each arm's ERP over prizes, and the manifest state
// location contains the reward gained after the previous arm pull.

var makeBanditWorld = function(numberOfArms) {
  var actions = range(numberOfArms);

  var advanceTime = function(manifestState) {
    var newTimeLeft = manifestState.timeLeft - 1;
    var terminateAfterAction = (newTimeLeft === 1);
    return update(manifestState, {timeLeft: newTimeLeft,
  				  terminateAfterAction: terminateAfterAction});
  };
  
  var manifestStateToActions = function(manifestState) {return actions;};

  var transition = function(state, action){
    // figure out what the prize is
    var prize = sample(state.latentState[action]);

    // make the new location the prize, advance the time
    var manifestStateWithReward = update(state.manifestState, {loc: prize});
    var newManifestState = advanceTime(manifestStateWithReward);
    
    return buildState( newManifestState, state.latentState);
  };

  var observe = function(state){
    // in beliefAgent, the observation here will be augmented with an 
    // observation of the manifest state, and belief updating will depend on
    // both that full observation and the action the agent took. This is all the
    // agent needs to update, so there is no need to have an observation here.
    // However, if we make the observation 'noObservation', the agent will not
    // update its beliefs, which is bad, so we arbitrarily set the observation
    // to always be 0.

    return 0;

  };

  return {
    manifestStateToActions: manifestStateToActions,
    transition: transition,
    observe: observe
  };
};

var makeBanditStartState = function(numberOfTrials, armToPrizeERP) {
  return {manifestState: {loc: 'start',
			  timeLeft: numberOfTrials,
			  terminateAfterAction: false},
	  latentState: armToPrizeERP};
};

var makeBandit = function(options) {

  assert.ok(_.has(options, ['numberOfArms', 'armToPrizeERP', 'numberOfTrials']));

  var numberOfArms = options.numberOfArms;
  var armToPrizeERP = options.armToPrizeERP;
  var numberOfTrials = options.numberOfTrials;
  var numericalPrizes = options.numericalPrizes;
  var prizeToUtility = options.prizeToUtility;
  
  // ensure that armToPrizeERP has an entry for every arm
  mapN(function(n){
    assert.ok(_.has(armToPrizeERP, _.toString(n)),
	      'not every arm has an entry in armToPrizeERP in makeBandit');
  }, numberOfArms);
  
  // ensure that armToPrizeERP has an ERP for every arm
  mapN(function(arm){
    assert.ok(isERP(armToPrizeERP[arm]),
	      'not every arm has an ERP in armToPrizeERP in makeBandit');
  }, numberOfArms);

  // ensure that prizes are numerical iff numericalPrizes is true,
  // and strings iff numericalPrizes is false
  // also, if numericalPrizes is false, and prizeToUtility is defined, ensure
  // that every prize has a utility
  if (numericalPrizes) {
    var supportIsNumeric = function(arm) {
      var erp = armToPrizeERP[arm];
      map(function(x){
	assert.ok(_.isFinite(x) && inSupport(x, erp),
		  'makeBandit: some prizes are non-numeric but numericalPrizes is true');
      }, erp.support);
    };

    mapN(supportIsNumeric, numberOfArms);
    
  } else {
    var supportIsStringy = function(arm) {
      var erp = armToPrizeERP[arm];
      map(function(x){
	assert.ok(_.isString(x) && inSupport(x, erp),
		  'makeBandit: some prizes are not strings but numericalPrizes is false');
      }, erp.support);
    };

    mapN(supportIsStringy, numberOfArms);

    if (prizeToUtility) {
      var prizesHaveUtilities = function(arm) {
	var erp = armToPrizeERP[arm];
	map(function(x){
	  assert.ok(_.isFinite(prizeToUtility[x]) && inSupport(x, erp),
		    'makeBandit: some prizes do not have utilities');
	}, erp.support);
      };

      mapN(prizesHaveUtilities, numberOfArms);
      
    } 
  }
  
  var world = makeBanditWorld(numberOfArms);
  var startState = makeBanditStartState(numberOfTrials, armToPrizeERP);

  return {
    world: world,
    startState: startState,
    armToPrizeERP: armToPrizeERP,
    numericalPrizes: numericalPrizes
  };
};


var numericBanditUtility = function(state, action) {
  var prize = state.manifestState.loc;
  return prize === 'start' ? 0 : prize;
};

var makeStringBanditUtility = function(prizeToUtility) {
  return function(state, action) {
    var prize = state.manifestState.loc;
    return prize === 'start' ? 0 : prizeToUtility[prize];
  };
};

var makeBanditUtility = function(bandit, prizeToUtility) {
  if (bandit.numericalPrizes) {
    return numericBanditUtility;
  } else {
    return makeStringBanditUtility(prizeToUtility);
  }
};

var makeBanditAgent = function(agentParams, bandit, beliefOrBeliefDelay,
			       prizeToUtility) {
  var priorBelief = agentParams.priorBelief;
  var numericalPrizes = bandit.numericalPrizes;

  if (ERPOverLatent(priorBelief)) {
    assert.ok(_.isEqual(priorBelief.manifestState,
			bandit.startState.manifestState),
	      'makeBanditAgent: priorBelief has wrong start manifestState');
  } else {
    assert.ok(_.isEqual(sample(priorBelief).manifestState,
			bandit.startState.manifestState),
	      'makeBanditAgent: priorBelief has wrong start manifestState');
  }
  
  var utility = makeBanditUtility(bandit, prizeToUtility);

  var makeAgent = getMakeAgentFunction(beliefOrBeliefDelay);

  return makeAgent(update(agentParams, {utility: utility}),
		   bandit.world);
};

// work in progress

// Inference by sampling full trajectories or by doing 'off policy' inference

// need to know if bandit is numeric - save this in bandit?

var inferBandit = function(bandit, baseAgentParams, prior, observedStateAction,
			   trajectoryOrOffPolicy, numRejectionSamples,
			   beliefOrBeliefDelay){
  var world = bandit.world;
  var startState = bandit.startState;
  var worldObserve = world.observe;
  var observe = getFullObserve(worldObserve);

  var simulate = getSimulateFunction(beliefOrBeliefDelay);

  var priorPrizeToUtility = prior.priorPrizeToUtility;
  var priorAgentPrior = prior.priorAgentPrior;
  var priorMyopia = prior.priorMyopia || false;


  assert.ok(isPOMDPWorld(world) && isPOMDPState(startState) && isERP(priorPrizeToUtility) && isERP(priorAgentPrior), 'inferirlbandit args ' + priorPrizeToUtility + ' \n\n priorAgentPrior' + priorAgentPrior );
  assert.ok(trajectoryOrOffPolicy == 'trajectory' || trajectoryOrOffPolicy=='offPolicy', 'trajectoryOrOffPolicy bad');
  assert.ok( isPOMDPState(observedStateAction[0][0]), 'fullstate in trajectory for inferirlbandit');

  
  return Enumerate(function(){
    // priors and makeAgent are specific to bandits
    var prizeToUtility = sample(priorPrizeToUtility);
    var priorBelief = sample(priorAgentPrior);
    var myopiaUpdate = priorMyopia? {myopia: sample(priorMyopia) } : {};
    var newAgentParams = update(baseAgentParams, update({priorBelief:priorBelief}, myopiaUpdate) );
    
    var agent = _makeBanditAgent(newAgentParams, bandit, beliefOrBeliefDelay);
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;

    // Factor on whole sampled trajectory (SLOW IF NOT DETERMINISTIC AND NUM SAMPLES HIGH)
    var factorOnTrajectory = function(){
      var trajectoryERP = Rejection( function(){
        return simulate(startState, world, agent, 'states')}, numRejectionSamples);
      factor(trajectoryERP.score([], map(first, observedStateAction)));
    };

    // Move agent through observed sequence 
    var factorSequenceOffPolicy = function(currentBelief, previousAction, timeIndex){
      if (timeIndex < observedStateAction.length) { 

        // Go to next world state and sample observation from that state
        var state = observedStateAction[timeIndex][0];
        var observation = observe(state);

        // Update agent's internal state and get action ERP
        var delay = 0;     
        var nextBelief = beliefOrBeliefDelay == 'belief' ?
            agentUpdateBelief(currentBelief, observation, previousAction) :
            agentUpdateBelief(currentBelief, observation, previousAction, delay);

        
        var nextActionERP = beliefOrBeliefDelay == 'belief' ? 
            agentAct(nextBelief) : agentAct(nextBelief, delay); 

        var observedAction = observedStateAction[timeIndex][1];
        factor(nextActionERP.score([], observedAction));

        // condition on next world state, passing through updated internal state
        factorSequenceOffPolicy(nextBelief, observedAction, timeIndex + 1);
      }
    };

    var doInfer = (trajectoryOrOffPolicy=='trajectory') ? factorOnTrajectory() : 
        factorSequenceOffPolicy(priorBelief,'noAction', 0);
    
    return {prizeToUtility: prizeToUtility, priorBelief:priorBelief, myopiaBound: newAgentParams.myopia.bound};
  });
};
