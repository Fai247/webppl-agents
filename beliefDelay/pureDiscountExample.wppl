// We know that donuts are immediately pleasurable, but one timestep later have
// a bad aftertaste, and that vegetables are initially unpleasant, but one
// timestep later have a very good aftertaste. We know that either veg or noodle
// gives net positive reward, and that the other gives net zero reward, but
// don't know which is which. We also aren't sure how much the agent discounts,
// or whether the agent is naive, but we do know that the agent knows the true
// latent state of the MDP.

// We observe the agent start off near donut south, but go to donut north.
// we know that the agent knows the true state, so it must not be that the agent
// just likes donuts. Instead, the solution is that the agent likes veg, but is
// a naive hyperbolic discounter, and aimed to go to veg. However, when the
// agent got near veg, the immediate reward of donut and the immediate negative
// reward of veg became very salient, and the later negative reward of donut
// and positive reward of veg looked small in comparison, so the agent changed
// its mind and went to donut instead.

// we also can conclude that the agent is probably not an extreme
// discounter, since they would go to donut south if they were, but it's within
// the realm of possibility because of softmax noise.

// WORLD PARAMS

var gridworldMDP = makeDonutWorld2({big: true, maxTimeAtRestaurant : 2});
var world = makeGridworldPOMDP(gridworldMDP);

// possible latent states

var trueLatentState = {'Donut N': true,
		       'Donut S': true,
		       'Veg': true,
		       'Noodle': true};

// start state
var startState = {manifestState: {loc: [3,1],
				  terminateAfterAction: false,
				  timeLeft: 14},
		  latentState: trueLatentState};

// POTENTIAL AGENT PARAMS

// possible utility functions
var donutUtilityTable = {
    'Donut N' : [20, -10],
    'Donut S' : [20, -10],
    'Veg'   : [-10, 10],
    'Noodle': [0, 0]
}

var vegUtilityTable = {
    'Donut N' : [10, -10],
    'Donut S' : [10, -10],
    'Veg'   : [-10, 20],
    'Noodle': [0, 0]
};

// the agent's prior
var agentPrior = getPriorBeliefGridworld( startState.manifestState,
					  function(){
					    return trueLatentState;
					  });

// PRIORS OVER AGENT PARAMS

var agentUtilityTableSampler = function() {
  return uniformDraw([donutUtilityTable, vegUtilityTable]);
};

var agentDiscountSampler = function() {
  return uniformDraw([0, 1, 100]);
};

var agentSophisticationSampler = function() {
  return uniformDraw(['sophisticated', 'naive']);
};

var pomdpUtilityFromManifestUtility = function(utility) {
  return function (state) { 
    return utility(state.manifestState);
  };
};

// see what the agent does with the parameters we want to infer to check if we're right

// var agent = makeBeliefDelayAgent({
//   priorBelief : agentPrior,
//   utility : pomdpUtilityFromManifestUtility(makeDonutUtility(world, vegUtilityTable)),
//   // utility : pomdpUtilityFromManifestUtility(makeDonutUtility(world, donutUtilityTable)),
//   alpha : 100,
//   noDelays: false,
//   discount: 1,
//   sophisticatedOrNaive: 'sophisticated',
//   myopia: {on: false, bound: 0},
//   boundVOI: {on: false, bound: 0}
// }, world);


// var trajectory = simulateBeliefDelayAgent(startState, world, agent, 'states');

// var locs = map(function(state) {return state.manifestState.loc;}, trajectory);

// console.log(locs);
// ash();


// PATH WE CONDITION ON
var path = [[3,1], [3,2], [3,3], [3,4], [3,5], [2,5]];

// INFERENCE

var conditionOnPath = function (agent, path, world, startState) {
  var agentAct = agent.act;
  var agentUpdateBelief = agent.updateBelief;
  var priorBelief = agent.params.priorBelief;
  var transition = world.transition;
  var worldObserve = world.observe;
  var observe = getFullObserve(worldObserve);

  var shouldTerminate = function (manifestState) {
    return manifestState.terminateAfterAction;
  };
  
  var _conditionOnPath = function(state, priorBelief, action, i) {
    var observation = observe(state);

    var delay = 0;

    var belief = agentUpdateBelief(priorBelief, observation, action, delay);

    var newAction = sample(agentAct(belief, delay));

    if (shouldTerminate(state.manifestState) || i >= path.length) {
      return 0;
    } else {   
      var nextState = transition(state, newAction);

      condition(_.isEqual(nextState.manifestState.loc, path[i]));

      return _conditionOnPath(nextState, belief, newAction, i+1);
    }
  };

  var startAction = 'noAction';

  return _conditionOnPath(startState, priorBelief, startAction, 1);
};

var posterior = Enumerate(function () { 

  var agentUtilityTable = agentUtilityTableSampler();
  var discount = agentDiscountSampler();
  var sophistication = agentSophisticationSampler();
  var agent = makeBeliefDelayAgent({
    priorBelief : agentPrior,
    utility : pomdpUtilityFromManifestUtility(makeDonutUtility(world, agentUtilityTable)),
    alpha : 100,
    noDelays: discount===0,
    discount: discount,
    sophisticatedOrNaive: sophistication,
    myopia: {on: false, bound: 0},
    boundVOI: {on: false, bound: 0}
  }, world);

  conditionOnPath(agent, path, world, startState);

  return {
    utilityTable : agentUtilityTable,
    discount: discount,
    sophistication: sophistication
  };

});

printERP(posterior);
