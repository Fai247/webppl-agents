// like gridworldInferExamples1, but with rich priors, such that we need to use
// MCMC inference

// noReverse is an optimisation for gridworldMDP that makes inference run faster
var gridworldMDP = makeDonutWorld2({big: true, noReverse: false});
var gridworldPOMDP = makeGridworldPOMDP(gridworldMDP);
var feature = gridworldMDP.feature;
var transition = gridworldPOMDP.transition;
var observe = gridworldPOMDP.observe;

var donutUtilityTable = {'Donut N': 4,
			 'Donut S': 4,
			 'Veg': 1,
			 'Noodle': 0,
			 timeCost: -0.1};
 
var tableToUtilityFunction = function(table) {
  return function(state, action) {
    if (state.manifestState.dead) {
      return 0;
    }
    var stateFeatureName = feature(state.manifestState).name;
    if (stateFeatureName) {
      return table[stateFeatureName];
    } else {
      return table.timeCost;
    }
  };
};

var makeUninformedPrior = function(p){
  return Enumerate(function(){
    if (flip(p)) {
      return {'Donut N': true,
	      'Donut S': false,
	      'Veg': true,
	      'Noodle': true};
    } else {
      return {'Donut N': true,
	      'Donut S': true,
	      'Veg': true,
	      'Noodle': true};
    }
  });
};

var startState = {manifestState: {loc: [3,1],
				  dead: false,
				  timeLeft: 6,
				  digest: 1},
		  latentState: {'Donut N': true,
				'Donut S': true,
				'Veg': true,
				'Noodle': true}};

var agent = makeBeliefAgent({utility: tableToUtilityFunction(donutUtilityTable),
			     alpha: 100,
			     priorBelief: makeUninformedPrior(0.8)},
			    gridworldPOMDP);

var observedTrajectory = simulateBeliefAgent(startState, gridworldPOMDP, agent, 6,
					     'stateAction');
// GridWorld.draw(gridworldMDP, {trajectory: observedTrajectory});
// TODO: labels

var utilityTablePrior = Enumerate(function(){
  var donutUtility = uniformDraw(range(5));
  return {'Donut N': donutUtility,
	  'Donut S': donutUtility,
	  'Veg': uniformDraw(range(5)),
	  'Noodle': uniformDraw(range(5)),
	  'timeCost': -0.1};
});

// change this to a print statement for the code box
console.log(utilityTablePrior.support().length);


var factorOnSequence = function(currentBelief, index, observedStateAction, agent){
  if (index >= observedStateAction.length){ 
    return []; // no-op
  } else {
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    
    var state = observedStateAction[index][0];
    var observedAction = observedStateAction[index][1];

    var observation = observe(state);
    var nextBelief = agentUpdateBelief(state.manifestState, currentBelief, observation);
    var nextActionERP = agentAct(state.manifestState, nextBelief, 0);
    
    factor(nextActionERP.score([], observedAction));
    
    return factorOnSequence(nextBelief, index + 1, observedStateAction, agent);
  }
};

var inferredAgent = function(){
  return printERP(MCMC(function(){
    var utilityTable = sample(utilityTablePrior);
    var utility = tableToUtilityFunction(utilityTable);
    var agentProbDSClosed = uniformDraw([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]);
    var agentPrior = makeUninformedPrior(agentProbDSClosed);
    var agent = makeBeliefAgent({utility: utility,
				 alpha: 100,
				 priorBelief: agentPrior},
				gridworldPOMDP);
  
    factorOnSequence(agentPrior, 0, observedTrajectory,
		     agent);
    
    return {utilityTable: utilityTable,
	    agentProbDSClosed: agentProbDSClosed};
  }));
};

timeit(inferredAgent);
