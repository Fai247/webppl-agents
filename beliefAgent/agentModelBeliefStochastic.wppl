
var totalTime = 10;

// ---------------
// Defining the Bandits decision problem

// Pull arm0 or arm1
var actions = [0,1];

// use latent "armToPrize" mapping in
// state to determine which prize agent gets
var transition = function(state, action){
  return update(state, 
                {prize: sample(state.armToPrize[action]), 
                 timeLeft: state.timeLeft - 1,
                 terminateAfterAction: state.timeLeft == 2})
};

// var transition = function(state, action){
//   var prize = (state.timeLeft > 4 && action==1) ? 'zero' : sample(state.armToPrize[action]); 
//   return update(state, 
//                 {prize: prize,
//                  timeLeft: state.timeLeft - 1,
//                  terminateAfterAction: state.timeLeft == 2})
// };

// after pulling an arm, agent observes associated prize
var observe = function(state){return state.prize;};

var getBetaERP = function(p){return Enumerate(
  function(){return categorical([1-p,p],['zero','one'])})};

var fairERP = getBetaERP(.5);

var betterProb = .6;
var betterERP = getBetaERP(betterProb);
var worseERP = getBetaERP(1-betterProb);

var halfERP = deltaERP('half');


var startState = { prize: 'start',
                   timeLeft:totalTime, 
                   terminateAfterAction:false,
                   armToPrize: {0:halfERP, 1:betterERP}
                 };
                
// ---------------
// Defining the POMDP agent

// agent's preferences over prizes
var utility = function(state,action){
  var prizeToUtility = {zero:0, one:1, half:.5, start:0};
  return prizeToUtility[state.prize];
};

// Agent's prior prior includes possibility that arm1 has no prize
// (instead of champagne)
var alternativeStartState = update(startState, {armToPrize:{0:halfERP, 1:worseERP}});

var priorBelief = Enumerate(function(){
  return categorical( [.5, .5], [startState, alternativeStartState]);
});


// Agent's belief update: directly translates the belief update
// equation above

var updateBelief = dp.cache(function(belief, observation, action){
  return Enumerate(function(){
    var state = sample(belief);
    var predictedNextState = transition(state, action);
    var predictedObservation = observe(predictedNextState);
    condition(_.isEqual(predictedObservation, observation));
    return predictedNextState;
  });
});

var act = dp.cache(
  function(belief) {
    return Enumerate(function(){
      var action = uniformDraw(actions);
      var eu = expectedUtility(belief, action);
      factor(100 * eu);
      return action;
    });
  });

var expectedUtility = dp.cache(
  function(belief, action) {
    return expectation(
      Enumerate(function(){
	var state = sample(belief);
	var u = utility(state, action);
	if (state.terminateAfterAction) {
	  return u;
	} else {
	  var nextState = transition(state, action);
	  var nextObservation = observe(nextState);
	  var nextBelief = updateBelief(belief, nextObservation, action);            
	  var nextAction = sample(act(nextBelief));   
	  return u + expectedUtility(nextBelief, nextAction);
	}
      }));
  });


var simulate = function(startState, priorBelief) {
    
  var sampleSequence = function(state, priorBelief, action) {
    var observation = observe(state);
    var belief = action=='startAction' ? priorBelief : updateBelief(priorBelief, observation, action);
    var action = sample(act(belief));
    var output = [ [state,action] ];
    
      
      if (state.terminateAfterAction){
        return output;
      } else {   
        var nextState = transition(state, action);
        return output.concat(sampleSequence(nextState, belief, action));
      }
    };
  return sampleSequence(startState, priorBelief, 'startAction');
};


map( function(state_action){
  var previousPrize = state_action[0].prize;
  var nextAction = state_action[1];
  console.log(' -> Prize ' + previousPrize + '\n');
  console.log('Arm: ' + nextAction);
},  simulate(startState, priorBelief));

