
var totalTime = 9;
var useSpecialUpdate = true;

// ---------------
// Defining the Bandits decision problem

// Pull arm0 or arm1
var actions = [0,1];

// use latent "armToPrize" mapping in
// state to determine which prize agent gets
var transition = function(state, action){
  return update(state, 
                {prize: sample(state.armToPrize[action]), 
                 timeLeft: state.timeLeft - 1,
                 terminateAfterAction: state.timeLeft == 2})
};


// after pulling an arm, agent observes associated prize
var observe = function(state){return state.prize;};

var getBetaERP = function(p){return Enumerate(
  function(){return categorical([1-p,p],['zero','one'])})};

var fairERP = getBetaERP(.5);

var betterProb = .6;
var betterERP = getBetaERP(betterProb);
var worseERP = getBetaERP(1-betterProb);

var halfERP = deltaERP('half');


var actualERP = getBetaERP(.001);
var startState = { prize: 'start',
                   timeLeft:totalTime, 
                   terminateAfterAction:false,
                   armToPrize: {0:halfERP, 1:actualERP}
                 };
                
// ---------------
// Defining the POMDP agent

// agent's preferences over prizes
var utility = function(state,action){
  var prizeToUtility = {zero:0, one:1, half:.5, start:0};
  return prizeToUtility[state.prize];
};

// Agent's prior prior includes possibility that arm1 has no prize
// (instead of champagne)
var agentState1 = update(startState, {armToPrize:{0:halfERP, 1:betterERP}});
var agentState2 = update(startState, {armToPrize:{0:halfERP, 1:worseERP}});

var priorBelief = Enumerate(function(){
  return categorical( [.5, .5], [agentState1, agentState2]);
});


// Agent's belief update: directly translates the belief update
// equation above

var updateBelief = dp.cache(function(belief, observation, action){
  return Enumerate(function(){
    var state = sample(belief);
    var predictedNextState = transition(state, action);
    var predictedObservation = observe(predictedNextState);
    condition(_.isEqual(predictedObservation, observation));
    return predictedNextState;
  });
});

var specialUpdateBelief = dp.cache(function(belief, observation, action){
  return Enumerate(function(){
    var state = sample(belief);
    var armToPrizeERPs = state.armToPrize;
    var ERPForArm = armToPrizeERPs[action];
    factor( ERPForArm.score( [], observation ) ); // because the observation is same as the prize in this case
    return transition(state,action) // doesn't matter which prize we're at: only what we infer about armToPrize
  });
});

var updateBelief = useSpecialUpdate ? specialUpdateBelief : updateBelief;

var act = dp.cache(
  function(belief) {
    return Enumerate(function(){
      var action = uniformDraw(actions);
      var eu = expectedUtility(belief, action);
      factor(100 * eu);
      return action;
    });
  });

var expectedUtility = dp.cache(
  function(belief, action) {
    return expectation(
      Enumerate(function(){
	var state = sample(belief);
	var u = utility(state, action);
	if (state.terminateAfterAction) {
	  return u;
	} else {
	  var nextState = transition(state, action);
	  var nextObservation = observe(nextState);
	  var nextBelief = updateBelief(belief, nextObservation, action);            
	  var nextAction = sample(act(nextBelief));   
	  return u + expectedUtility(nextBelief, nextAction);
	}
      }));
  });


var simulate = function(startState, priorBelief) {
    
  var sampleSequence = function(state, priorBelief, action) {
    var observation = observe(state);
    var belief = action=='startAction' ? priorBelief : updateBelief(priorBelief, observation, action);
    var action = sample(act(belief));
    var output = [ [state,action] ];
    
      
      if (state.terminateAfterAction){
        return output;
      } else {   
        var nextState = transition(state, action);
        return output.concat(sampleSequence(nextState, belief, action));
      }
    };
  return sampleSequence(startState, priorBelief, 'startAction');
};


var displayTrajectory = function( trajectory ){
  var out = map( function(state_action){
    var previousPrize = state_action[0].prize;
    var nextAction = state_action[1];
    return [previousPrize, nextAction];
  }, trajectory);  

  var out = _.flatten(out);
  return out.slice(1,out.length-1);
};

var trajectory = simulate(startState, priorBelief);

console.log( JSON.stringify(displayTrajectory(trajectory)));
// agent should pick 1, get multiple zeros and then switch to
// half for rest of it. currently this is failing. 
