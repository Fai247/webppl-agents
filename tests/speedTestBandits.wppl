console.log('--- "IRL" bandit tests ---\n');

var getPriorBelief = function(numberOfTrials, armToPrizeERPThunk){
  
  var startManifestState = {timeLeft: numberOfTrials, 
                            loc: 'start', 
                            terminateAfterAction: false};
  
  return Enumerate( function(){
    var latentState = armToPrizeERPThunk();
    return buildState(startManifestState, latentState);
  });
};


var speedTestBandits = function(beliefOrBeliefDelay){
  console.log('\nSpeedtest on bandits for agent: ', beliefOrBeliefDelay);

  var getBandit = function(numberOfTrials){
    var armToPrizeERP = {
      0:deltaERP('a'),
      1:deltaERP('a'),
      2:deltaERP('a'),
      3:deltaERP('a')
    };

    return makeBandit({
      numberOfArms: 4,
      armToPrizeERP: armToPrizeERP,
      numberOfTrials: numberOfTrials
    });
  };
  
  var getAgent = function(numberOfTrials, bandit){

    var armToPrizeERPThunk = function(){
      var dist = function(){
	return categorical([0.02, 0.49, 0.49],
			   [deltaERP('a'), deltaERP('b'), deltaERP('c')]);
      };

      return {0:deltaERP('a'), 1:dist(), 2:dist(), 3:dist()};
    };
    var priorBelief = getPriorBelief( numberOfTrials, armToPrizeERPThunk);
    
    var baseAgentParams = {
      priorBelief: priorBelief,
      alpha: 100,
      noDelays: true,
      discount: 0,
      sophisticatedOrNaive: 'naive'
    };

    var agentParams = update(baseAgentParams, {priorBelief:priorBelief}); 
    var prizeToUtility = {a:5, b:10, c:-8}; 
    return makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			   prizeToUtility);
  };

  var testTime = function(numberOfTrials){
    
    var bandit = getBandit(numberOfTrials);
    var agent = getAgent(numberOfTrials, bandit);

    var thunk = function(){
      return simulate(bandit.startState, bandit.world, agent, 'actions');
    };

    var out = timeit(thunk);

    var actions = out.value.slice(0,3);
    if (numberOfTrials > 5){
      assert.ok( _.difference([1,2,3], actions).length === 0,
		 'fail belief bandit example 4');
    }
    // console.log('\n Perceived Time: ', numberOfTrials, '  (locs, timeit) ', trajectoryToLocations(out.value), 
    //             out.runtimeInMilliseconds);
    return out.runtimeInMilliseconds + ' ms';
  };

  var numberTrialsValues = [5,6,7];
  var runTimes = map( testTime, numberTrialsValues);
  console.log('[numberOfTrials, runTime]: ', zip(numberTrialsValues, runTimes) );
  console.log('----completed speed test for "irl" bandits' );
};  

var runSpeedTests = function(){
  map( speedTestBandits, ['belief', 'beliefDelay'] );
};


var banditGenerativeNoDelay = function(beliefOrBeliefDelay){
  console.log('\Run IRL Bandit Generative for agent: ', beliefOrBeliefDelay);

  // Prizes are [a,b] and agent prefers c > a > b

  // world params
  var options = {
    numberOfArms: 2,
    armToPrizeERP: {0:deltaERP('a'), 1:deltaERP('b')},
    numberOfTrials: 3
  };
  var numberOfTrials = options.numberOfTrials;
  var armToPrizeERP = options.armToPrizeERP;
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;

  // agent params
  var prizeToUtility = {a:10, b:5, c:100};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive'
  };

  var getTrajectory = function(priorBelief){
    var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
    var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    return simulate(startState, world, agent, 'actions');
  };

  // Agent knows armToPrizeERP, picks arm with best prize
  var priorBelief = getPriorBelief( numberOfTrials,
				    function(){return armToPrizeERP;});
  //  console.log('prior: ', sample(priorBelief), sample(priorBelief), sample(priorBelief)); ash();
  var trajectory = getTrajectory( priorBelief );
  assert.ok( trajectory[0] === 0, 'fail banditexample 1');

  // Agent has .5 chance on arm 1 having best prize c, and so tries 1 before switching to 0.
  var priorBelief = getPriorBelief( numberOfTrials, function(){
    return categorical( [.5, .5], [ armToPrizeERP,
				   {0:deltaERP('a'), 1:deltaERP('c')}]);
  });
  var trajectory = getTrajectory( priorBelief );
  
  assert.ok( trajectory[0] === 1 && trajectory[1] === 0, 'fail banditexample 2');


 
  
  // --------------------------------
  // Example: Each arm independently either gives a or b. Since b is better, agent keeps exploring to try
  // to get it.
  
  // world params
  var options = {numberOfArms: 4,
		 armToPrizeERP: {
		   0:deltaERP('a'),
		   1:deltaERP('a'),
		   2:deltaERP('a'),
		   3:deltaERP('a')},
		 numberOfTrials: 6};
  var numberOfTrials = options.numberOfTrials;
  var armToPrizeERP = options.armToPrizeERP;
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;

  // agent params
  var prizeToUtility = {a:5, b:10, c:-8};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive'
  };
  
  // Agent thinks each arm might offer c, and so tries them all (as c is so good -- and b is not that bad)
  // should the above comment be different?
  var priorBelief = getPriorBelief(numberOfTrials, function(){
    var dist = function(){
      return categorical([0.5, 0.5], [deltaERP('a'), deltaERP('b')]);
    };
    
    return {0:dist(), 1:dist(), 2:dist(), 3:dist()};
  });
  
  var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
  var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			      prizeToUtility);
  var trajectory = simulate(startState, world, agent, 'actions');
  
  var actions = trajectory.slice(0,4);
  assert.ok( _.difference(_.range(4),actions).length === 0,
	     'fail bandit example 3');



  console.log('passed BanditGenerativeNoDelay');
};
  

var banditGenerativeDelay = function(){
  var beliefOrBeliefDelay = 'beliefDelay';

  // world params
  var options = {
    numberOfArms: 4,
    armToPrizeERP: {
      0:deltaERP('a'),
      1:deltaERP('a'),
      2:deltaERP('a'),
      3:deltaERP('a')},
    numberOfTrials: 6
  };
  var numberOfTrials = options.numberOfTrials;
  var armToPrizeERP = options.armToPrizeERP;
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;
  
  // ----------
  // Discounting example: agent thinks arms other than 0 could have b, with u=10, or
  // c, with u= -8. Non discounter will explore but discounter will just take 0, which
  // is known to have utility 5. (All arms still yield prize a). 

  var priorBelief = getPriorBelief(numberOfTrials,
    function(){
      var dist = function(){
	return categorical([.02, 0.49, 0.49],
			   [deltaERP('a'), deltaERP('b'), deltaERP('c')]);
      };

      return {0:deltaERP('a'), 1:dist(), 2:dist(), 3:dist()};
    });

  var baseAgentParams = {
    priorBelief: priorBelief,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
  };
 
  var prizeToUtility = {a:5, b:10, c:-8}; 

  // No discounting
  var agentParams = update(baseAgentParams, {priorBelief:priorBelief});
  var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			      prizeToUtility);;
  var thunk = function(){
    return simulate(startState, world, agent, 'actions');
  };
  var out = timeit(thunk);
  var actions = out.value.slice(0,3);
  assert.ok(  _.difference([1,2,3], actions).length == 0, 'fail bandit example 4');
  
  console.log('\n No discounting: (actions, timeit) ', out.value,
  	      out.runtimeInMilliseconds);

  // Discounting
  var replaceParams = {discount:4, noDelays:false, priorBelief:priorBelief};
  var agentParams = update(baseAgentParams, replaceParams);
  var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			      prizeToUtility);;
  var thunk = function(){
    return simulate(startState, world, agent, 'actions');
  };
  var out = timeit(thunk);
  var actions = out.value.slice(0,3);
  assert.ok(  _.difference([0,0,0],actions).length == 0, 'fail bandit example 5');
  console.log( '\n Discounting: (actions, timeit) ', 
               out.value, out.runtimeInMilliseconds+' ms');

  // Myopic (faster than discounting)
  var testRewardMyopic = function(rewardMyopicBound,exploreOrNot){
    var replaceParams = {
      discount:0, 
      noDelays:false,
      sophisticatedOrNaive: 'naive',
      rewardMyopic:{bound:rewardMyopicBound}, 
      priorBelief:priorBelief
    };
    var agentParams = update(baseAgentParams, replaceParams);
    var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    var thunk = function(){
      return simulate(startState, world, agent, 'actions');
    };
    var out = timeit(thunk);
    var actions = out.value.slice(0,3);
    var prediction = exploreOrNot==='explore' ? [1,2,3] : [0,0,0]; 
    console.log( '\n RewardMyopic bound ' + rewardMyopicBound + ' (actions, timeit) ', 
                 out.value, out.runtimeInMilliseconds + ' ms');
    assert.ok(  _.difference(prediction,actions).length == 0,
		'fail bandit example rewardMyopic');
  };
  console.log( '\n REWARD-MYOPIC \n RewardMyopic should be faster than discount');
  testRewardMyopic(1,'not');
  testRewardMyopic(2,'not');
  testRewardMyopic(3,'explore');

  // updateMyopic (faster than discounting)
  var testUpdateMyopic = function(updateMyopicBound, exploreOrNot){
    var replaceParams = {
      alpha: 101,
      discount:0, 
      noDelays:false,
      sophisticatedOrNaive: 'naive',
      updateMyopic: {bound:updateMyopicBound}, 
      priorBelief: priorBelief,
      fastUpdateBelief: false
    };
    var agentParams = update(baseAgentParams, replaceParams);
    var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    var thunk = function(){
      return simulate(startState, world, agent, 'actions');
    };
    var out = timeit(thunk);
    var actions = out.value.slice(0,3);
    var prediction = exploreOrNot==='explore' ? [1,2,3] : [0,0,0];
    assert.ok(  _.difference(prediction,actions).length === 0, 'fail bandit example updateMyopic');
    console.log( '\n UpdateMyopic bound: ' + updateMyopicBound + ' (actions, timeit) ',
                 out.value, out.runtimeInMilliseconds + ' ms');
  };
  console.log('\n UPDATE-MYOPIC \n UpdateMyopic should be similar in runtime to discount');
  testUpdateMyopic(0,'not');
  testUpdateMyopic(1,'explore');

  console.log('\n Passed banditGenerativeDelay');
};

var banditGenerativeAll = function(){
  banditGenerativeNoDelay('belief');
  banditGenerativeNoDelay('beliefDelay');
  banditGenerativeDelay();
};


runSpeedTests();
banditGenerativeAll();
