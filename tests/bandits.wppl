/* jshint shadow: true, newcap: false, maxerr:500, sub:false, latedef:false */

console.log('--- Stringy "IRL" bandit tests ---\n');

var getPriorBelief = function(numberOfTrials, armToPrizeERPThunk,
			      beliefOrBeliefDelay){
  return _getPriorBelief(numberOfTrials, 'start', armToPrizeERPThunk);
};


var speedTestBandits = function(beliefOrBeliefDelay){
  console.log('\nSpeedtest on bandits for agent: ', beliefOrBeliefDelay);
  
  var simulate = getSimulateFunction(beliefOrBeliefDelay);

  var getBandit = function(numberOfTrials){
    var armToPrizeERP = {
      0:deltaERP('a'),
      1:deltaERP('a'),
      2:deltaERP('a'),
      3:deltaERP('a')
    };

    return makeBandit({
      numberOfArms: 4,
      armToPrizeERP: armToPrizeERP,
      numberOfTrials: numberOfTrials
    });
  };
  
  var getAgent = function(numberOfTrials, bandit){

    var armToPrizeERPThunk = function(){
      var dist = function(){
	return categorical([0.02, 0.49, 0.49],
			   [deltaERP('a'), deltaERP('b'), deltaERP('c')]);
      };

      return {0:deltaERP('a'), 1:dist(), 2:dist(), 3:dist()};
    };
    var priorBelief = getPriorBelief( numberOfTrials, armToPrizeERPThunk,
				      beliefOrBeliefDelay) ;
    
    var baseAgentParams = {
      priorBelief: priorBelief,
      alpha: 100,
      noDelays: true,
      discount: 0,
      sophisticatedOrNaive: 'naive',
      myopia: {on:false, bound:0},
      boundVOI: {on:false, bound:0}
    };

    var agentParams = update(baseAgentParams, {priorBelief:priorBelief}); 
    var prizeToUtility = {a:5, b:10, c:-8}; 
    return makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			   prizeToUtility);
  };

  var testTime = function(numberOfTrials){
    
    var bandit = getBandit(numberOfTrials);
    var agent = getAgent(numberOfTrials, bandit);

    var thunk = function(){
      return simulate(bandit.startState, bandit.world, agent, 'actions');
    };

    var out = timeit(thunk);

    var actions = out.value.slice(0,3);
    if (numberOfTrials > 5){
      assert.ok( _.difference([1,2,3], actions).length === 0,
		 'fail belief bandit example 4');
    }
    // console.log('\n Perceived Time: ', numberOfTrials, '  (locs, timeit) ', trajectoryToLocations(out.value), 
    //             out.runtimeInMilliseconds);
    return out.runtimeInMilliseconds + ' ms';
  };

  var numberTrialsValues = [5,6,7];
  var runTimes = map( testTime, numberTrialsValues);
  console.log('[numberOfTrials, runTime]: ', zip(numberTrialsValues, runTimes) );
  console.log('----completed speed test for "irl" bandits' );
};  

var runSpeedTests = function(){
  map( speedTestBandits, ['belief', 'beliefDelay'] );
};


var banditGenerativeNoDelay = function(beliefOrBeliefDelay){
  console.log('\Run stringy Bandit Generative for agent: ', beliefOrBeliefDelay);
  var simulate = getSimulateFunction(beliefOrBeliefDelay);

  // Prizes are [a,b] and agent prefers c > a > b

  // world params
  var options = {
    numberOfArms: 2,
    armToPrizeERP: {0:deltaERP('a'), 1:deltaERP('b')},
    numberOfTrials: 3
  };
  var numberOfTrials = options.numberOfTrials;
  var armToPrizeERP = options.armToPrizeERP;
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;

  // agent params
  var prizeToUtility = {a:10, b:5, c:100};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };

  var getTrajectory = function(priorBelief){
    var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
    var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    return simulate(startState, world, agent, 'actions');
  };

  // Agent knows armToPrizeERP, picks arm with best prize
  var priorBelief = getPriorBelief( numberOfTrials,
				    function(){return armToPrizeERP;},
				    beliefOrBeliefDelay);
//  console.log('prior: ', sample(priorBelief), sample(priorBelief), sample(priorBelief)); ash();
  var trajectory = getTrajectory( priorBelief );
  assert.ok( trajectory[0] === 0, 'fail banditexample 1');

  // Agent has .5 chance on arm 1 having best prize c, and so tries 1 before switching to 0. 
  var priorBelief = getPriorBelief( numberOfTrials, function(){
    return categorical( [.5, .5], [ armToPrizeERP,
				   {0:deltaERP('a'), 1:deltaERP('c')}]);
  }, beliefOrBeliefDelay);
  var trajectory = getTrajectory( priorBelief );
  
  assert.ok( trajectory[0] === 1 && trajectory[1] === 0, 'fail banditexample 2');

  
 
  
  // --------------------------------
  // Example: Each arm independently either gives a or b. Since b is better, agent keeps exploring to try
  // to get it.
  
  // world params
  var options = {numberOfArms: 4,
		 armToPrizeERP: {
		   0:deltaERP('a'),
		   1:deltaERP('a'),
		   2:deltaERP('a'),
		   3:deltaERP('a')},
		 numberOfTrials: 6};
  var numberOfTrials = options.numberOfTrials;
  var armToPrizeERP = options.armToPrizeERP;
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;

  // agent params
  var prizeToUtility = {a:5, b:10, c:-8};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };
  
  // Agent thinks each arm might offer c, and so tries them all (as c is so good -- and b is not that bad)
  // should the above comment be different?
  var priorBelief = getPriorBelief(numberOfTrials, function(){
    var dist = function(){
      return categorical([0.5, 0.5], [deltaERP('a'), deltaERP('b')]);
    };
    
    return {0:dist(), 1:dist(), 2:dist(), 3:dist()};
  }, beliefOrBeliefDelay);
  
  var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
  var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			      prizeToUtility);
  var trajectory = simulate(startState, world, agent, 'actions');
  
  var actions = trajectory.slice(0,4);
  assert.ok( _.difference(_.range(4),actions).length === 0,
	     'fail bandit example 3');


  // // Agent starts in state 0 and so gets an observation immediately. so it wont check 0 again
  // var startState = update(startState, {manifestState: update(startState.manifestState,{loc:0})});
  // var worldAndStart = update(worldAndStart, {startState:startState});
  // var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart, beliefOrBeliefDelay);
  // var trajectory = simulate(startState, world, agent,'states');
  
  // var locs = trajectoryToLocations(trajectory).slice(1,4);
  // assert.ok( _.difference([1,2,3],locs).length == 0, 'fail bandit example 3.5');
  console.log('passed BanditGenerativeNoDelay');
};
  

var banditGenerativeDelay = function(){
  var beliefOrBeliefDelay = 'beliefDelay';

  // world params
  var options = {
    numberOfArms: 4,
    armToPrizeERP: {
      0:deltaERP('a'),
      1:deltaERP('a'),
      2:deltaERP('a'),
      3:deltaERP('a')},
    numberOfTrials: 6
  };
  var numberOfTrials = options.numberOfTrials;
  var armToPrizeERP = options.armToPrizeERP;
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;
  
  // ----------
  // Discounting example: agent thinks arms other than 0 could have b, with u=10, or
  // c, with u= -8. Non discounter will explore but discounter will just take 0, which
  // is known to have utility 5. (All arms still yield prize a). 

  var priorBelief = getPriorBelief(numberOfTrials,
    function(){
      var dist = function(){
	return categorical([.02, 0.49, 0.49],
			   [deltaERP('a'), deltaERP('b'),deltaERP('c')]);
      };

      return {0:deltaERP('a'), 1:dist(), 2:dist(), 3:dist()};
    }, beliefOrBeliefDelay);

  var baseAgentParams = {
    priorBelief: priorBelief,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };
 
  var prizeToUtility = {a:5, b:10, c:-8}; 

  // No discounting
  var agentParams = update(baseAgentParams, {priorBelief:priorBelief});
  var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			      prizeToUtility);;
  var thunk = function(){
    return simulateBeliefDelayAgent(startState, world, agent, 'actions');
  };
  var out = timeit(thunk);
  var actions = out.value.slice(0,3);
  assert.ok(  _.difference([1,2,3], actions).length == 0, 'fail bandit example 4');
  
  console.log('\n No discounting: (actions, timeit) ', out.value,
	      out.runtimeInMilliseconds);

  // Discounting
  var replaceParams = {discount:4, noDelays:false, priorBelief:priorBelief};
  var agentParams = update(baseAgentParams, replaceParams);
  var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
			      prizeToUtility);;
  var thunk = function(){
    return simulateBeliefDelayAgent(startState, world, agent, 'actions');
  };
  var out = timeit(thunk);
  var actions = out.value.slice(0,3);
  assert.ok(  _.difference([0,0,0],actions).length == 0, 'fail bandit example 5');
  console.log( '\n Discounting: (actions, timeit) ', 
               out.value, out.runtimeInMilliseconds+' ms');

  // Myopia (faster than discounting)
  var testMyopia = function(myopiaBound,exploreOrNot){
    var replaceParams = {
      discount:0, 
      noDelays:false,
      sophisticatedOrNaive: 'naive',
      myopia:{on:true, bound:myopiaBound}, 
      priorBelief:priorBelief
    };
    var agentParams = update(baseAgentParams, replaceParams);
    var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    var thunk = function(){
      return simulateBeliefDelayAgent(startState, world, agent, 'actions');
    };
    var out = timeit(thunk);
    var actions = out.value.slice(0,3);
    var prediction = exploreOrNot==='explore' ? [1,2,3] : [0,0,0]; 
    console.log( '\n Myopia bound ' + myopiaBound + ' (actions, timeit) ', 
                 out.value, out.runtimeInMilliseconds + ' ms');
    assert.ok(  _.difference(prediction,actions).length == 0,
		'fail bandit example myopia');
  };
  console.log( '\n MYOPIA \n Myopia should be faster than discount');
  
  testMyopia(1,'not');
  testMyopia(2,'not');
  testMyopia(3,'explore');

  // BoundVOI (faster than discounting)
  var testBoundVOI = function(boundVOIBound, exploreOrNot){
    var replaceParams = {
      alpha: 101,
      discount:0, 
      noDelays:false,
      sophisticatedOrNaive: 'naive',
      boundVOI:{on:true, bound:boundVOIBound}, 
      priorBelief:priorBelief
    };
    var agentParams = update(baseAgentParams, replaceParams);
    var agent = makeBanditAgent(agentParams, bandit, beliefOrBeliefDelay,
				prizeToUtility);
    var thunk = function(){
      return simulateBeliefDelayAgent(startState, world, agent, 'actions');
    };
    var out = timeit(thunk);
    var actions = out.value.slice(0,3);
    var prediction = exploreOrNot==='explore' ? [1,2,3] : [0,0,0]; 
    assert.ok(  _.difference(prediction,actions).length == 0, 'fail bandit example boundVOI');
    console.log( '\n BoundVOI bound: ' + boundVOIBound + ' (actions, timeit) ',
                 out.value, out.runtimeInMilliseconds + ' ms');
  };
  console.log('\n BOUND VOI \n BoundVOI should be similar in runtime to discount');
  testBoundVOI(0,'not');
  testBoundVOI(1,'explore');
  testBoundVOI(1,'explore');

  console.log('\n Passed banditGenerativeDelay');
};

var banditGenerativeAll = function(){
  banditGenerativeNoDelay('belief');
  banditGenerativeNoDelay('beliefDelay');
  banditGenerativeDelay();
};


var testInferBandit = function(beliefOrBeliefDelay){
  console.log('started testInferBandit, beliefOrBeliefDelay? ', beliefOrBeliefDelay);
  
  // Prizes are [a,b]. If agent chooses 0, then they prefer 'a'. 
  
  // world params
  var options = {
    numberOfArms: 2,
    armToPrizeERP: {0:deltaERP('a'), 1:deltaERP('b')},
    numberOfTrials: 3
  };
  var numberOfArms = options.numberOfArms;
  var armToPrizeERP = options.armToPrizeERP;
  var numberOfTrials = options.numberOfTrials;
  var bandit = makeBandit(options);
  
  // agent params
  var baseAgentParams = noDiscountBaseAgentParams;

  var agentPrior = getPriorBelief(numberOfTrials, function(){
    return {0:deltaERP('a'), 1:deltaERP('b')};
  }, beliefOrBeliefDelay);
  var priorAgentPrior = deltaERP(agentPrior);
  var prior = {
    priorPrizeToUtility: Enumerate(function(){
      return uniformDraw( [{a:0, b:1}, {a:1, b:0} ] );
    }),
    
    priorAgentPrior: priorAgentPrior
  };
  
  // EXAMPLE 1
  var observedStateAction = [['start',1], ['b',1], ['b',1]]; 
  var latentState = armToPrizeERP;
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);

  // Test on two different inference functions
  var erp1 = inferBandit( bandit, baseAgentParams, prior,
			  fullObservedStateAction, 'trajectory', 10,
			  beliefOrBeliefDelay);
  var erp2 = inferBandit( bandit, baseAgentParams, prior,
			  fullObservedStateAction,'offPolicy', 10,
			  beliefOrBeliefDelay);
  assert.ok( sample(erp1).prizeToUtility.a === 0
	     && sample(erp2).prizeToUtility.a === 0, 'testbandit infer 1');

  // EXAMPLE 2
  var observedStateAction = [['start',0], ['a',0], ['a',0]]; 
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);

  var erp1 = inferBandit( bandit, baseAgentParams, prior,
			  fullObservedStateAction, 'trajectory', 10,
			  beliefOrBeliefDelay); 
  var erp2 = inferBandit( bandit, baseAgentParams, prior,
			  fullObservedStateAction,'offPolicy', 10,
			  beliefOrBeliefDelay);
  assert.ok( sample(erp1).prizeToUtility.a === 1
	     && sample(erp2).prizeToUtility.a === 1, 'testbandit infer 2');

  console.log('passed easy inferBandit tests');
  
  
  // EXAMPLE 3 - INFER PRIOR AND UTILITY
  
  // Two arms: {0:a, 1:c}. Agent stays at 0. 
  // Explanation: u(a) high and prior that 1:b is low.

  // True armToPrizeERP: 0:a, 1:c
  // True priorBelief:  0:a, 1:categorical([.05,.95],[b,c])
  // True utilities: {a:10, b:20, c:1}
  
  // world params
  var options = {
    numberOfArms: 2,
    armToPrizeERP: {0:deltaERP('a'), 1:deltaERP('c')},
    numberOfTrials: 5
  };
  var numberOfArms = options.numberOfArms;
  var armToPrizeERP = options.armToPrizeERP;
  var numberOfTrials = options.numberOfTrials;
  var bandit = makeBandit(options);
  
  // agent params
  var baseAgentParams = noDiscountBaseAgentParams;

  // Prior on agent's prizeToUtility
  var truePrizeToUtility = {a:10, b:20, c:1};
  var priorPrizeToUtility = Enumerate(function(){
    return {a: uniformDraw([0,3,10]), b:20, c:1};
  });
  
  // Prior on agent's prior
  var trueAgentPrior = getPriorBelief(numberOfTrials, function(){
    return {
      0:deltaERP('a'),
      1: categorical([.05, .95], [deltaERP('b'), deltaERP('c')])
    };
  }, beliefOrBeliefDelay);
  var falseAgentPrior = getPriorBelief(numberOfTrials, function(){
    return {
      0:deltaERP('a'),
      1: categorical([.5, .5], [deltaERP('b'), deltaERP('c')])
    };
  }, beliefOrBeliefDelay);

  var priorAgentPrior = Enumerate(function(){
    return flip() ? trueAgentPrior : falseAgentPrior;
  });
  
  var prior = {priorPrizeToUtility: priorPrizeToUtility, priorAgentPrior: priorAgentPrior};

  var latentState = armToPrizeERP;
  var observedStateAction = [['start',0], ['a',0], ['a',0], ['a',0], ['a',0]]; 
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);


  var out1 = timeit( function(){
    return inferBandit( bandit, baseAgentParams, prior, fullObservedStateAction,
			'trajectory', 10, beliefOrBeliefDelay);
  });
  var out2 = timeit( function(){
    return inferBandit( bandit, baseAgentParams, prior, fullObservedStateAction,
			'offPolicy', 10, beliefOrBeliefDelay);
  });
  console.log('\n Time for Example 3, 2 arms and time 5: [from states, offpolicy]', out1.runtimeInMilliseconds, out2.runtimeInMilliseconds);
  
  var testERP = function(erp){
    var out = sample(erp);
    assert.ok( out.prizeToUtility.a===10, 'testbandit inferbelief example 4' );
  };
  map(testERP,[out1.value,out2.value]);
  

  // He goes to 1 every time => u(a)==0=
  var observedStateAction = [['start',1], ['c',1], ['c',1], ['c',1], ['c',1]]; 
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);

  var erp1 = inferBandit( bandit, baseAgentParams, prior,
			  fullObservedStateAction, 'trajectory', 10,
			  beliefOrBeliefDelay); 
  var erp2 = inferBandit( bandit, baseAgentParams, prior,
			  fullObservedStateAction,'offPolicy', 10,
			  beliefOrBeliefDelay);

  var testERP = function(erp){
    var out = sample(erp);
    assert.ok( out.prizeToUtility.a == 0, 'testbandit inferbelief example 5' );
  };
  map(testERP,[erp1,erp2]);
 
  console.log('\n-----\npassed ALL inferBandit tests');
};

var runTestInferBandit = function(){
  testInferBandit('beliefDelay');
  testInferBandit('belief');
};

var speedTestOnly = function(){
  speedTestBandits('beliefDelay');
  console.log('\n\ BELIEF DELAY speedtest');
  speedTestBandits('beliefDelay');
};

var beliefAgentTests = function(){
  banditGenerativeNoDelay('belief');
  testInferBandit('belief');
  speedTestBandits('belief');
};


var beliefDelayAgentTests = function(){
  banditGenerativeNoDelay('beliefDelay');
  banditGenerativeDelay();
  testInferBandit('beliefDelay');   
  speedTestBandits('beliefDelay');
};


var stochasticTest1 = function() {
  // prizes are deterministic, and the agent knows this (as well as what the
  // prizes actually are). the agent should just pick the arm with the highest
  // utility prize.
  
  // params for world
  var options = {
    numberOfArms: 2,
    armToPrizeERP: {0: deltaERP(0), 1: deltaERP(1)},
    numberOfTrials: 3,
    numericalPrizes: true
  };
  var bandit = makeBandit(options);
  var world = bandit.world;
  var startState = bandit.startState;

  // agent parameters

  var prior = deltaERP(startState);

  var agentParams = {alpha: 100,
		     priorBelief: prior};

  var agent = makeBanditAgent(agentParams, bandit, 'belief');

  // agent's behaviour

  var trajectory = simulateBeliefAgent(startState, world, agent, 'actions');

  // check the agent is choosing the highest-utility arm

  assert.ok(_.isEqual(most(trajectory), [1,1]),
	    'agent is not choosing right arms in first stochastic test');

  console.log('passed first stochastic test');
};

var stochasticTest2 = function() {
  // there are two arms, one with a deterministic prize, one with a random
  // prize, and the agent knows this. No matter how long the time left is,
  // the agent should choose the highest utility arm, whether it be the sure
  // or the unsure arm.

  var uncertainArmRiskERP = Enumerate(function() {
    return uniformDraw([0, 10]);
  });

  var uncertainArmSafeERP = Enumerate(function() {
    return uniformDraw([0, 1.2]);
  });

  var shortRiskBandit = makeBandit({
    numberOfArms: 2,
    numberOfTrials: 3,
    armToPrizeERP: {0: deltaERP(1), 1: uncertainArmRiskERP},
    numericalPrizes: true
  });
  
  var shortSafeBandit = makeBandit({
    numberOfArms: 2,
    numberOfTrials: 3,
    armToPrizeERP: {0: deltaERP(1), 1: uncertainArmSafeERP},
    numericalPrizes: true
  });
  
  var longRiskBandit = makeBandit({
    numberOfArms: 2,
    numberOfTrials: 20,
    armToPrizeERP: {0: deltaERP(1), 1: uncertainArmRiskERP},
    numericalPrizes: true
  });

  var longSafeBandit = makeBandit({
    numberOfArms: 2,
    numberOfTrials: 20,
    armToPrizeERP: {0: deltaERP(1), 1: uncertainArmSafeERP},
    numericalPrizes: true
  });

  // agent parameters

  var priorShortRisk = deltaERP(shortRiskBandit.startState);

  var priorShortSafe = deltaERP(shortSafeBandit.startState);
  
  var priorLongRisk = deltaERP(longRiskBandit.startState);

  var priorLongSafe = deltaERP(longSafeBandit.startState);

  // simulating agents
  var riskyShortAgentParams = {utility: makeBanditUtility(shortRiskBandit),
			       alpha: 100,
			       priorBelief: priorShortRisk};

  var riskyShortAgent = makeBanditAgent(riskyShortAgentParams, shortRiskBandit,
					'belief');
  var riskyShortTrajectory = simulateBeliefAgent(shortRiskBandit.startState,
						 shortRiskBandit.world,
						 riskyShortAgent, 'actions');

  assert.ok(_.isEqual(riskyShortTrajectory[0], 1),
	    'agent not risky enough with short lifespan');

  var riskyLongAgentParams = {utility: makeBanditUtility(longRiskBandit),
  			      alpha: 100,
  			      priorBelief: priorLongRisk};

  var riskyLongAgent = makeBanditAgent(riskyLongAgentParams, longRiskBandit,
				       'belief');
  var riskyLongTrajectory = simulateBeliefAgent(longRiskBandit.startState,
						longRiskBandit.world,
  						riskyLongAgent, 'actions');

  assert.ok(_.isEqual(riskyLongTrajectory[0], 1),
  	    'agent not risky enough with long lifespan');

  var safeShortAgentParams = {utility: makeBanditUtility(shortSafeBandit),
  			      alpha: 100,
  			      priorBelief: priorShortSafe};

  var safeShortAgent = makeBanditAgent(safeShortAgentParams, shortSafeBandit,
				       'belief');
  var safeShortTrajectory = simulateBeliefAgent(shortSafeBandit.startState,
						shortSafeBandit.world,
  						safeShortAgent, 'actions');

  assert.ok(_.isEqual(safeShortTrajectory[0], 0),
  	    'agent not safe enough with short lifespan');

  var safeLongAgentParams = {utility: makeBanditUtility(longSafeBandit),
  			     alpha: 100,
  			     priorBelief: priorLongSafe};

  var safeLongAgent = makeBanditAgent(safeLongAgentParams, longSafeBandit,
				      'belief');
  var safeLongTrajectory = simulateBeliefAgent(longSafeBandit.startState,
					       longSafeBandit.world,
  					       safeLongAgent, 'actions');

  assert.ok(_.isEqual(safeLongTrajectory[0], 0),
  	    'agent not safe enough with long lifespan');

  console.log('passed second stochastic test');
};

var stochasticTest3 = function(){
  // have example where agent isn't sure what the probabilities for the second
  // arm are. if the lifetime is long enough, agent should explore, and stay on
  // that arm if good news, leave if not. if lifetime is short, agent should not
  // explore.

  // note that this test is nondeterministic - if the code is right, it should
  // always pass, but if the code is wrong, it might pass anyway sometimes.


  // parameters for world
  
  var luckyArmToPrizeERP = {0: deltaERP(1),
			    1: categoricalERP([0.2, 0.8], [0, 1.5])};

  var unluckyArmToPrizeERP = {0: deltaERP(1),
		       1: categoricalERP([0.8, 0.2], [0, 1.5])};

  var shortBandit = makeBandit({
    numberOfTrials: 2,
    numberOfArms: 2,
    armToPrizeERP: luckyArmToPrizeERP,
    numericalPrizes: true
  });

  var longLuckyBandit = makeBandit({
    numberOfTrials: 12,
    numberOfArms: 2,
    armToPrizeERP: luckyArmToPrizeERP,
    numericalPrizes: true
  });

  var longUnluckyBandit = makeBandit({
    numberOfTrials: 12,
    numberOfArms: 2,
    armToPrizeERP: unluckyArmToPrizeERP,
    numericalPrizes: true
  });

  // parameters for agent
  
  var agentArmToPrizeERPPrior = function() {
    return uniformDraw([luckyArmToPrizeERP, unluckyArmToPrizeERP]);
  };

  var shortPrior = {manifestState: shortBandit.startState.manifestState,
		    latentStateERP: agentArmToPrizeERPPrior};

  var longPrior = {manifestState: longLuckyBandit.startState.manifestState,
		   latentStateERP: agentArmToPrizeERPPrior};

  // simulating agent in short environment
  
  var agentParamsShort = {alpha: 100,
			  priorBelief: shortPrior};

  var agentShort = makeBanditAgent(agentParamsShort, shortBandit, 'belief');

  var trajectoryShort = simulateBeliefAgent(shortBandit.startState,
					    shortBandit.world, agentShort,
					    'actions');

  assert.ok(trajectoryShort[0] === 0, 'agent explores when it shouldnt');

  // simulating agent in long environments

  var agentParamsLong = {alpha: 100,
			 priorBelief: longPrior};

  var agentLong = makeBanditAgent(agentParamsLong, longLuckyBandit, 'belief');

  var trajectoryLongLucky = simulateBeliefAgent(longLuckyBandit.startState,
						longLuckyBandit.world,
						agentLong, 'stateAction');

  assert.ok(trajectoryLongLucky[0][1] === 1, 'agent doesnt explore enough');

  if (trajectoryLongLucky[1][0] === 1.5) {
    assert.ok(trajectoryLongLucky[1][1] === 1, 'agent should follow up luck');
  }

  var trajectoryLongUnlucky = simulateBeliefAgent(longUnluckyBandit.startState,
						  longUnluckyBandit.world,
						  agentLong, 'stateAction');
  
  if (trajectoryLongUnlucky[1][0] === 0) {
    assert.ok(trajectoryLongUnlucky[1][1] === 0,
	      'agent should learn from bad luck');
  }

  console.log('passed third stochastic test');
};

// console.log('\n\n------\n  BELIEF DELAY \n');
// beliefDelayAgentTests();
// console.log('\n\n------\n  BELIEF \n');
// beliefAgentTests();
// console.log('\n\n-----------\n ALL STRINGY "IRL" BANDIT TESTS PASSED');

console.log('\n\n-----------\n BEGINNING STOCHASTIC NUMERICAL BANDIT TESTS \n-----------');
stochasticTest1();
stochasticTest2();
// stochasticTest3();
console.log('\n\n-----------\n ALL STOCHASTIC NUMERICAL BANDIT TESTS PASSED');
null;




