// test exponential discounting for hyperbolic and beliefDelay agent
// takes approximately 1 minute to run

// in gridworld, exponential discounter should never go to donut north.
// this test is probabilistic in the discount: it always passes when exponential
// discounting is correctly implemented, but will sometimes pass when it is not.
var test1 = function() {
  var startState = { 
    loc : [3,0],
    terminateAfterAction : false,
    timeLeft : 13
  };

  var world = makeRestaurantChoiceMDP({noReverse:false});

  var makeRestaurantUtilityFunction = function (world, rewards) { 
    return function(state, action) {
      var getFeature = world.feature;
      var feature = getFeature(state);
      if (feature.name) { return rewards[feature.name][state.timeAtRestaurant]; }
      return rewards.timeCost;
    };
  };

  var restaurantUtility = makeRestaurantUtilityFunction(world, {
    'Donut N' : [10, -10],
    'Donut S' : [10, -10],
    'Veg'   : [-10, 20],
    'Noodle': [0, 0],
    'timeCost': -.01
  });

  var gamma = uniform(0, 1);

  var exponentialDiscount = function(delay) {
    return Math.pow(gamma, delay);
  };
  
  var baseAgentParams = {
    utility : restaurantUtility,
    alpha : 5000, 
    discount : 0,
    discountFunction: exponentialDiscount
  };

  var naiveAgent = makeMDPAgent( 
    update(baseAgentParams, {sophisticatedOrNaive: 'naive'}), 
    world
  );

  var sophisticatedAgent = makeMDPAgent( 
    update(baseAgentParams, {sophisticatedOrNaive: 'sophisticated'}), 
    world
  );

  var naiveTrajectoryDistribution = Infer({ method: 'enumerate' }, function(){
    return simulateMDP(startState, world, naiveAgent, 'states');
  });

  var mostLikelyNaiveTrajectory = MAP(naiveTrajectoryDistribution).val;
  
  var sophisticatedTrajectoryDistribution = Infer({ method: 'enumerate' }, function(){
    return simulateMDP(startState, world, sophisticatedAgent, 'states');
  });

  var mostLikelySophisticatedTrajectory = MAP(sophisticatedTrajectoryDistribution).val;

  var goesToDonutN = function(trajectory){
    return any(function(state){return _.isEqual(state.loc, [2,5]);},
	       trajectory);
  };

  assert.ok(!goesToDonutN(mostLikelyNaiveTrajectory)
	    && !goesToDonutN(mostLikelySophisticatedTrajectory),
	    'exponential discounter should never go to donut north, test 1 fails with gamma = ' + gamma);
};

// trajectories of naive and sophisticated agent should be identical. this test
// is also probabilistic in the same sense as the previous test.

var simulateUntilManyLikelyActions = function(startState, world, agent) {
  var act = agent.act;
  var expectedUtility = agent.expectedUtility;
  var transition = world.transition;
  var stateToActions = world.stateToActions;

  var manyLikelyActions = function(state) {
    var availableActions = stateToActions(state);
    var actProbs = map(function(action){return act(state).score(action);},
		       availableActions);
    var positiveActProbs = filter(function(prob){return prob > 0.01;},
				  actProbs);
    return positiveActProbs.length > 1;
  };

  
  var sampleSequence = function (state) {
    var delay = 0;
    var action = sample(act(state, delay));
    var nextState = transition(state, action); 
    var out = action;
    return state.terminateAfterAction || manyLikelyActions(state) ?
      [out] : [out].concat(sampleSequence(nextState));
  };
  return sampleSequence(startState);
};


var test2 = function() {
  var startState = { 
    loc : [3,0],
    terminateAfterAction : false,
    timeLeft : 13
  };

  var world = makeRestaurantChoiceMDP({noReverse:false});

  var makeRestaurantUtilityFunction = function (world, rewards) { 
    return function(state, action) {
      var getFeature = world.feature;
      var feature = getFeature(state);
      if (feature.name) { return rewards[feature.name][state.timeAtRestaurant]; }
      return rewards.timeCost;
    };
  };

  var restaurantUtility = makeRestaurantUtilityFunction(world, {
    'Donut N' : [10, -10],
    'Donut S' : [10, -10],
    'Veg'   : [-10, 20],
    'Noodle': [0, 0],
    'timeCost': -.01
  });

  var gamma = uniform(0.05,1);
  
  var exponentialDiscount = function(delay) {
    return Math.pow(gamma, delay);
  };
  
  var baseAgentParams = {
    utility : restaurantUtility,
    alpha : 500000, 
    discount : 0,
    discountFunction: exponentialDiscount
  };

  var naiveAgent = makeMDPAgent( 
    update(baseAgentParams, {sophisticatedOrNaive: 'naive'}), 
    world
  );

  var sophisticatedAgent = makeMDPAgent( 
    update(baseAgentParams, {sophisticatedOrNaive: 'sophisticated'}), 
    world
  );

  var naiveTrajectory = simulateUntilManyLikelyActions(startState, world, naiveAgent,
					   'states');
  
  var sophisticatedTrajectory = simulateUntilManyLikelyActions(startState, world,
						   sophisticatedAgent,
						   'states');

  assert.ok(_.isEqual(naiveTrajectory, sophisticatedTrajectory),
	    'trajectories of naive and sophisticated agents should be equal, test 2 fails');
};

// exponential discounter should not explore in bandits, unless discount factor
// is small
var test3 = function(){

  var getPriorBelief = function(numberOfTrials, armToPrizeDistThunk){
    
  var startManifestState = {timeLeft: numberOfTrials, 
                            loc: 'start', 
                            terminateAfterAction: false};
    
    return Infer({ method: 'enumerate' },  function(){
      var latentState = armToPrizeDistThunk();
      return buildState(startManifestState, latentState);
    });
  };
  
  var options = {
    numberOfArms: 2,
    armToPrizeDist: {
      0: Delta({ v: 0.5 }),
      1: Delta({ v: 1 })
    },
    numberOfTrials: 3,
    numericalPrizes: true
  };
  var armToPrizeDist = options.armToPrizeDist;
  var numberOfTrials = options.numberOfTrials;
  var bandit = makeBanditPOMDP(options);
  var world = bandit.world;
  var startState = bandit.startState;

  var armToPrizeDistSampler = function() {
    return {
      0: Delta({ v: 0.5 }),
      1: categorical([0.6, 0.4], [Delta({ v: 0 }), Delta({ v: 1 })])
    };
  };

  var agentPrior = getPriorBelief(numberOfTrials, armToPrizeDistSampler);

  var lightDiscount = function(delay) {
    return Math.pow(0.8, delay);
  };
  
  var lightDiscounterParams = { 
    alpha: 100000,
    priorBelief: agentPrior,
    discount: 0,
    discountFunction: lightDiscount,
    sophisticatedOrNaive: 'naive',
    noDelays: false
  };

  var lightDiscounter = makeBanditAgent(lightDiscounterParams, bandit,
					'beliefDelay');
  
  var lightDiscounterTrajectory = simulatePOMDP(startState, world,
					        lightDiscounter,
					        'actions');

  assert.ok(lightDiscounterTrajectory[0] === 1, 'discounting agent with large discount factor should explore but does not, test 3 fails');


  var heavyDiscount = function(delay) {
    return Math.pow(0.4, delay);
  };
  
  var heavyDiscounterParams = { 
    alpha: 100000,
    priorBelief: agentPrior,
    discount: 0,
    discountFunction: heavyDiscount,
    sophisticatedOrNaive: 'naive',
    noDelays: false
  };

  var heavyDiscounter = makeBanditAgent(heavyDiscounterParams, bandit,
					'beliefDelay');
  
  var heavyDiscounterTrajectory = simulatePOMDP(startState, world,
					        heavyDiscounter,
					        'actions');

  assert.ok(heavyDiscounterTrajectory[0] === 0, 'discounting agent with small discount factor should not explore but does, test 3 fails');
  
  console.log('test 3 passes');
};

repeat(10, test1);
console.log('test 1 passes');
repeat(10, test2);
console.log('test 2 passes');
test3();
console.log('all tests of exponential discounting agents pass');
