/* jshint shadow: true, newcap: false, maxerr:500, sub:false, latedef:false */


// from thunk to prior only on latents
var getPriorBelief = function(perceivedTotalTime, armToPrizeThunk){
  return Enumerate(armToPrizeThunk);
};


var speedTestIRLBandits = function(beliefOrBeliefDelay){
  console.log('\n Speedtest on IRL bandits for agent: ', beliefOrBeliefDelay);
  
  var simulate = getSimulateFunction(beliefOrBeliefDelay);

  var getWorld = function(perceivedTotalTime){
    var numArms = 4;
    var armToPrize = {0:'a', 1:'a', 2:'a', 3:'a'};
    return makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  };
  
  var getAgent = function(perceivedTotalTime, worldAndStart){

    var armToPrizeThunk = function(){
      var dist = function(){return categorical([.02, 0.49, 0.49], ['a', 'b','c']);};
      return {0:'a', 1:dist(), 2:dist(), 3:dist()};
    };
    var priorBelief = getPriorBelief( perceivedTotalTime, armToPrizeThunk) ;
    
    var baseAgentParams = {
      priorBelief: priorBelief,
      alpha: 100,
      noDelays: true,
      discount: 0,
      sophisticatedOrNaive: 'naive',
      myopia: {on:false, bound:0},
      boundVOI: {on:false, bound:0}
    };

    var agentParams = update(baseAgentParams, {priorBelief:priorBelief}); 
    var prizeToUtility = {a:5, b:10, c:-8}; 
    return makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart, beliefOrBeliefDelay);
  };

  var testTime = function(perceivedTotalTime){
    var worldAndStart = getWorld(perceivedTotalTime);
    var agent = getAgent(perceivedTotalTime, worldAndStart);
    var thunk = function(){return simulate(worldAndStart.startState, worldAndStart.world,
                                           agent, worldAndStart.actualTotalTime, 'states');};
    var out = timeit(thunk);
    var locs = trajectoryToLocations(out.value).slice(1,4);
    if (perceivedTotalTime > 5){
      assert.ok(  _.difference([1,2,3],locs).length == 0, 'fail belief bandit example 4');
    }
    // console.log('\n Perceived Time: ', perceivedTotalTime, '  (locs, timeit) ', trajectoryToLocations(out.value), 
    //             out.runtimeInMilliseconds);
    return out.runtimeInMilliseconds + ' ms';
  };
  var timeValues = [5,6,7];
  var runTimes = map( testTime, timeValues);
  console.log('\n\n [perceivedTime, runTime]: ', zip(timeValues, runTimes) );
  console.log('\n completed speed test irlBandits' )
};  

var runSpeedTests = function(){
  map( speedTestIRLBandits, ['belief', 'beliefDelay'] );
};




var runIRLBanditExamples = function(){

  // Prizes are [a,b] and agent prefers c > a > b
  
  // world params
  var numArms = 2;
  var armToPrize = {0:'a', 1:'b'};
  var perceivedTotalTime = 3;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  var world = worldAndStart.world;
  var startState = worldAndStart.startState;

  // agent params
  var prizeToUtility = {a:10, b:5, c:100};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };

  var getTrajectory = function(priorBelief){
    var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
    var agent = makeIRLBanditAgent(prizeToUtility, agentParams, worldAndStart);
    return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');
  };

  // Agent knows armToPrize, picks arm with best prize
  var trajectory = getTrajectory( Enumerate(function(){return armToPrize;}));
  assert.ok( trajectory[1].manifestState.loc == 0, 'fail banditexample 1');

  // Agent has .5 chance on arm 1 having best prize c, and so tries 1 before switching to 0. 
  var trajectory = getTrajectory(Enumerate(
    function(){
      return categorical( [.5, .5], [armToPrize, {0:'a', 1:'c'}]);
    }));
  assert.ok( trajectory[1].manifestState.loc == 1 && trajectory[2].manifestState.loc == 0, 'fail banditexample 2');
  

  
  
  // --------------------------------
  // Example: Each arm independently either gives a or b. Since b is better, agent keeps exploring to try
  // to get it.
  
  // world params
  var numArms = 4;
  var armToPrize = {0:'a', 1:'a', 2:'a', 3:'a'};
  var perceivedTotalTime = 6;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  var world = worldAndStart.world;
  var startState = worldAndStart.startState;

  // agent params
  var prizeToUtility = {a:5, b:10, c:-8};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };
  
  // Agent thinks each arm might offer c, and so tries them all (as c is so good -- and b is not that bad)  
  var priorBelief = Enumerate(
    function(){
      var dist = function(){return categorical([0.5, 0.5], ['a','b']);};
      return {0:dist(), 1:dist(), 2:dist(), 3:dist()};
    });
  
  var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var trajectory = simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');
  
  var locs = trajectoryToLocations(trajectory).slice(1,5);
  assert.ok( _.difference(_.range(4),locs).length == 0, 'fail bandit example 3');


  // Agent starts in state 0 and so gets an observation immediately. so it wont check 0 again
  var startState = update(startState, {manifestState: update(startState.manifestState,{loc:0})});
  var worldAndStart = update(worldAndStart, {startState:startState});
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var trajectory = simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');
  
  var locs = trajectoryToLocations(trajectory).slice(1,4);
  assert.ok( _.difference([1,2,3],locs).length == 0, 'fail bandit example 3.5');
 
  
 
  
  // ----------
  // Discounting example: agent thinks arms other than 0 could have b, with u=10, or
  // c, with u= -8. Non discounter will explore but discounter will just take 0, which
  // is known to have utility 5. (All arms still yield prize a). 

  var priorBelief = Enumerate(
    function(){
      var dist = function(){return categorical([.02, 0.49, 0.49], ['a', 'b','c']);};
      return {0:'a', 1:dist(), 2:dist(), 3:dist()};
    });

  var baseAgentParams = {
    priorBelief: priorBelief,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };
 
  var prizeToUtility = {a:5, b:10, c:-8}; 

  // No discounting
  var agentParams = update(baseAgentParams, {priorBelief:priorBelief});
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var thunk = function(){return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');};
  var out = timeit(thunk);
  var locs = trajectoryToLocations(out.value).slice(1,4);
  assert.ok(  _.difference([1,2,3],locs).length == 0, 'fail bandit example 4');
  
  console.log('\n No discounting: (locs, timeit) ', trajectoryToLocations(out.value), out.runtimeInMilliseconds);

  // Discounting
  var replaceParams = {discount:4, noDelays:false, priorBelief:priorBelief};
  var agentParams = update(baseAgentParams, replaceParams);
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var thunk = function(){return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');};
  var out = timeit(thunk);
  var locs = trajectoryToLocations(out.value).slice(1,4);
  assert.ok(  _.difference([0,0,0],locs).length == 0, 'fail bandit example 5');
  console.log( '\n Discounting: (locs, timeit) ', 
               trajectoryToLocations(out.value), out.runtimeInMilliseconds+' ms');

  // Myopia (faster than discounting)
  var testMyopia = function(myopiaBound,exploreOrNot){
    var replaceParams = {
      discount:0, 
      noDelays:false,
      sophisticatedOrNaive: 'naive',
      myopia:{on:true, bound:myopiaBound}, 
      priorBelief:priorBelief
    };
    var agentParams = update(baseAgentParams, replaceParams);
    var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
    var thunk = function(){return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');};
    var out = timeit(thunk);
    var locs = trajectoryToLocations(out.value).slice(1,4);
    var prediction = exploreOrNot==='explore' ? [1,2,3] : [0,0,0]; 
    console.log( '\n Myopia bound ' + myopiaBound + ' (locs, timeit) ', 
                 trajectoryToLocations(out.value), out.runtimeInMilliseconds + ' ms');
    assert.ok(  _.difference(prediction,locs).length == 0, 'fail bandit example myopia');
  };
  console.log( '\n MYOPIA \n Myopia should be faster than discount')
  testMyopia(1,'not');
  testMyopia(2,'not');
  testMyopia(3,'explore');

  // BoundVOI (faster than discounting)
  var testBoundVOI = function(boundVOIBound, exploreOrNot){
    var replaceParams = {
      discount:0, 
      noDelays:false,
      sophisticatedOrNaive: 'naive',
      boundVOI:{on:true, bound:boundVOIBound}, 
      priorBelief:priorBelief
    };
    var agentParams = update(baseAgentParams, replaceParams);
    var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
    var thunk = function(){return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');};
    var out = timeit(thunk);
    var locs = trajectoryToLocations(out.value).slice(1,4);
    var prediction = exploreOrNot==='explore' ? [1,2,3] : [0,0,0]; 
    assert.ok(  _.difference(prediction,locs).length == 0, 'fail bandit example boundVOI');
    console.log( '\n BoundVOI bound: ' + boundVOIBound + ' (locs, timeit) ', trajectoryToLocations(out.value), out.runtimeInMilliseconds + ' ms');
  };
  console.log('\n BOUND VOI \n BoundVOI should be similar in runtime to discount');
  testBoundVOI(1,'not');
  testBoundVOI(2,'explore');
  testBoundVOI(2,'explore');

  console.log('\n All runIRLBanditExamples passed');
};



var testInferIRLBandit = function(beliefOrBeliefDelay){
  // Prizes are [a,b]. If agent chooses 0, then they prefer 'a'. 
  
  // world params
  var numArms = 2;
  var armToPrize = {0:'a', 1:'b'};
  var perceivedTotalTime = 3;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  
  // agent params
  var baseAgentParams = noDiscountBaseAgentParams;

  var prior = {
    priorPrizeToUtility: Enumerate(function(){
      return categorical( [.5, .5], [{a:0, b:1}, {a:1, b:0} ] );
    }),
    
    priorAgentPrior: Enumerate(function(){return deltaERP({0:'a', 1:'b'});})
  };

  // EXAMPLE 1
  var observedStateAction = [['start',1], [1,1], [1,1]]; 
  var latentState = armToPrize;
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);
  
  // Test on two different inference functions
  var erp1 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10); 
  var erp2 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction,'offPolicy', 10);
  assert.ok( sample(erp1).prizeToUtility.a == 0 && sample(erp2).prizeToUtility.a == 0, 'testbandit infer 1');
  
  
  // EXAMPLE 2
  var observedStateAction = [['start',0], [0,0], [0,0]]; 
  var observedStates = map(first, observedStateAction);
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);

  var erp1 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10); 
  var erp2 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction,'offPolicy', 10);
  assert.ok( sample(erp1).prizeToUtility.a == 1 && sample(erp2).prizeToUtility.a == 1, 'testbandit infer 2');

  console.log('passed easy testBanditInferBeliefOnly tests');
  
  
  // EXAMPLE 3 - INFER PRIOR AND UTILITY
  
  // Two arms: {0:a, 1:c}. Agent stays at 0. 
  // Explanation: u(a) high and prior that 1:b is low.

  // True armToPrize: 0:a, 1:c
  // True priorBelief:  0:a, 1:categorical([.05,.95],[b,c])
  // True utilities: {a:10, b:20, c:1}
  
  // world params
  var numArms = 2;
  var armToPrize = {0:'a', 1:'c'};
  var perceivedTotalTime = 5;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  
  // agent params
  var baseAgentParams = noDiscountBaseAgentParams;

  // Prior on agent's prizeToUtility
  var truePrizeToUtility = {a:10, b:20, c:1};
  var priorPrizeToUtility = Enumerate(function(){
    return {a: uniformDraw([0,3,10]), b:20, c:1};
  });
  
  // Prior on agent's prior
  var trueAgentPrior = Enumerate(function(){
    return {0:'a', 1: categorical([.05, .95], ['b','c']) };
  });
  var falseAgentPrior = Enumerate(function(){
    return {0:'a', 1: categorical([.5, .5], ['b','c']) };
  });

  var priorAgentPrior = Enumerate(function(){
    return flip() ? trueAgentPrior : falseAgentPrior;
  });
  
  var prior = {priorPrizeToUtility: priorPrizeToUtility, priorAgentPrior: priorAgentPrior};

  var latentState = armToPrize;
  var observedStateAction = [['start',0], [0,0], [0,0], [0,0], [0,0]]; 
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);

  
  var out1 = timeit( function(){return inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10, beliefOrBeliefDelay)});
  var out2 = timeit( function(){return inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'offPolicy', 10, beliefOrBeliefDelay)});
  console.log('\n Time for Example 3, 2 arms and time 5: [from states, offpolicy]', out1.runtimeInMilliseconds, out2.runtimeInMilliseconds);
  var testERP = function(erp){
    var out = sample(erp);
    assert.ok( out.prizeToUtility.a==10  && _.isFinite( out.priorBelief.score([], armToPrize)),
               'testbandit inferbelief example 4' );
  };
  map(testERP,[out1.value,out2.value]);


  // He goes to 1 every time => u(a)==0=
  var observedStateAction = [['start',1], [1,1], [1,1], [1,1], [1,1]]; 
  var fullObservedStateAction = stateActionPairsToFullStates(observedStateAction, latentState);

  var erp1 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10); 
  var erp2 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction,'offPolicy', 10);

  var testERP = function(erp){
    var out = sample(erp);
    assert.ok( out.prizeToUtility.a == 0, 'testbandit inferbelief example 5' );
  };
  map(testERP,[erp1,erp2]);
 
  console.log('\n-----\npassed ALL testBanditInfer tests');
};

var runTestInferIRLBandit = function(){
  testInferIRLBandit('beliefDelay');
  testInferIRLBandit('belief');
};




var testBoundVOI = function(){

  // Three paths:
  // There are three paths from 'start', each of length 2.
  // 1. *immediate*: Immediate payoff (which is low).
  // 2. *certain*: Delayed payoff 2nd step (no prior uncertainty). Better than 1. 
  // 3. *eitherAorB*: Payoff that is delayed and depends on choosing between A and B, based
  // on observation at previous timestep. Highest payoff

  // When boundVOI==1, agent will not take superior *eitherAorB* option. 
  
  
  var makeThreePathsWorldAndStart = function(perceivedTime, noisyObserve){
    var transitionTable = {
      start: { immediate0: 'immediate0',
               certain0: 'certain0',
               eitherAorB_0: 'eitherAorB_0' },
      
      immediate0: { immediate1: 'immediate1'},
      
      certain0: {certain1: 'certain1'},
      
      eitherAorB_0: { eitherAorB_A: 'eitherAorB_A', 
                      eitherAorB_B: 'eitherAorB_B'}, 
      
      immediate1: {start: 'start'},
      certain1: {start: 'start'},
      eitherAorB_A: {start: 'start'},
      eitherAorB_B: {start: 'start'}
    };
    
    
    var transition = function(state, action){
      var newTimeLeft = state.manifestState.timeLeft - 1;
      var dead = (newTimeLeft ==1);
      var newLoc = action;
      assert.ok(newLoc === transitionTable[state.manifestState.loc][action], 'transition' );
      
      var newManifestState = {loc:newLoc, timeLeft:newTimeLeft, dead:dead };
      return buildState(newManifestState, state.latentState);
    };

    var manifestStateToActions = function(manifestState){
      return _.keys( transitionTable[manifestState.loc]);
    };
    
    var observe = function(state){
      if (state.manifestState.loc=='eitherAorB_0'){return state.latentState;}
      else {return 'noObservation';}
    };
    
    // var observe = !noisyObserve ? observe :
    //     function(state){
    //       if (state.manifestState.loc=='eitherAorB_0'){
    //         if (flip .6){return state.latentState;}


    var world = {manifestStateToActions: manifestStateToActions, transition:transition, observe:observe};
    var start = buildState({timeLeft:perceivedTime, loc:'start', dead:false}, 'gotoA');
    return {world:world, startState:start, transitionTable:transitionTable};
  };
  
  
  var makeThreePathsAgent = function(utilityTable, baseAgentParams, worldAndStart){
    var transitionTable = worldAndStart.transitionTable;
    
    var priorBelief = baseAgentParams.priorBelief;
    assert.ok( _.isFinite(priorBelief.score([],worldAndStart.startState.latentState)), 
             'makeIRLBanditAgent does not have true latent in support');
    
    var utility = function(state,action){
      var loc = state.manifestState.loc;
      if ((loc=='eitherAorB_A' && state.latentState=='gotoA') || 
          (loc=='eitherAorB_B' && state.latentState=='gotoB') ){
        return utilityTable['eitherAorBPrize'];}
      if (loc == 'immediate0'){return utilityTable['immediatePrize'];}
      if (loc == 'certain1'){return utilityTable['certainPrize'];}
      return 0;
    };

    map( function(loc){
      var state = {manifestState:{loc:loc}};
      assert.ok( _.isFinite(utility(state,'blah')), 'bad utility');
    }, _.keys(transitionTable) );
    
    return makeBeliefDelayAgent(update(baseAgentParams,{utility:utility}), worldAndStart.world);
  };

                             
  var runThreePaths = function( totalTime, utilityTable, paramsUpdate ){

    var worldAndStart = makeThreePathsWorldAndStart(totalTime);
    
    var baseAgentParams = {
      alpha: 100,
      noDelays: false,
      discount: 0.1,
      sophisticatedOrNaive: 'naive',
      myopia: {on:false, bound:0},
      priorBelief: Enumerate(function(){return flip() ? 'gotoA' : 'gotoB';}),
      boundVOI: {on:false, bound:0}
    };
    
    var params = update(baseAgentParams, paramsUpdate);
    var agent = makeThreePathsAgent(utilityTable, params, worldAndStart);
    return simulateBeliefDelayAgent(worldAndStart.startState, worldAndStart.world, agent, totalTime, 'states');
  };

  
  var utilityTable = {eitherAorBPrize: 10, immediatePrize: 6, certainPrize: 9};  
  var totalTime = 3;
  
  // low discount agent
  var paramsUpdate = {discount: 0.1};
  var out = trajectoryToLocations( runThreePaths( totalTime, utilityTable, paramsUpdate ) );
  assert.ok( arraysEqual( out, [ 'start', 'eitherAorB_0', 'eitherAorB_A' ]), 'test boundVOI 1');

  // high discount agent
  var paramsUpdate = {discount: 4};
  var out = trajectoryToLocations( runThreePaths( totalTime, utilityTable, paramsUpdate ) );
  assert.ok( arraysEqual( out, [ 'start', 'immediate0', 'immediate1' ]), 'test boundVOI 2');

  // myopic agent
  var paramsUpdate = {discount:0, myopia: {on:true, bound:1} };
  var out = trajectoryToLocations( runThreePaths( totalTime, utilityTable, paramsUpdate ) );
  assert.ok( arraysEqual( out, [ 'start', 'immediate0', 'immediate1' ]), 'test boundVOI 3');

  // boundVOI agent
  var paramsUpdate = {discount:0, boundVOI:{on:true, bound:1} };
  var out = trajectoryToLocations( runThreePaths( totalTime, utilityTable, paramsUpdate ) );
  assert.ok( arraysEqual( out, [ 'start', 'certain0', 'certain1' ]), 'test boundVOI 4');

  console.log('\n Passed all boundVOI tests');
};


    

    

// Meant to test whether dependencies between elements of the latent
// state are treated correctly. Abandonded for now, since all the
// agents we've considered perform the same on this particular task. 
var testNoObservationDependentLatents = function(){

  var getEUs = function(actionSequences, prizeToUtility, priorArmToPrize){
    var actionSequenceToUtility = function(actionSequence, armToPrize){
      return sum( map( function(action){
        return prizeToUtility[ armToPrize[action] ];
      }, actionSequence));
    };
    
    var actionSequenceToEU = function(actionSequence){
      return sum(map( function(probArmToPrize){
        var prob = probArmToPrize[0];
        var armToPrize = probArmToPrize[1];
        return prob*actionSequenceToUtility(actionSequence,armToPrize);
      }, priorArmToPrize));
    };
    
    return zip(actionSequences,  map(actionSequenceToEU,actionSequences));
  };
  

  var makeLineWorld = function() {
    // This is a bandit world, but with a custom observation function, and a 
    
    // From start, you go to zero or 1 (start is like 0, but with no reward)
    // Then 0,1,2 lie on a line. You can't stay put. At 2, you must go back to zero. 
    
    
    // next state is simply current action
    var mdpTransition = function (state, action) {
      var dead = state.timeLeft - 1 == 1 ? true : false;
      return update(state, { loc:action, timeLeft:state.timeLeft - 1, dead:dead });
    };
    
    var manifestStateToActions = function(manifestState){
      var loc = manifestState.loc;
      if (loc=='start' || loc==0){ return [0,1]; } 
      if (loc==1){return [0,2];}
      if (loc==2){return [0];}
    };
    
    var transition = function(state, action){
      return buildState( mdpTransition(state.manifestState, action), state.latentState);
    };
    
    var observe = function(state){return 'noObservation';};
    
    return {manifestStateToActions: manifestStateToActions, transition:transition, observe:observe};
  };
  
  // World
  var world = makeLineWorld();
  var trueLatentState = {0:'a', 1:'bad', 2:'bad'};
  var perceivedTotalTime = 3;
  var actualTotalTime = perceivedTotalTime;
  var startState = buildState( {loc:'start', timeLeft:perceivedTotalTime, dead:false}, trueLatentState);  
  
  // Agent params:
  
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive',
    myopia: {on:false, bound:0},
    boundVOI: {on:false, bound:0}
  };

  
  var getUtility = function(prizeToUtility){
    return function(state,action){
      var loc = state.manifestState.loc;
      if (loc=='start'){return 0;};
      return prizeToUtility[state.latentState[loc]];
    };
  };

  var isPrizeToUtility = function(x){
    return hasProperties(prizeToUtility,['a','open1', 'open2', 'closed1', 'closed2', 'bad']);
  };
  
  var getTrajectory = function(prizeToUtility, priorComponents){
    assert.ok(isPrizeToUtility(prizeToUtility), 'args getTraj');

    var priorStateToPrize = zip(priorComponents.probs, priorComponents.stateToPrizeValues);
    var actionSequences = [ [1,2], [0,0], [0,1], [1,0] ];
    console.log('\n\n action to EU, computed direction: ', getEUs(actionSequences, prizeToUtility, priorStateToPrize) );  

    var priorBelief = Enumerate(function(){
      return categorical(priorComponents.probs, priorComponents.stateToPrizeValues);                         
    });

    var agentParams = update(baseAgentParams, {priorBelief: priorBelief, utility:getUtility(prizeToUtility)});
    var agent = makeBeliefDelayAgent(params, world);
    return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');
  };

  // AGENT'S UTILITIES
  // For state 0, the prize is 0, for state 1 the prize is open1 or open2, etc.
  var prizeToUtility = {a:5, open1:11, open2:12, closed1:0, closed2:0, bad:-1};
  
  // AGENT'S PRIOR ON STATE TO PRIZES
  // Agent prior on latent states
  var priorComponents = {
    probs: [.3333333333, .333333333, .3333333333],
    stateToPrizeValues: [ {0:'a', 1:'open1', 2:'closed2'},  
                          {0:'a', 1:'closed1', 2:'open2'}, 
                          {0:'a', 1:'bad', 2:'bad'} ]
  };
  
  var traj = getTrajectory(prizeToUtility, priorComponents);
  console.log('\n\n Params:  prizeToUtility  ', prizeToUtility);
  console.log('Prior Belief: ');
  printERP(priorBelief);
  console.log('Trajectory: ', traj, ' \n\n',trajectoryToLocations(traj));

};


runSpeedTests();
runIRLBanditExamples();
runTestInferIRLBandit();
testBoundVOI();
console.log('\n\n-----------\n ALL IRL BANDIT TESTS PASSED');
null



