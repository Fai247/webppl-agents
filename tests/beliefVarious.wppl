

// Tests for src/beliefAgent.wppl



var testAgent = function(){
  
  // bandit setting

  var priorFromLatentERPBandits = function(latentBelief, perceivedTotalTime) {
    return Enumerate(function(){
      return {manifestState: {loc: 'start',
			      timeLeft: perceivedTotalTime,
			      terminateAfterAction: false},
	      latentState: sample(latentBelief)};
    });
  };
  
  // if agent knows the latent state, they should just do the optimal action
  var numArms1 = 3;
  var armToRewards1 = {'start': 0, rewards: [10, 0, 15]};
  var perceivedTotalTime1 = 10;
  var latentBelief1 = deltaERP(armToRewards1);
  var priorBelief1 = priorFromLatentERPBandits(latentBelief1, perceivedTotalTime1);
  var locs1 = trajectoryToLocations(runBeliefAgentBandit(numArms1, armToRewards1,
  							 priorBelief1,
  							 perceivedTotalTime1));
  map(function(index){assert.ok( locs1[index] === 2, 'banditsTest1');}, rest(_.range(10)));

  // if the agent is unsure about the latent state, they should explore the
  // most promising arm first, and then update

  var numArms2 = 3;
  var armToRewards2 = {start: 0, rewards: [0, 1, 0]};
  var perceivedTotalTime2 = 3;
  var latentBelief2 = Enumerate(function() {
    return uniformDraw([armToRewards1, armToRewards2]);
  });
  var priorBelief2 = priorFromLatentERPBandits(latentBelief2, perceivedTotalTime2);
  var locs2 = trajectoryToLocations(runBeliefAgentBandit(numArms2, armToRewards2,
  							 priorBelief2,
  							 perceivedTotalTime2));
  assert.ok(locs2[1] === 2 && locs2[2] === 1, 'banditsTest2');

  // even if one arm has lower expected reward, the agent should still explore
  // it if it has higher value of information

  var numArms3 = 2;
  var armToRewards3 = {start: 0, rewards: [1, 0]};
  var arm1Bonanza = {start: 0, rewards: [1, 2]};
  var latentBelief3 = Enumerate(function(){
    return categorical([0.8, 0.2], [armToRewards3, arm1Bonanza]);
  });
  var perceivedTotalTime3 = 10;
  var priorBelief3 = priorFromLatentERPBandits(latentBelief3, perceivedTotalTime3);
  var locs3 = trajectoryToLocations(runBeliefAgentBandit(numArms3, armToRewards3,
  							 priorBelief3,
  							 perceivedTotalTime3));
  assert.ok(locs3[1] === 1 && locs3[2] === 0, 'banditsTest3');

  // agent thinks that each arm could have high reward, and thinks the reward
  // of each arm is independent, so tries them all

  var numArms4 = 4;
  var armToRewards4 = {start: 0, rewards: [1,1,1,1]};
  var latentBelief4 = Enumerate(function(){
    var dist = function(){return uniformDraw([1, 10]);};
    return {start: 0, rewards: [dist(), dist(), dist(), dist()]};
  });
  var perceivedTotalTime4 = 6;
  var priorBelief4 = priorFromLatentERPBandits(latentBelief4, perceivedTotalTime4);
  var locs4 = trajectoryToLocations(runBeliefAgentBandit(numArms4, armToRewards4,
  							 priorBelief4,
  							 perceivedTotalTime4));
  assert.ok(_.difference(_.range(4), locs4).length === 0, 'banditsTest4');

  console.log('bandit tests passed');



  
  // gridworld tests

  var gridworldMDP = makeDonutWorld2({big: true, maxTimeAtRestaurant : 1});
  var gridworldPOMDP = makeGridworldPOMDP(gridworldMDP);

  var makePriorGridworld = function(manifestState, latentStateERP) {
    return Enumerate(function(){
      return buildState(manifestState, sample(latentStateERP));
    });
  };
  
  // if the agent knows the latent state, they should go to the closest
  // restaurant that they like

  var startState1 = {manifestState: {loc: [3,1],
  				     terminateAfterAction: false,
				     timeAtRestaurant: 1,
  				     timeLeft: 10},
  		     latentState: {'Donut N': true,
  				   'Donut S': true,
  				   Veg: true,
  				   Noodle: true}};
  var utilityTable1 = {'Donut N': 5,
  		       'Donut S': 5,
  		       Veg: 0,
  		       Noodle: 0,
  		       timeCost: -0.1};
  var latentPrior1 = deltaERP(startState1.latentState);
  var prior1 = makePriorGridworld(startState1.manifestState, latentPrior1);
  var agent1 = makeBeliefAgent({
    utility: tableToUtilityFunction(utilityTable1, gridworldPOMDP),
    alpha: 100,
    priorBelief: prior1
  }, gridworldPOMDP);
  var gridLocs1 = trajectoryToLocations(simulateBeliefAgent(startState1, gridworldPOMDP,
  							    agent1, 'states'));
  assert.ok(_.isEqual(gridLocs1[1],[2,1]), 'gridTest1');

  // if the agent knows that both donuts are either open or closed, and they
  // learn that donut south is closed, they should go to their favourite
  // non-donut restaurant
  var startState2 = {manifestState: {loc: [3,1],
  				     terminateAfterAction: false,
				     timeAtRestaurant: 1,
  				     timeLeft: 20},
  		     latentState: {'Donut N': false,
  				   'Donut S': false,
  				   Veg: true,
  				   Noodle: true}};
  var utilityTable2 = {'Donut N': 5,
  		       'Donut S': 5,
  		       Veg: 0,
  		       Noodle: 3,
  		       timeCost: -0.1};
  var latentPrior2 = Enumerate(function(){
    return categorical([0.2, 0.8], [startState2.latentState,
  				    startState1.latentState]);
  });
  var prior2 = makePriorGridworld(startState2.manifestState, latentPrior2);
  var agent2 = makeBeliefAgent({
    utility: tableToUtilityFunction(utilityTable2, gridworldPOMDP),
    alpha: 200,
    priorBelief: prior2
  }, gridworldPOMDP);
  var gridLocs2 = trajectoryToLocations(simulateBeliefAgent(startState2, gridworldPOMDP,
  							    agent2, 'states'));
  assert.ok(_.isEqual(gridLocs2[3], [0,1]) && _.isEqual(gridLocs2[9], [4,3]),
  	    'gridTest2');
  
  // even if a restaurant has low expected utility, maybe you should go there
  // anyway for value of information purposes. For instance, you don't like
  // donuts, love noodles, think veg is ok, and think that the donut shop is
  // open iff the noodle shop is. In this situation, you might walk by the donut
  // shop and then go on to veg even though that path has lower a priori
  // expected utility than going directly to the noodle shop

  var startState3 = {manifestState: {loc: [3,4],
				     terminateAfterAction: false,
				     timeAtRestaurant: 1,
				     timeLeft: 10},
		     latentState: {'Donut N': false,
				   'Donut S': false,
				   Veg: true,
				   Noodle: false}};
  var utilityTable3 = {'Donut N': 0,
		       'Donut S': 0,
		       Veg: 1,
		       Noodle: 3,
		       timeCost: -0.1};
  var latentPrior3 = Enumerate(function(){
    return uniformDraw([startState1.latentState, startState3.latentState]);
  });
  var prior3 = makePriorGridworld(startState3.manifestState, latentPrior3);
  var agent3 = makeBeliefAgent({utility: tableToUtilityFunction(utilityTable3, gridworldPOMDP),
				alpha: 300,
				priorBelief: prior3}, gridworldPOMDP);
  var gridLocs3 = trajectoryToLocations(simulateBeliefAgent(startState3, gridworldPOMDP,
  							    agent3, 'states'));

  assert.ok(_.isEqual(gridLocs3[1], [3,5]) && _.isEqual(gridLocs3[3], [4,6]),
  	    'gridTest3');

  // check that the agent doesn't double-count utility from restaurants
  // (or to figure out settings to make that so)

  var restaurant = {
    donut: {name: 'Donut'},
    noodle: {name: 'Noodle'}
  };
  
  var makeForkRoad = function(options) { 
    var _ = ' '; 
    var D = restaurant.donut;
    var N = restaurant.noodle;

    var myOptions = options || {};

    var features =
	  [[ D , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ , '#'],
	   [ _ ,  N ]];
    
    return makeGridWorld(features, myOptions);
  };

  var forkyGridworldMDP = makeForkRoad({noReverse: true});
  var forkyGridworldPOMDP = makeGridworldPOMDP(forkyGridworldMDP);
  
  var startState4 = {manifestState: {loc: [0,0],
				     terminateAfterAction: false,
				     timeAtRestaurant: 1,
				     timeLeft: 20},
		     latentState: {Donut: true,
				   Noodle: true}};
  var utilityTable4 = {Noodle: 4,
		       Donut: 4.6,
		       timeCost: -0.1};
  var latentPrior4 = deltaERP(startState4.latentState);
  var prior4 = makePriorGridworld(startState4.manifestState, latentPrior4);
  var agent4 = makeBeliefAgent({utility: tableToUtilityFunction(utilityTable4, forkyGridworldPOMDP),
				alpha: 100,
				priorBelief: prior4}, forkyGridworldPOMDP);
  var gridLocs4 = trajectoryToLocations(simulateBeliefAgent(startState4, forkyGridworldPOMDP,
							    agent4,'states'));

  assert.ok(_.isEqual(gridLocs4[1], [1,0]), 'gridTest4');


  // if one restaurant would be excellent but is too far for the agent to reach,
  // the agent should go to the closer restaurant
  var startState5 = {manifestState: {loc: [0,0],
				     terminateAfterAction: false,
				     timeAtRestaurant: 1,
				     timeLeft: 3},
		     latentState: {Donut: true,
				   Noodle: true}};
  var utilityTable5 = {Noodle: 4,
		       Donut: 50,
		       timeCost: -0.1};
  var latentPrior5 = deltaERP(startState5.latentState);
  var prior5 = makePriorGridworld(startState5.manifestState, latentPrior5);
  var agent5 = makeBeliefAgent({utility: tableToUtilityFunction(utilityTable5, forkyGridworldPOMDP),
				alpha: 100,
				priorBelief: prior5}, forkyGridworldPOMDP);
  var gridLocs5 = trajectoryToLocations(simulateBeliefAgent(startState5, forkyGridworldPOMDP,
							    agent5, 'states'));

  assert.ok(_.isEqual(gridLocs5[1], [1,0]), 'gridTest5');
  
  console.log('gridworld tests passed');
  
  return 0;
};

testAgent();
