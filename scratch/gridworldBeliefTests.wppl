// here, we observe the agent in a gridworld, going past a nearby donut shop
// and towards a further away donut shop. we can conclude that the agent likes
// donuts, but thinks that the nearby donut shop is closed.

// assume that the agent is not a discounter, does not have bounded VOI, etc



// Make big gridworld
var getWorld = function(){
  var gridworldMDP = makeDonutWorld2({big: true});
  return makeGridworldPOMDP(gridworldMDP);
};

var tableToUtilityFunction = function(table, feature ) {
  
  return function(state, action) {
    if (state.manifestState.terminateAfterAction) {
      return 0;
    }
    var stateFeatureName = feature(state.manifestState).name;
    if (stateFeatureName) {  
      return table[stateFeatureName];
    } else {
      return table.timeCost;
    }
  };
};


var getStartState = function(startingLocation, perceivedTotalTime, latentState){
  return buildState({loc: startingLocation,
		     terminateAfterAction: false,
		     timeLeft: perceivedTotalTime,
		     timeAtRestaurant: 0},
	            latentState);
};


var getPriorBelief = function(totalTime, startingLocation, latentStateSampler){
  return _getPriorBelief(totalTime, startingLocation, latentStateSampler, true);
};


var inferGridWorld = function(world, startState, baseParams, trueAgentParams, prior, trajectoryOrOffPolicy, 
                              numRejectionSamples, beliefOrBeliefDelay){
  
  var worldObserve = world.observe;
  var observe = getFullObserve(worldObserve);
  var feature = world.feature;

  var makeAgent = getMakeAgentFunction(beliefOrBeliefDelay);
  var simulate = getSimulateFunction(beliefOrBeliefDelay);

  var priorUtilityTable = prior.priorUtilityTable
  var priorAgentPrior = prior.priorAgentPrior;

  assert.ok(isPOMDPState(startState) && isERP(priorUtilityTable) && isERP(priorAgentPrior), 'inferGridworld args');
  assert.ok(trajectoryOrOffPolicy == 'trajectory' || trajectoryOrOffPolicy=='offPolicy', 'trajectoryOrOffPolicy bad');
  assert.ok(_.isNumber(numRejectionSamples) && isPOMDPWorld(world), 'inferGridWorld args' );

  // get observations
  var agent = makeAgent(trueAgentParams, world);
  var observedStateAction = simulate(startState, world, agent, 'stateAction');
  assert.ok( isPOMDPState(observedStateAction[0][0]), 'fullstate in trajectory for inferGridWorld args');
  
  
  return Enumerate(function(){
    // priors and makeAgent are specific to IRL Bandits
    var utilityTable = sample(priorUtilityTable);
    var utility = tableToUtilityFunction(utilityTable, feature);

    var priorBelief = sample(priorAgentPrior);
    
    var params = update(baseParams, {utility:utility, priorBelief: priorBelief});
    
    var agent = makeAgent(params, world);
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;

    // Factor on whole sampled trajectory (SLOW IF NOT DETERMINISTIC AND NUM SAMPLES HIGH)
    var factorOnTrajectory = function(){
      var trajectoryERP = Rejection( function(){
        return simulate(startState, world, agent, 'states')}, numRejectionSamples);
      factor(trajectoryERP.score([], map(first, observedStateAction)));
    };

    // Move agent through observed sequence 
    var factorSequenceOffPolicy = function(currentBelief, previousAction, timeIndex){
      if (timeIndex < observedStateAction.length) { 

        // Go to next world state and sample observation from that state
        var state = observedStateAction[timeIndex][0];
        var observation = observe(state);

        // Update agent's internal state and get action ERP
        var delay = 0;     
        var nextBelief = beliefOrBeliefDelay == 'belief' ?
            agentUpdateBelief(currentBelief, observation, previousAction) :
            agentUpdateBelief(currentBelief, observation, previousAction, delay);


        var nextActionERP = beliefOrBeliefDelay == 'belief' ? 
            agentAct(nextBelief) : agentAct(nextBelief, delay); 

        var observedAction = observedStateAction[timeIndex][1];
        factor(nextActionERP.score([], observedAction));

        // condition on next world state, passing through updated internal state
        factorSequenceOffPolicy(nextBelief, observedAction, timeIndex + 1);
      }
    };

    var doInfer = (trajectoryOrOffPolicy=='trajectory') ? factorOnTrajectory() : 
        factorSequenceOffPolicy(priorBelief,'noAction', 0);
    
    return {utilityTable: utilityTable,
	    priorBelief: priorBelief};
  });
};





// Possible latent states
var allOpenLatentState = {
  'Donut N': true,
  'Donut S': true,
  'Veg': true,
  'Noodle': true
};

var onlyDonutSouthClosedLatentState = {
  'Donut N': true,
  'Donut S': false,
  'Veg': true,
  'Noodle': true
};

// Possible utility functions for agent

var donutUtilityTable = {'Donut N': 5,
			 'Donut S': 5,
			 'Veg': 1,
			 'Noodle': 1,
			 'timeCost': -0.1};
 
var vegUtilityTable = {'Donut N': 1,
		       'Donut S': 1,
		       'Veg': 3,
		       'Noodle': 1,
		       'timeCost': -0.1};


// Make world
var world = getWorld();
var feature = world.feature;
var perceivedTotalTime = 10;
var startingLocation = [2,1];
var trueLatentState = allOpenLatentState;
var startState = getStartState(startingLocation, perceivedTotalTime, trueLatentState);


// agent's true prior and alternative hypothesis about agent's prior
var uninformedLatentStateSampler = function(){
  return flip(.8) ? onlyDonutSouthClosedLatentState : trueLatentState;
};

var truePriorBelief = getPriorBelief(perceivedTotalTime, startingLocation, 
                                      uninformedLatentStateSampler);
var alternativePriorBelief = getPriorBelief(perceivedTotalTime, startingLocation,
                                             function(){return trueLatentState;});


// true agent (which we use to simulate trajectory)
var baseParams = {
  priorBelief: null,
  utility: null,
  alpha: 100,
  noDelays: true,
  discount: 0,
  sophisticatedOrNaive: 'sophisticated',
  myopia: {on: false, bound: 0},
  boundVOI: {on: false, bound: 0},
};
var trueAgentParams = update(baseParams, 
                    {priorBelief: truePriorBelief,
                     utility: tableToUtilityFunction(donutUtilityTable, feature)});


var priorUtilityTable = function(){return uniformDraw([donutUtilityTable, vegUtilityTable]);};
var priorAgentPrior = function(){return uniformDraw([truePriorBelief, alternativePriorBelief]);};

var prior = {priorUtilityTable: Enumerate(priorUtilityTable),
             priorAgentPrior: Enumerate(priorAgentPrior)};


var erp = inferGridWorld(world, startState, baseParams, trueAgentParams, prior, 'trajectory', 
                         10, 'belief');
console.log('erp');
printERP(erp);
ash();




// helper function for inference
var factorAlongTrajectory = function(index, trajectory, agent, belief){
  if (index >= trajectory.length) {
    return [];
  } else {

    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    
    var state = trajectory[index][0];
    var observedAction = trajectory[index][1];

    var observation = observe(state);
    var nextBelief = agentUpdateBelief(state.manifestState, belief, observation, 0);
    var nextActionERP = agentAct(state.manifestState, nextBelief, 0);

    factor(nextActionERP.score([], observedAction));
    
    return factorAlongTrajectory(index + 1, trajectory, agent, nextBelief);
  }
};


// the inference itself
var getPosterior = function(priorUtilityTable, priorAgentPrior) {
  return printERP(Enumerate(function(){
    
    var utilityTable = priorUtilityTable();
    var utility = tableToUtilityFunction(utilityTable, feature);
    var priorBelief = priorAgentPrior();


    var params = update(baseParams, {priorBelief: priorBelief, utility: utility});
    
    var agent = makeAgent(params, gridworldPOMDP);

    
    factorAlongTrajectory(0, observedTrajectory, agent, priorBelief);

    return {utilityTable: utilityTable,
	    priorBelief: priorBelief};
  }));
};

timeit( function(){return getPosterior(priorUtilityTable, priorAgentPrior)});
