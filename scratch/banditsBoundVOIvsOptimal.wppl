// Note: the action ERPs for the myopic agent and the optimal agent are not
// identical because the myopic agent thinks the EUs are lower (because it
// doesn't forsee exploiting from future exploration) and softmax noise means
// that this is reflected in the action ERPs. However, they are close, and the
// MAP is always the same (meaning that they would act the same if they had no
// softmax noise and broke ties in the same way).

var world = makeStochasticBanditWorld(2);
var worldObserve = world.observe;
var observe = getFullObserve(worldObserve);
var transition = world.transition;

var probablyChampagneERP = categoricalERP([0.4, 0.6], ['nothing', 'champagne']);
var probablyNothingERP = categoricalERP([0.6, 0.4], ['nothing', 'champagne']);

var trueLatent = {0: probablyNothingERP,
		  1: probablyChampagneERP};

var timeLeft = 7;

var startState = buildStochasticBanditStartState(timeLeft, trueLatent);

var prior = Enumerate(function(){
  var latentState = {0: uniformDraw([probablyChampagneERP, probablyNothingERP]),
		     1: categorical([0.6, 0.4], [probablyChampagneERP,
						 probablyNothingERP])};
  return buildStochasticBanditStartState(timeLeft, latentState);
});

// var prior = deltaERP(buildStochasticBanditStartState(timeLeft, trueLatent));

var prizeToUtility = {start: 0, nothing: 0, champagne: 2};
var utility = makeStochasticBanditUtility(prizeToUtility);

var optimalAgentParams = {utility: utility,
			  alpha: 100,
			  priorBelief: prior,
			  fastUpdateBelief: false};
var optimalAgent = makeBeliefAgent(optimalAgentParams, world);

var myopicAgentParams = {utility: utility,
			 alpha: 100,
			 priorBelief: prior,
			 sophisticatedOrNaive: 'naive',
			 boundVOI: {on: true, bound: 1},
			 noDelays: false,
			 discount: 0,
			 myopia: {on: false, bound: 0},
			 fastUpdateBelief: false};
var myopicAgent = makeBeliefDelayAgent(myopicAgentParams, world);

var nearlyEqualActionERPs = function(erp1, erp2) {
  var nearlyEqual = function(float1, float2) {
    return Math.abs(float1 - float2) < 0.05;
  };
  return nearlyEqual(erp1.score([], 0), erp2.score([], 0))
    && nearlyEqual(erp1.score([], 1), erp2.score([], 1));
};

// it's important that we simulate the two agents such that they get the same
// prizes when pulling the same arms, so that we can check if their
// actions are the same. We could not ensure this by simply simulating one agent
// and then the other.
var sampleTwoSequences = function(states, priorBeliefs, actions) {
  var optimalState = states[0];
  var optimalPriorBelief = priorBeliefs[0];
  var optimalAction = actions[0];

  var optimalAct = optimalAgent.act;
  var optimalUpdateBelief = optimalAgent.updateBelief;
  
  var myopicState = states[1];
  var myopicPriorBelief = priorBeliefs[1];
  var myopicAction = actions[1];
  
  var myopicAct = myopicAgent.act;
  var myopicUpdateBelief = myopicAgent.updateBelief;

  var optimalObservation = observe(optimalState);
  var myopicObservation = observe(myopicState);
  
  var delay = 0;
  var newMyopicBelief = myopicUpdateBelief(myopicPriorBelief, myopicObservation,
					   myopicAction, delay);
  var newOptimalBelief = optimalUpdateBelief(optimalPriorBelief,
					     optimalObservation, optimalAction);

  // testing
  var myopicEU = myopicAgent.expectedUtility;
  var optimalEU = optimalAgent.expectedUtilityBelief;

  var newMyopicActionERP = myopicAct(newMyopicBelief, delay);
  var newOptimalActionERP = optimalAct(newOptimalBelief);

  var newMyopicAction = sample(newMyopicActionERP);
  var newOptimalAction = nearlyEqualActionERPs(newMyopicActionERP,
					       newOptimalActionERP)
	? newMyopicAction : sample(newOptimalActionERP);

  // console.log('\naction ERPs:');
  // console.log('optimal agent:');
  // printERP(optimalAct(newOptimalBelief));
  // console.log('myopic agent:');
  // printERP(myopicAct(newMyopicBelief, delay));
  // console.log('Are these ERPs nearly equal?');
  // console.log(nearlyEqualActionERPs(newMyopicActionERP, newOptimalActionERP));
  // console.log('Myopic EU of action 0: ');
  // console.log(myopicEU(newMyopicBelief, 0, delay));
  // console.log('Myopic EU of action 1: ');
  // console.log(myopicEU(newMyopicBelief, 1, delay));
  // console.log('Optimal EU of action 0: ');
  // console.log(optimalEU(newOptimalBelief, 0));
  // console.log('Optimal EU of action 1: ');
  // console.log(optimalEU(newOptimalBelief, 1));

  var output = [optimalState, myopicState, newOptimalAction, newMyopicAction];

  if (optimalState.manifestState.terminateAfterAction) {
    // we don't have to worry about different runtimes for the optimal and
    // myopic agents, since they start with the same timeLeft
    return output;
  } else {
    var nextPriorBeliefs = [newOptimalBelief, newMyopicBelief];
    var nextActions = [newOptimalAction, newMyopicAction];
    if (_.isEqual(optimalState, myopicState) && _.isEqual(newOptimalAction,
							  newMyopicAction)) {
      var nextState = transition(optimalState, newOptimalAction);
      var nextStates = [nextState, nextState];
      return output.concat(sampleTwoSequences(nextStates, nextPriorBeliefs,
					      nextActions));
    } else {
      var nextOptimalState = transition(optimalState, newOptimalAction);
      var nextMyopicState = transition(myopicState, newMyopicAction);
      var nextStates = [nextOptimalState, nextMyopicState];
      return output.concat(sampleTwoSequences(nextStates, nextPriorBeliefs,
					      nextActions));
    }
  }
};

var startAction = 'noAction';
// console.log('Comparison of myopic and optimal with 2 arms:');
// console.log(sampleTwoSequences([startState, startState], [prior, prior],
// 			       [startAction, startAction]));

// // then, look at what boundVOI does with 3 arms

// var world2 = makeStochasticBanditWorld(3);
// var worldObserve2 = world2.observe;
// var observe2 = getFullObserve(worldObserve2);
// var transition2 = world2.transition;

// var timeLeft2 = 7;

// var trueLatent2 = {0: probablyNothingERP,
// 		   1: probablyChampagneERP,
// 		   2: probablyNothingERP};

// var startState2 = buildStochasticBanditStartState(timeLeft2, trueLatent2);

// var prior2 = Enumerate(function(){
//   var latentState = {0: uniformDraw([probablyChampagneERP, probablyNothingERP]),
// 		     1: categorical([0.6, 0.4], [probablyChampagneERP,
// 						 probablyNothingERP]),
// 		     2: categorical([0.6, 0.4], [probablyChampagneERP,
// 						 probablyNothingERP])};
//   return buildStochasticBanditStartState(timeLeft2, latentState);
// });

// // var prior = deltaERP(buildStochasticBanditStartState(timeLeft, trueLatent));

// var myopicAgentParams2 = {utility: utility,
// 			  alpha: 100,
// 			  priorBelief: prior2,
// 			  sophisticatedOrNaive: 'naive',
// 			  boundVOI: {on: true, bound: 1},
// 			  noDelays: false,
// 			  discount: 0,
// 			  myopia: {on: false, bound: 0},
// 			  fastUpdateBelief: false};
// var myopicAgent2 = makeBeliefDelayAgent(myopicAgentParams2, world2);


// var optimalAgentParams2 = {utility: utility,
// 			   alpha: 100,
// 			   priorBelief: prior2,
// 			   fastUpdateBelief: false};
// var optimalAgent2 = makeBeliefAgent(optimalAgentParams2, world2);


// // console.log('\n Myopic with 3 arms:');
// // console.log(simulateBeliefDelayAgent(startState2, world2, myopicAgent2, 'stateAction'));

// var f = function(){
//   return simulateBeliefDelayAgent(startState2, world2, myopicAgent2, 'stateAction');
// };

// var g = function(){
//   return simulateBeliefDelayAgent(startState2, world2, optimalAgent2, 'stateAction');
// };


// console.log('Time to run optimal agent: ');
// console.log(timeit(g).runtimeInMilliseconds);

// console.log('Time to run myopic agent: ');
// console.log(timeit(f).runtimeInMilliseconds);

// I thought that the myopic agent was taking longer than the optimal agent, but
// this went away asymptotically (starting at t = 6).

// That being said, it still takes quite a long time (1 minute if you want it to
// be faster than the optimal agent), and I'm not sure how valuable it is to
// show what the boundVOI agent does without demonstrating deviation from
// optimal behaviour

// see how it scales

var varyTime = function(n) {
  console.log(n);
  var world = makeStochasticBanditWorld(2);

  var probablyChampagneERP = categoricalERP([0.2, 0.8], ['nothing', 'champagne']);
  var probablyNothingERP = categoricalERP([0.8, 0.2], ['nothing', 'champagne']);

  var trueLatent = {0: deltaERP('chocolate'),
  		    1: probablyChampagneERP};
  var falseLatent = update(trueLatent, {1: probablyNothingERP});

  var startState = buildStochasticBanditStartState(n, trueLatent);

  var prior = Enumerate(function(){
    var latent = uniformDraw([trueLatent, falseLatent]);
    return buildStochasticBanditStartState(n, latent);
  });

  var prizeToUtility = {start: 0, nothing: 0, chocolate: 1, champagne: 1.5};
  var utility = makeStochasticBanditUtility(prizeToUtility);

  var agentParams = {utility: utility,
	             alpha: 100,
		     priorBelief: prior,
		     sophisticatedOrNaive: 'naive',
		     boundVOI: {on: true, bound: 1},
		     noDelays: false,
		     discount: 0,
		     myopia: {on: false, bound: 0},		     
		     fastUpdateBelief: false};
  var agent = makeBeliefDelayAgent(agentParams, world);

  var f = function() {
    return simulateBeliefDelayAgent(startState, world, agent, 'stateAction');
  };

  return timeit(f).runtimeInMilliseconds.toPrecision(3) * 0.001;
};

// Varying the lifetime of the agent
console.log('Varying lifetimes for myopic agent');
var lifetimes = _.range(16).slice(2);
var runtimes = map(varyTime, lifetimes);

console.log('\nLifetimes: ' + lifetimes);
console.log('Runtimes in seconds: ' + runtimes);

var varyArms = function(n) {
  console.log(n);
  var world = makeStochasticBanditWorld(n);

  var probablyChampagneERP = categoricalERP([0.2, 0.8], ['nothing', 'champagne']);
  var probablyNothingERP = categoricalERP([0.8, 0.2], ['nothing', 'champagne']);
  
  var makeLatentState = function(numArms) {
    return map(function(x){return probablyChampagneERP;}, _.range(numArms));
  };

  var startState = buildStochasticBanditStartState(5, makeLatentState(n));

  var latentSampler = function(numArms) {
    return map(function(x){return uniformDraw([probablyNothingERP,
					       probablyChampagneERP]);},
	       _.range(numArms));
  };
  var prior = Enumerate(function(){
    var latentState = latentSampler(n);
    return buildStochasticBanditStartState(5, latentState);
  });

  var prizeToUtility = {start: 0, nothing: 0, champagne: 1};

  var utility = makeStochasticBanditUtility(prizeToUtility);
  var agentParams = {utility: utility,
		     alpha: 100,
		     priorBelief: prior,
		     sophisticatedOrNaive: 'naive',
		     boundVOI: {on: true, bound: 1},
		     noDelays: false,
		     discount: 0,
		     myopia: {on: false, bound: 0},
		     fastUpdateBelief: false};
  var agent = makeBeliefDelayAgent(agentParams, world);

  var f = function() {
    var trajectory = simulateBeliefDelayAgent(startState, world, agent, 'stateAction');
    return trajectory;
  };

  return timeit(f).runtimeInMilliseconds.toPrecision(3) * 0.001;

};

// varying the number of arms
console.log('\nVarying the number of arms for myopic agent');
var arms = [1,2,3];
var runtimes_arms = map(varyArms, arms);
console.log('\nNumber of arms:' + arms);
console.log('Runtimes in sec:' + runtimes_arms);
