// TODO: print out nicely

// pretty sure something is screwy here, but not sure what
// my suspicion is that one or both of these agents are not doing what I expect
// them to be doing.

// boundVOI agent is always 50/50, and not seeing many positive utilities. weird.
// this seems independent of the prior over probablyChampagne and
// probablyNothing, and of the ERPs probablyChampagne etc.

// However, changing the bound to 2 works. This could be because with a bound
// of 1, you never imagine yourself updating. But if that's true, why does the
// agent not actually update?

// decreasing the bound to 1 and making a strict inequality in updateBelief
// solves things, so it must be due to that. But I still don't get why the agent
// isn't updating. and at any rate, I don't see why it doesn't think that arm 1
// is good.

// So the myopic agent with bound 1 *is* updating its beliefs, but *still* is
// always indifferent between actions. So its bleak assessment of actions must
// be due to it not expecting to update, somehow.

// Also, myopic agent with bound 2 has expected utilities that are too low.
// Something else must be wrong here - maybe the agent is really myopic?
// maybe it is actually a hyperbolic discounter?
// note that the expected utilities are less by an additive factor, which is
// super weird, but they become the same right before the end. Is the myopic
// agent's expected utilities just the EU of a single arm pull?
// increasing the boundVOI bound makes it closer, which makes this explanation
// more likely

// when I recurse on state for the myopic agent, the expected utilities become
// very very close to the state-recurring optimal agent, but not identical
// (like 0.05 less).
// why? I guess because the agent isn't taking into account exploration?
// this shouldn't matter but maybe softmax noise makes it matter? and why does
// state vs belief recursion make such a big difference? what bug is in my code?

var world = makeStochasticBanditWorld(2);
var worldObserve = world.observe;
var observe = getFullObserve(worldObserve);
var transition = world.transition;

var probablyChampagneERP = categoricalERP([0.4, 0.6], ['nothing', 'champagne']);
var probablyNothingERP = categoricalERP([0.6, 0.4], ['nothing', 'champagne']);

var trueLatent = {0: probablyNothingERP,
		  1: probablyChampagneERP};

var timeLeft = 7;

var startState = buildStochasticBanditStartState(timeLeft, trueLatent);

var prior = Enumerate(function(){
  var latentState = {0: uniformDraw([probablyChampagneERP, probablyNothingERP]),
		     1: categorical([0.6, 0.4], [probablyChampagneERP,
						   probablyNothingERP])};
  return buildStochasticBanditStartState(timeLeft, latentState);
});

// var prior = deltaERP(buildStochasticBanditStartState(timeLeft, trueLatent));

var prizeToUtility = {start: 0, nothing: 0, champagne: 2};
var utility = makeStochasticBanditUtility(prizeToUtility);

var optimalAgentParams = {utility: utility,
			  alpha: 100,
			  priorBelief: prior,
			  recurseOnStateOrBelief: 'state',
			  fastUpdateBelief: false};
var optimalAgent = makeBeliefAgent(optimalAgentParams, world);

var myopicAgentParams = {utility: utility,
			 alpha: 100,
			 priorBelief: prior,
			 sophisticatedOrNaive: 'naive',
			 boundVOI: {on: true, bound: 3},
			 recurseOnStateOrBelief: 'belief',
			 noDelays: false,
			 discount: 0,
			 myopia: {on: false, bound: 0},
			 fastUpdateBelief: false};
var myopicAgent = makeBeliefDelayAgent(myopicAgentParams, world);

// console.log('myopic prior: ');
// printERP(myopicAgentParams.priorBelief);

// it's important that we simulate the two agents such that they get the same
// prizes when pulling the same arms, so that we can check if their
// actions are the same. We could not ensure this by simply simulating one agent
// and then the other.
var sampleTwoSequences = function(states, priorBeliefs, actions) {
  var optimalState = states[0];
  var optimalPriorBelief = priorBeliefs[0];
  var optimalAction = actions[0];

  var optimalAct = optimalAgent.act;
  var optimalUpdateBelief = optimalAgent.updateBelief;
  
  var myopicState = states[1];
  var myopicPriorBelief = priorBeliefs[1];
  var myopicAction = actions[1];
  
  var myopicAct = myopicAgent.act;
  var myopicUpdateBelief = myopicAgent.updateBelief;

  var optimalObservation = observe(optimalState);
  var myopicObservation = observe(myopicState);
  
  var delay = 0;
  var newMyopicBelief = myopicUpdateBelief(myopicPriorBelief, myopicObservation,
					   myopicAction, delay);
  var newOptimalBelief = optimalUpdateBelief(optimalPriorBelief,
					     optimalObservation, optimalAction);

  // testing
  var myopicEU = myopicAgent.expectedUtility;
  var optimalEU = optimalAgent.expectedUtilityBelief;

  // problem: what if the agents act differently by accident?
  var newMyopicAction = sample(myopicAct(newMyopicBelief, delay));
  var newOptimalAction = sample(optimalAct(newOptimalBelief));

  console.log('\naction ERPs:');
  console.log('optimal agent:');
  printERP(optimalAct(newOptimalBelief));
  console.log('myopic agent:');
  printERP(myopicAct(newMyopicBelief, delay));
  console.log('Myopic EU of action 0: ');
  console.log(myopicEU(newMyopicBelief, 0, delay));
  console.log('Myopic EU of action 1: ');
  console.log(myopicEU(newMyopicBelief, 1, delay));
  console.log('Optimal EU of action 0: ');
  console.log(optimalEU(newOptimalBelief, 0));
  console.log('Optimal EU of action 1: ');
  console.log(optimalEU(newOptimalBelief, 1));

  var output = [optimalState, myopicState, newOptimalAction, newMyopicAction];

  if (optimalState.manifestState.terminateAfterAction) {
    // we don't have to worry about different runtimes for the optimal and
    // myopic agents, since they start with the same timeLeft
    return output;
  } else {
    var nextPriorBeliefs = [newOptimalBelief, newMyopicBelief];
    var nextActions = [newOptimalAction, newMyopicAction];
    if (_.isEqual(optimalState, myopicState) && _.isEqual(newOptimalAction,
							  newMyopicAction)) {
      var nextState = transition(optimalState, newOptimalAction);
      var nextStates = [nextState, nextState];
      return output.concat(sampleTwoSequences(nextStates, nextPriorBeliefs,
					      nextActions));
    } else {
      var nextOptimalState = transition(optimalState, newOptimalAction);
      var nextMyopicState = transition(myopicState, newMyopicAction);
      var nextStates = [nextOptimalState, nextMyopicState];
      return output.concat(sampleTwoSequences(nextStates, nextPriorBeliefs,
					      nextActions));
    }
  }
};

var startAction = 'noAction';

console.log(sampleTwoSequences([startState, startState], [prior, prior],
			       [startAction, startAction]));

// console.log(simulateBeliefAgent(startState, world, optimalAgent, 'stateAction'));



// then, look at what boundVOI does with 3 arms
// see how it scales
