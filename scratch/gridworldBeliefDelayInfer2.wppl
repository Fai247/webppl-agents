// here, we observe the agent start off near donut south, but go to donut north.
// we know that the agent knows the true state, so it must not be that the agent
// just likes donuts. Instead, the solution is that the agent likes veg, but is
// a hyperbolic discounter, aimed to go to veg, but got tempted by donuts
// instead. we also can conclude that the agent is probably not an extreme
// discounter, since they would go to donut south if they were, but it's within
// the realm of possibility because of softmax noise.

var gridworldMDP = makeDonutWorld2({big: true});
var gridworldPOMDP = makeGridworldPOMDP(gridworldMDP);
var feature = gridworldMDP.feature;
var transition = gridworldPOMDP.transition;
var observe = gridworldPOMDP.observe;

// here, we define tables listing the utility of each restaurant, and a function
// that converts these tables into utility functions.

var donutUtilityTable = {'Donut N': 2,
			 'Donut S': 2,
			 'Veg': 1,
			 'Noodle': 0,
			 timeCost: -0.1};
 
var vegUtilityTable = {'Donut N': 1,
		       'Donut S': 1,
		       Veg: 2,
		       Noodle: 0,
		       timeCost: -0.1};

var tableToUtilityFunction = function(table) {
  return function(state, action) {
    if (state.manifestState.dead) {
      return 0;
    }
    var stateFeatureName = feature(state.manifestState).name;
    if (stateFeatureName) {
      return table[stateFeatureName];
    } else {
      return table.timeCost;
    }
  };
};

// we start the agent off near Donut South
var startState = {manifestState: {loc: [3,1],
				  dead: false,
				  timeLeft: 10,
				  digest: 0},
		  latentState: {'Donut N': true,
				'Donut S': true,
				'Veg': true,
				'Noodle': true}};

// making the agent
var params = {
  alpha: 100,
  noDelays: false,
  discount: 2,
  sophisticatedOrNaive: 'naive',
  myopia: {on: false, bound: 0},
  priorBelief: deltaERP(startState.latentState),
  boundVOI: {on: false, bound: 0},
  utility: tableToUtilityFunction(vegUtilityTable)
};
var agent = makeBeliefDelayAgent(params, gridworldPOMDP);

// generating the agent's trajectory
var observedTrajectory = simulateBeliefDelayAgent(startState, gridworldPOMDP,
						  agent, 'stateAction');
// (visualise trajectory in codebox)

// helper function for inference
var factorAlongTrajectory = function(index, trajectory, agent, belief){
  if (index >= trajectory.length) {
    return [];
  } else {

    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    
    var state = trajectory[index][0];
    var observedAction = trajectory[index][1];

    var observation = observe(state);
    var nextBelief = agentUpdateBelief(state.manifestState, belief, observation, 0);
    var nextActionERP = agentAct(state.manifestState, nextBelief, 0);

    factor(nextActionERP.score([], observedAction));
    
    return factorAlongTrajectory(index + 1, trajectory, agent, nextBelief);
  }
};

// the inference itself
var agentPosterior = function() {
  return printERP(Enumerate(function(){
    var alpha = 100;
    
    var utilityTable = uniformDraw([donutUtilityTable, vegUtilityTable]);
    var utility = tableToUtilityFunction(utilityTable);
    var agentPrior = deltaERP(startState.latentState);

    var isMyopic = false;
    var myopiaBound = 0;

    var hasBoundVOI =  false;
    var voiBound = 0;

    var discount = uniformDraw([0, 2, 100]);
    var noDelays = (discount === 0);

    var sophisticatedOrNaive = 'naive';

    var params = {
      alpha: alpha,
      utility: utility,
      noDelays: noDelays,
      discount: discount,
      sophisticatedOrNaive: sophisticatedOrNaive,
      myopia: {on: isMyopic, bound: myopiaBound},
      priorBelief: agentPrior,
      boundVOI: {on: hasBoundVOI, bound: voiBound}
    };
    
    var agent = makeBeliefDelayAgent(params, gridworldPOMDP);

    factorAlongTrajectory(0, observedTrajectory, agent, agentPrior);

    return {utilityTable: utilityTable,
	    discount: discount};
  }));
};

timeit(agentPosterior);
