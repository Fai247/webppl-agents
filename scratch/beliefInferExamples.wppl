var printOut = function ( trajectory ) {
  console.log('trajectory', map( function (state) {return state.manifestState.loc;},
				 trajectory) );
  //console.log('expUtilities', out.startEU);
};

// ----------------------------------------------
// inference of an agent's utility and/or priorBelief based on their trajectory.
// assumed that we see some initial
// segment of the trajectory: not necessarily the whole trajectory, but without gaps.
// we also assume that the inference function knows the latent state.
// ----------------------------------------------

// first inference function: generate an agent, look at the ERP over trajectories
// that agent generates, get the score of the actual trajectory. Only works for
// bandits.

// *observedTrajectory* is the trajectory actually seen, *latentState* is the latent
// state of the POMDP, *utilitySignPrior* is an ERP which represents our prior over the
// agent's utilities, *beliefPrior* is an ERP which represents our prior over the
// agent's prior over latent states, *alpha* regulates the agent's softmax noise,
// assumed to be the same as the actual agent, and *world* is the POMDP that the
// trajectory comes from.
var inferUtilityBeliefBandits1 = function(observedTrajectory, utilitySignPrior,
					  beliefPrior, alpha, world) {
  var trajectoryLength = observedTrajectory.length;
  
  var rewardUtility = function(state, action) {
    return state.latentState[state.manifestState.loc];
  };
  
  return Enumerate(function(){
    var agentUtilitySign = sample(utilitySignPrior);
    var agentUtility = function(state, action) {
      return agentUtilitySign * rewardUtility(state, action);
    };
    var priorBelief = sample(beliefPrior);
    var agent = makeBeliefAgent({utility: agentUtility,
				 alpha: alpha,
				 priorBelief: priorBelief},
				world);
    var newTrajectory = simulateBeliefAgent(observedTrajectory[0], world, agent,
					    trajectoryLength,'states');
    condition(_.isEqual(newTrajectory, observedTrajectory));
    return {utilitySign: agentUtilitySign, prior: priorBelief};
  });
};


// second inference function: generate an agent. In the first state of the trajectory,
// update that agent's beliefs, get the agent's ERP over next states, get the score
// of the actual next state, then put the agent in the actual next state, and repeat
// this process for the whole trajectory.

// same args as before
var inferUtilityBeliefBandits2 = function(observedTrajectory, utilitySignPrior,
					  beliefPrior, alpha, world) {
  var trajectoryLength = observedTrajectory.length;
  var perceivedTotalTime = observedTrajectory[0].manifestState.timeLeft;
  
  var rewardUtility = function(state, action) {
    return state.latentState[state.manifestState.loc];
  };

  return Enumerate(function(){
    var agentUtilitySign = sample(utilitySignPrior);
    var agentUtility = function(state, action){
      return agentUtilitySign * rewardUtility(state, action);
    };
    var agentPrior = sample(beliefPrior);
    var agent = makeBeliefAgent({utility: agentUtility,
				 alpha: alpha,
				 priorBelief: agentPrior},
				world);
    
    var agentActionBelief = agent.agent;
    var transition = world.transition;
    var observe = world.observe;

    var sampleAlongTrajectory = function(state, currentBelief, actionsSampled) {
      if (actionsSampled === trajectoryLength - 1) {
	return [];
      } else {
	var actionERP = Enumerate(function(){
	  return sample(agentActionBelief(state.manifestState, currentBelief,
					  observe(state))).action;
	});

	var newBelief = sample(agentActionBelief(state.manifestState, currentBelief,
						 observe(state))).belief;

	var nextState = observedTrajectory[actionsSampled + 1];
	
	factor(actionERP.score([], nextState.manifestState.loc));
	
	return sampleAlongTrajectory(nextState, newBelief, actionsSampled + 1);
      }
    };

    sampleAlongTrajectory(observedTrajectory[0], agentPrior, 0);

    return {utilitySign: agentUtilitySign, prior: agentPrior};
  });
};

// third inference function: doing inference on an agent's utilities and beliefs
// in a gridworld. Inference works the same way as in *inferUtilityBeliefBandits2*.

// arguments: [fill in later]

var inferGridworld = function(observedTrajectory, utilityTablePrior, beliefPrior,
			      alpha, world) {
  var trajectoryLength = observedTrajectory.length;
  var perceivedTotalTime = observedTrajectory[0].manifestState.timeLeft;

  return Enumerate(function(){
    var utilityTable = sample(utilityTablePrior);
    var utility = function(state, action) {

    };
  });
  
}


// testing inference for bandits.

var numArms = 2;
var trueLatent = {start: 0, 0: 10, 1: 1};

var rewardUtility = function(state, action) {
  return state.latentState[state.manifestState.loc];
};

var negRewardUtility = function(state, action) {
  return -rewardUtility(state, action);
};
var agentBelief = Enumerate(function(){
  if (flip(0.8)) {
    return {start: 0, 0: 10, 1: 20};
  } else {
    return trueLatent;
  }
});

var informedBelief = Enumerate(function(){
  return trueLatent;
});

var observedTrajectory = runBandits(numArms, trueLatent, agentBelief, 5);

var utilitySignPrior = Enumerate(function(){
  if (flip(0.5)) {
    return 1;
  } else {
    return -1;
  }
});

var agentBeliefPrior = Enumerate(function(){
  if (flip(0.5)) {
    return agentBelief;
  } else {
    return informedBelief;
  }
});

// infer whether the agent likes reward and what it knows
var inferredAgent = inferUtilityBeliefBandits1(observedTrajectory, utilitySignPrior,
					       agentBeliefPrior, 100,
					       makeBandits(numArms));

var inferredAgent2 = inferUtilityBeliefBandits2(observedTrajectory, utilitySignPrior,
					       agentBeliefPrior, 100,
					       makeBandits(numArms));

// tells you whether utility is rewardUtility or negRewardUtility
var sampleUtilitySign = function(){
  var utilitySign = sample(inferredAgent2).utilitySign;
  console.log(utilitySign);
};

repeat(4, sampleUtilitySign);

// samples an agent prior over latent states, samples from that prior
var sampleAgentPrior = function(){
  var agentLatentState = sample(sample(inferredAgent2).prior);
  console.log(JSON.stringify(agentLatentState));
};

repeat(20, sampleAgentPrior);

ash();
