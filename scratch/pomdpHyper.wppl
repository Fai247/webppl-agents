


var inGrid = function(gridMap, loc) {
  return (loc[0] >= 0 && loc[0] < gridMap.xLim &&
          loc[1] >= 0 && loc[1] < gridMap.yLim);
}

var isBlockedLoc = function(gridMap, loc) {
  var getFeature = gridMap.feature;
  var feature = getFeature({ loc : loc});
  return feature === '#';
}

var isAllowedState = function(gridMap, loc) {
  return (inGrid(gridMap, loc)) && !isBlockedLoc(gridMap, loc);
};

var advanceStateTime = function(state) { 
  var dead = state.timeLeft > 0 ? state.dead : true;
  return update(state, { timeLeft : state.timeLeft - 1, dead : dead});
}

var moveState = function(gridMap, state, action) { 
    var loc = state.loc; 
    var gridTransition = {l: [loc[0]-1, loc[1]],
                          r: [loc[0]+1, loc[1]],
                          u: [loc[0], loc[1]+1],
                          d: [loc[0], loc[1]-1]};

    var possibleNextLoc = gridTransition[action];
    assert.ok(possibleNextLoc != undefined, 'action was not found');

    var nextLoc = isAllowedState(gridMap, possibleNextLoc) ? possibleNextLoc : loc;
    return update(state, { loc : nextLoc });
}

var makeGridTransition = function(gridMap) {
  return function(state,action){    
    var getFeature = gridMap.feature;
    var state2 = advanceStateTime(state)

    if ( getFeature(state2).name ) { // there's a restaurant here
      var state3 =  update(state2, { digest : state2.digest -1}); 
      if (state3.digest <= 0) {
        return update(state3, { dead : true });
      }
      return state3;
    }
    return moveState(gridMap, state2, action);
  };
};

var makeGridMap = function(rfeatures)  { 
  var features = rfeatures.reverse();
  return { 
    features : features,
    xLim : features[0].length,
    yLim : features.length,
    feature : function (state) { 
      return features[state.loc[1]][state.loc[0]];
    }
  };
};

var makeDonutWorldExtended = function(options){
  var _ = ' '; 
  var D = { name : 'Donut'} ;
  var V = { name : 'Veg'} ;
  var N = { name : 'Noodle'} ;

  var X = options.bottomLeftNook;
  var features = [['#', '#', '#',  V , '#', '#'],
                  ['#', '#',  _ ,  _ ,  _ , '#'],  
                  ['#', D  ,  _ , '#',  _ ,  N ],
                  ['#', '#',  _ , '#',  _ , '#'],
                  [ X ,  _ ,  _ ,  _ ,  _ , '#'],
                  ['#', '#',  _ , '#', '#', '#']];

  var gridMap = makeGridMap(features); 

  var transition = makeGridTransition(gridMap);
  var actions = ['l', 'r', 'u', 'd'];

  var stateToActions = function (state) { 
    var possibleActions = filter(function (action) { 
      var newState = transition(state, action);
      return state.loc[0] !== newState.loc[0] || state.loc[1] !== newState.loc[1];
    }, actions);

    if (possibleActions.length > 0) {
      return possibleActions;
    } else { 
      return [actions[0]];
    }
  };

  return update(gridMap,
      { transition : transition, 
        actions    : actions,
        stateToActions    : stateToActions,
      });
};

var makeDonutUtility = function (max_digest, rewards) { 
  return function(world, state, action) {
    var getFeature = world.feature;
    var feature = getFeature(state);

    if (state.dead)   { return 0; }
    if (feature.name) { return rewards[feature.name][max_digest - state.digest]; }
    return -0.01; // TIMECOST (should this be part of *rewards* at least optionally?
  };
};

var buildState = function(manifestState,latentState){
  return {manifestState:manifestState, latentState:latentState};
};

var isERP = function(x){return x.hasOwnProperty('score') && x.hasOwnProperty('sample');}
var isGreaterZero = function(x){return _.isFinite(x) && x>0;};
var isState = function(x){x.hasOwnProperty('manifestState') && x.hasOwnProperty('latentState') &&
                          _.isFinite(x.manifestState.timeLeft);}





var makeHyperbolicDiscounter = function (utility, alpha, discount, sophisticatedOrNaive, world, priorBelief) {
  assert.ok(isERP(priorBelief));
  
  var stateToActions = world.stateToActions;
  var transition = world.transition;
  var observe = world.observe;

  var updateBelief = dp.cache(
    function(manifestState, currentBelief, observation){
      return Enumerate( function(){
        var latentState = sample(currentBelief);
        factor( observe(buildState(manifestState, latentState)).score([], observation)); // TODO change
        return latentState;
      });
    });
  
  var _agent = dp.cache( 
    function(manifestState, currentBelief, observation, delay){
      assert.ok(isGreaterZero(manifestState.timeLeft)  && isERP(currentBelief) && _.isFinite(delay), 'agent args')
      
      return Enumerate(function(){
        var newBelief = updateBelief(manifestState, currentBelief, observation);
        var action = uniformDraw( stateToActions(state) );
        var expectedUtility = expectation(
          Enumrate(function(){
            var state = buildState(manifestState, sample(newBelief));
            assert.ok(state.manifestState == manifestState, 'didnt build state correctly');
            return _expUtility(state, action, newBelief, delay);   
          }));

        factor(alpha * eu);
        return {action: action, belief:newBelief};
      });      
    });
  var agent = function(manifestState, currentBelief, observation) { 
    return _agent(manifestState, currentBelief, observation, 0);
  };

  var _expUtility = dp.cache(
    function(state, action, currentBelief, delay){
      var u = 1.0/(1 + discount*delay) * utility(world, state, action);
      
      assert.ok(_.isFinite(u) && isERP(currentBelief) && isState(state), 'expUtility args');
      
      if (state.manifestState.dead){
        return u; 
      } else {                     
        return u + expectation( Enumerate(function(){
          var nextState = transition(state, action); 
          var perceivedDelay = { naive : delay + 1, sophisticated : 0}[sophisticatedOrNaive]; 
          var agentNext = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));
     
          return _expUtility(nextState, agentNext.action, agentNext.belief, delay+1);  
        }));
      }                      
    });
  var expUtility = function(state, action, currentBelief) { return _expUtility(state, action, currentBelief, 0); };

    
  return {
    utility : utility,
    expUtility : expUtility,
    agent : agent,
    _expUtility : _expUtility,
    _agent : _agent,   // TODO why expose this?
    alpha : alpha, 
    discount : discount,
    priorBelief: priorBelief
  };
};

var simulate = function(startState, world, agent, actualTotalTime, perceivedTotalTime, statesOrActions) { 
  assert.ok(  actualTotalTime <= perceivedTotalTime && isState(startState), 'simulate args');
    
  var agentAction = agent.agent;
  var transition = world.transition;
  var observe = world.observe;
  var startState = update(startState, update(state.latentState, { timeLeft : perceivedTotalTime }));

  var isGridworld = arraysEqual(world.actions, ['l', 'r', 'u', 'd']);
  var cutoffCondition = isGridworld ? function(actualTimeLeft, state){return actualTimeLeft==0 || state.dead;} :
      function(actualTimeLeft, state){return actualTimeLeft==0};
  
  var sampleSequence = function (state, currentBelief, actualTimeLeft) {
      
    if (cutoffCondition(actualTimeLeft, state.manifestState) ){
        return [];
      } else {
        var agentNext = sample(agentAction(state.manifestState, currentBelief, observe(state)));
        var nextState = transition(state, agentNext.action);
        var out = {states:state, actions:action, both:[state,action]}[statesOrActions]; // could return observations
        return [ out ].concat( sampleSequence(nextState, agentNext.belief, actualTimeLeft-1));
      }
    };
 
  return sampleSequence(startState, priorBelief, actualTotalTime);
};


var mdpSim = function(startState, world, agent, actualTotalTime, perceivedTotalTime) { 
  // TODO check utility matches states: assert.ok( all not undefined: map(agent.utility, world.states) 

  assert.ok(perceivedTotalTime > 1,'perceivedTime<=1. If=1 then should have state.dead, but then simulate wont work');
    
  var _agentExp = agent._expUtility;
  var stateToActions = world.stateToActions;

  var trajectory = simulate(startState, world, agent, actualTotalTime, perceivedTotalTime, 'states');
  var len = trajectory.length;

  // expUtility from the startState, which should be different for naive and sophisticated agents
  var expectedUtilityStart = map(function(state){
    return [state.loc,
            map(function(a){return _agentExp(state, a, len - state.timeLeft);}, stateToActions(state))]; 
  }, trajectory);
  
  // TODO expUtility from the second time step onwards, we just slice the trajectory
  var getExpectedUtilityFromTimestep = function(timestep){
    var trajectory = trajectory.slice(timestep,trajectory.length);
    return map(function(state){
      return [state.loc, 
              map(function(a){return _agentExp(state, a, trajectory.length - state.timeLeft);}, 
                  stateToActions(state))]; 
    }, trajectory);
  };

  return {trajectory: trajectory, startEU: expectedUtilityStart};
  //GridWorld.draw(world, {trajectory : trajectory, expUtilities : exp });
}


var printOut = function( out ){
  console.log('trajectory', map( function(state){return state.loc;}, out.trajectory) );
  console.log('expUtilities',  out.startEU);
};


// TODO get output working for donut, including exp utilities

// ADD POMDP STUFF WITH BANDITS, paying attention to
// (a) using the MDP transition to create the POMDP (maybe building the function
// with in the body of simulate, so save computation time
// (b) adding updateDelay so that delay overhead avoided when discount==0
// (c) combining pomdp and digest thing. 


var testDonut = function(){
  var start = { 
    loc : [2,0],
    dead : false,
    digest : 4
  };

  var donutUtility = makeDonutUtility(4, {
    'Donut' : [1, 0, 0, 0],
    'Veg'   : [0, 0, 0, 3.5],
    'Noodle': [0, 0, 0, 0]
  });

  var world = makeDonutWorldExtended({ bottomLeftNook : '#' });
  var agent = makeHyperbolicDiscounter(donutUtility, 500, 1, 'sophisticated', world);
  var trajectory = simulate(start, world, agent, 12, 12, 'states');
  assert.ok(gridEqual(last(trajectory).loc, [3, 5]), "sophisticated didn't end up in 3,5");
  assert.ok(trajectory[5].loc[0] == 4, "sophisticated didn't take right side");
  
  var world2 = makeDonutWorldExtended({ bottomLeftNook : { name : 'Donut' } });
  var agent2 = makeHyperbolicDiscounter(donutUtility, 500, 1, 'naive', world2);
  var trajectory = simulate(start, world2, agent2, 12, 12, 'states');
  assert.ok(gridEqual(last(trajectory).loc, [1, 3]), "naive didn't end up in 1,3");
  assert.ok(trajectory[3].loc[0] == 2, "naive didn't take left side");
  
  
  var state = update(start, { loc : [2,1], timeLeft : 12 });
  var naiveExp = agent2.expUtility;
  var naive22utilities = map(function (a) { return  naiveExp(state, a); }, world.actions);
  
  var sophExp = agent.expUtility;
  var soph22utilities = map(function (a) { return  sophExp(state, a); }, world.actions);
  
  assert.ok(
    naive22utilities[0] != soph22utilities[0] &&
      naive22utilities[1] != soph22utilities[1] &&
      naive22utilities[2] != soph22utilities[2] &&
      naive22utilities[3] != soph22utilities[3] 
    , "some utilities match!");

  console.log('passed tests donut');
};
testDonut();
ash();



var testLine = function(){
  
  var makeLine = function(noiseProb){
    var detTransition = function(state,action){
      var newLoc = (state.loc == 0 & action==-1) ? 0 : state.loc + action;
      var dead = state.timeLeft - 1  == 1 ? true : false;
      assert.ok(state.timeLeft != 0, 'detTransition for makeLine');
      return update(state,{loc:newLoc, timeLeft:state.timeLeft-1, dead:dead});
    };
    
    var stochasticTransition = function(state,action){
      return flip(noiseProb) ? detTransition(state,uniformDraw([-1,1])) : detTransition(state,action);
    };
    
    var transition = noiseProb > 0 ? stochasticTransition : detTransition;
    var stateToActions = function(state){return  [-1,1];};
    
    return {stateToActions: stateToActions, transition: transition};
  };

  
  var noiseProb = 0;
  var world = makeLine(noiseProb);

  // agent params
  var utility = function(world, state, action){
    if (state.loc==1){return -4;}
    return state.loc;
  };

  var alpha = 100;
  var start = {loc:0, dead:false};

  var smallTrajectory = function(){
    var agent = makeHyperbolicDiscounter(utility, alpha, 0, 'naive', world);
    var trajectory = mdpSim(start, world, agent, 2, 2).trajectory;
    console.log('start, trajectory', JSON.stringify(start), JSON.stringify(trajectory) );
    assert.ok( _.isEqual([update(start,{timeLeft:2}), update(start,{timeLeft:1, dead:true})],
                         trajectory), 'smallTraj test'); 
  };
  smallTrajectory();
  
  
  // test length==4 trajectories
  var mediumTrajectory = function(noiseProb){
    var world = makeLine(noiseProb);
    map( function(naiveOrSophisticated){
      var perceivedTotalTime = 4;
      var actualTotalTime = 4;  
      
      var runSim = function(discount){
        var agent = makeHyperbolicDiscounter(utility, alpha, discount, naiveOrSophisticated, world);
        return mdpSim(start, world, agent, actualTotalTime, perceivedTotalTime);
      };
      
      assert.ok( runSim(0).trajectory[1].loc == 1, 'no discount go to 1'+' naiveOr:'+naiveOrSophisticated );
      assert.ok( runSim(.1).trajectory[1].loc == 1, 'small discount go to 1' +' naiveOr:'+naiveOrSophisticated );
      assert.ok( runSim(1).trajectory[1].loc == 0, 'bigger discount stay' +' naiveOr:'+naiveOrSophisticated );
      assert.ok( runSim(2).trajectory[1].loc == 0, 'even bigger discount stay' + '  naiveOr:'+naiveOrSophisticated);
    }, ['naive', 'sophisticated']);
  };
  
  map(mediumTrajectory,[0,.01]);
  console.log('Passed testLine');
};

//testLine();



var testBandits = function(){

  var makeBandits = function(numArms, noiseProb){
    var actions = _.range(numArms);
    
    var detTransition = function(state,action){
      var dead = state.timeLeft - 1  == 1 ? true : false;
      return update(state,{loc:action, timeLeft:state.timeLeft-1, dead:dead});
    };

    // try best arm, risk going to 'start' (zero payoff)
    var stochasticTransition = function(state,action){
      return (flip(noiseProb) && action==0 ) ? detTransition(state,'start') : detTransition(state,action);
    };
    
    var transition = noiseProb > 0 ? stochasticTransition : detTransition;
    var stateToActions = function(state){return actions;};
    
    return {stateToActions: stateToActions, transition:transition, defaultStart: {loc:'start', dead:false}};
  };


  var smallTest = function(){
    // fixed params
    var numArms = 2; 
    var utility = function(world, state, action){
      return {'start':0, 0:10, 1:5}[state.loc];
    };
    var alpha = 100;
    var discount = 0;

    var noiseProb = 0;
    var world = makeBandits(numArms, noiseProb);
    var agent = makeHyperbolicDiscounter(utility, alpha, discount , 'naive', world);
    var trajectory =  mdpSim(world.defaultStart, world, agent, 2, 2).trajectory;
    assert.ok(trajectory[1].loc == 0, 'testbandits small');

    // with noise, there's .6 chance that you go 'start' and get zero if you go 0, so go 1 instead
    var noiseProb = 0.6;
    var world = makeBandits(numArms, noiseProb);
    var agent = makeHyperbolicDiscounter(utility, alpha, discount , 'naive', world);
    var trajectory =  mdpSim(world.defaultStart, world, agent, 2, 2).trajectory;
    assert.ok(trajectory[1].loc == 1, 'testbandits small');
  };
  smallTest();
  
  console.log('passed testbandits');
};
testBandits();

ash();
// var t1 = timeit( function () { 
//   var f = agent.agent;
//   var trajectory = f(start, world, 0);
// });




// version of makeHyperbolic with two expectations
var makeHyperbolicDiscounterDoubleExpectation = function (utility, alpha, discount, sophisticatedOrNaive, world, priorBelief) {
  assert.ok(isERP(priorBelief));
  
  var stateToActions = world.stateToActions;
  var transition = world.transition;
  var observe = world.observe;

  var updateBelief = dp.cache(
    function(manifestState, currentBelief, observation){
      return Enumerate( function(){
        var latentState = sample(currentBelief);
        factor( observe(buildState(manifestState, latentState)).score([], observation)); // TODO change
        return latentState;
      });
    });
  
  var _agent = dp.cache( 
    function(manifestState, currentBelief, observation, delay){
      assert.ok(isGreaterZero(manifestState.timeLeft)  && isERP(currentBelief) && _.isFinite(delay), 'agent args')
      
      return Enumerate(function(){
        var newBelief = updateBelief(manifestState, currentBelief, observation);
        var action = uniformDraw( stateToActions(state) );
        var expectedUtility = expectation(
          Enumrate(function(){
            var state = buildState(manifestState, sample(newBelief));
            assert.ok(state.manifestState == manifestState, 'didnt build state correctly');
            return _expUtility(state, action, newBelief, delay);   
          }));

        factor(alpha * eu);
        return {action: action, belief:newBelief};
      });      
    });
  var agent = function(manifestState, currentBelief, observation) { 
    return _agent(manifestState, currentBelief, observation, 0);
  };

  var _expUtility = dp.cache(
    function(state, action, currentBelief, delay){
      var u = 1.0/(1 + discount*delay) * utility(world, state, action);
      
      assert.ok(_.isFinite(u) && isERP(currentBelief) && isState(state), 'expUtility args');
      
      if (state.manifestState.dead){
        return u; 
      } else {                     
        return u + expectation( Enumerate(function(){
          var nextState = transition(state, action); 
          var perceivedDelay = { naive : delay + 1, sophisticated : 0}[sophisticatedOrNaive]; 
          var agentNext = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));
     
          return _expUtility(nextState, agentNext.action, agentNext.belief, delay+1);  
        }));
      }                      
    });
  var expUtility = function(state, action, currentBelief) { return _expUtility(state, action, currentBelief, 0); };
    
  return {
    utility : utility,
    expUtility : expUtility,
    agent : agent,
    _expUtility : _expUtility,
    _agent : _agent,   // TODO why expose this?
    alpha : alpha, 
    discount : discount,
    priorBelief: priorBelief
  };
};
