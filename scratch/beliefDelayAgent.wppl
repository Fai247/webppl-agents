
// TODO for beliefDelay agent

// 1. Fix getExpectedUtilities for pomdp case by having option
// of outputting beliefs as well as [state,action]
// 2. Get gridworld examples running (to make sure there's no gotchas)
// 3. Add some inference examples and add agent to /src
// 4. Add myopia and bound VOI agents. 

// Helper functions for testing argument types (warning: heuristics only)
var isGreaterZero = function (x) {return _.isFinite(x) && x > 0;};
var isERP = function (x) {return x.hasOwnProperty('score') && x.hasOwnProperty('sample');};
var isState = function (x) {x.hasOwnProperty('latentState') && _.isFinite(x.manifestState.timeLeft);};
var isGridworld = function(world){return arraysEqual(world.actions, ['l', 'r', 'u', 'd']);};
var isWorld = function (x) { return x.hasOwnProperty('transition') && x.hasOwnProperty('manifestStateToActions') &&
                             x.hasOwnProperty('observe');};


// POMDP AGENT AND WORLD

// Helper function for building state from its components
var buildState = function (manifestState, latentState) {
  return { manifestState:manifestState, latentState:latentState };
};



var makeBeliefDelayAgent = function (agentParams, world){
  map(function(s){assert.ok(agentParams.hasOwnProperty(s),'makeAgent args');}, 
      ['utility','alpha','discount','sophisticatedOrNaive', 'priorBelief']);
  assert.ok( isWorld(world), 'world argument lacks transition, stateToActions, or observe');

  var utility = agentParams.utility;
  
  var manifestStateToActions = world.manifestStateToActions;
  var transition = world.transition;
  var observe = world.observe;

  // Set all delays to zero to speed up runtime
  var getPerceivedDelay = function(currentDelay){
    return agentParams.noDelays ? 0 :
      {naive: currentDelay + 1, sophisticated: 0 }[agentParams.sophisticatedOrNaive];
  };
  
  var incrementDelay = function(currentDelay){
    return agentParams.noDelays ? 0 : currentDelay + 1;
  };

  // *currentBelief* is ERP on latent states, returns posterior ERP on latent states
  var updateBelief = dp.cache(
    function (manifestState, currentBelief, observation) {
      return Enumerate(function () {
        var latentState = sample(currentBelief);
        var state = buildState(manifestState, latentState);
        condition(_.isEqual(observe(state), observation));
        return latentState;
      });
    });

  var _agent = dp.cache(
    function (manifestState, currentBelief, observation, delay) {
      assert.ok(isGreaterZero(manifestState.timeLeft) && isERP(currentBelief) && _.isFinite(delay), 'agent args fail' +
                'agent args   ' + JSON.stringify(manifestState) + JSON.stringify(sample(currentBelief)));


      
      var newBelief = updateBelief(manifestState, currentBelief, observation);
      return Enumerate(function () {
        var action = uniformDraw(manifestStateToActions(manifestState));
        var eu = _expectedUtility(manifestState, newBelief, action, delay);
        factor(agentParams.alpha * eu);
        return { action: action, belief: newBelief };
      });
    });
  
  var agent = function (manifestState, currentBelief, observation) {
    return _agent(manifestState, currentBelief, observation, 0);
  };

  var _expectedUtility = dp.cache(
    function (manifestState, currentBelief, action, delay) {
      return expectation(
        Enumerate(function () {
         

          var latentState = sample(currentBelief);
          var state = buildState(manifestState, latentState);
          var u = 1.0 / (1 + agentParams.discount * delay) * utility(state, action);
          if (state.manifestState.dead) {
            return u;
          } else {
            var nextState = transition(state, action);
            var perceivedDelay = getPerceivedDelay(delay);
            var nextAction = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));

            var futureU = _expectedUtility(nextState.manifestState, nextAction.belief, nextAction.action, incrementDelay(delay));
                                                                                                                        
            return u + futureU;
          }
        }));
    });

  var expectedUtility = function (manifestState, currentBelief, action) {
    return _expectedUtility(manifestState, currentBelief, action, 0);
  };

  return { agent : agent, _expectedUtility : _expectedUtility, agentParams: agentParams};
};



var simulateBeliefDelayAgent = function (startState, world, agent, actualTotalTime, outputStatesOrActions) {
  var perceivedTotalTime = startState.manifestState.timeLeft;
  assert.ok( actualTotalTime <= perceivedTotalTime && isState(startState), 'simulate args');
  assert.ok( perceivedTotalTime  > 1, 'perceivedTime<=1. If=1 then should have state.dead, but then simulate wont work');

  var agentAction = agent.agent;
  var priorBelief = agent.agentParams.priorBelief;
  var transition = world.transition;
  var observe = world.observe;

  var cutoffCondition = isGridworld(world) ? function (actualTimeLeft, state) {return actualTimeLeft == 0 || state.dead;} :
      function (actualTimeLeft, state) {return actualTimeLeft == 0;};

  

  var sampleSequence = function(state, currentBelief, actualTimeLeft) {
    if (cutoffCondition(actualTimeLeft, state.manifestState) ) {
      return [];
    } else {
      var nextAction = sample(agentAction(state.manifestState, currentBelief, observe(state)));
      
      var nextState = transition(state, nextAction.action);
      var out = {states:state, actions:nextAction, both:[state, nextAction],
                 stateBelief: [state, currentBelief]}[outputStatesOrActions]; // could return observations
      
      return [out].concat( sampleSequence(nextState, nextAction.belief, actualTimeLeft - 1));
    }
  };
  return sampleSequence(startState, priorBelief, actualTotalTime);
};


// TODO. grab the beliefERPs so this works in belief setting also
var getExpectedUtilities = function (startState, world, agent, actualTotalTime){
  assert.ok( !agent.agentParams.noDelays, 'delays switched off');
  
  var trajectory = simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'stateBelief');
  
  var expectedUtility = agent._expectedUtility;
  var stateToActions = world.manifestStateToActions;

  
  
  var getExpectedUtilityFromTimestep = function(t){
    var trajectoryAfterT = trajectory.slice(t, trajectory.length);
    var len = trajectoryAfterT.length;
    
    return map(function(stateBelief){
      var manifestState = stateBelief[0].manifestState;
      var belief = stateBelief[1];
      var timeLeft = manifestState.timeLeft;

      return [stateBelief[0], 
              map(function(a){
                return expectedUtility(manifestState, belief, a, len - timeLeft);
              }, stateToActions(manifestState))
             ];
    }, trajectoryAfterT);
  };

  var timestepToExpectedUtilities = map(getExpectedUtilityFromTimestep, _.range(trajectory.length));

  return {trajectory: trajectory, timestepToExpectedUtilities: timestepToExpectedUtilities};
  
};


var printOut = function ( trajectory ) {
  console.log('trajectory', map( function (state) {return state.manifestState.loc;}, trajectory) );
  //console.log('expUtilities', out.startEU);
};



// Multi-arm deterministic bandits.
// Rewards of arms are fixed by the latent state. 
var makeBandits = function (numArms) {
  var actions = _.range(numArms);
  
  var mdpTransition = function (state, action) {
    var dead = state.timeLeft - 1 == 1 ? true : false;
    return update(state, { loc:action, timeLeft:state.timeLeft - 1, dead:dead });
  };
  
  var manifestStateToActions = function (manifestState) {return actions;};

  var transition = function(state, action){
    return buildState( mdpTransition(state.manifestState, action), state.latentState);
  };

  var observe = function(state){ // agent observes the reward of arm when he chooses it
    return state.latentState[state.manifestState.loc];
  };
  
  return {manifestStateToActions: manifestStateToActions, transition:transition, observe:observe};
};


// *armToRewards* is the actual rewards for each arm (true latentState)
// *priorBelief* is agent's belief about rewards, which must have true latentState in support
var runBandits = function (numArms, armToRewards, priorBelief, discount, noDelays, perceivedTotalTime){
  
  map( function(n){assert.ok(_.isFinite(armToRewards[n]),'check armTo')}, _.range(numArms) );
  var world = makeBandits(numArms);

  // agent params 
  assert.ok( _.isFinite(priorBelief.score([],armToRewards)), "actual latent not in prior's support" )
  
  var agentParams = { 
    utility : function (state,action) {return state.latentState[state.manifestState.loc];}, // utility == reward
    alpha: 100,
    discount: discount,
    sophisticatedOrNaive: 'naive',
    priorBelief: priorBelief,
    noDelays: noDelays
  };

  var agent = makeBeliefDelayAgent(agentParams, world);

  var actualTotalTime = perceivedTotalTime;
  var startState = {manifestState:{loc:'start', timeLeft: perceivedTotalTime, dead:false}, 
                    latentState: armToRewards};

  var agent = makeBeliefDelayAgent(update(agentParams,{noDelays:false}),world);
  var out = getExpectedUtilities(startState, world, agent, actualTotalTime);
  //console.log(JSON.stringify(out.timestepToExpectedUtilities));
  //ash();
  map( function(EUs){
    console.log('EUs: ', EUs, ' \n\n');
  }, out.timestepToExpectedUtilities);
  ash();
  return simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'states');
};


var testBandits = function () {

  // Agent thinks 0 is likely better. It is better and so agent stays
  var numArms = 2;
  var armToRewards = {'start':0, 0:10, 1:5}; 
  var priorBelief = Enumerate(function(){
    return flip(.8) ? armToRewards : update(armToRewards,{1:15});
  });
  var discount = 0;
  var noDelays = true;
  var perceivedTotalTime = 3;
  var trajectory = runBandits(numArms, armToRewards, priorBelief, discount, noDelays, perceivedTotalTime);
  map( function(index){assert.ok( trajectory[index].manifestState.loc == 0);}, [1,2] );

  
  

  // Discounting agent will explore a risky arm less
  var testDiscount = function(discount){
    var numArms = 2;
    var armToRewards = {'start':0, 0:1, 1:5};
    
    var priorBelief = Enumerate(function(){
      var utility1 = uniformDraw([-10, 5]);
      return {'start':0, 0:1, 1:utility1};
    });
    var noDelays = false;
    var perceivedTotalTime = 5;
   
    return last(runBandits(numArms, armToRewards, priorBelief, discount, noDelays, perceivedTotalTime)).manifestState.loc;
  };
  assert.ok( testDiscount(0) == 1, 'testdiscount');
  assert.ok( testDiscount(2) == 0, 'testdiscount');

  console.log('passed testbandits');  
};
  
testBandits();                                                       
ash();


var testLine = function () {

  var makeLine = function (noiseProb) {
    var detTransition = function (state, action) {
      var newLoc = (state.loc == 0 & action == -1) ? 0 : state.loc + action;
      var dead = state.timeLeft - 1 == 1 ? true : false;
      assert.ok(state.timeLeft != 0, 'detTransition for makeLine');
      return update(state, { loc:newLoc, timeLeft:state.timeLeft - 1, dead:dead });
    };

    var stochasticTransition = function (state, action) {
      return flip(noiseProb) ? detTransition(state, uniformDraw([-1, 1])) : detTransition(state, action);
    };

    var transition = noiseProb > 0 ? stochasticTransition : detTransition;
    var stateToActions = function (state) {return [-1, 1];};

    return { stateToActions: stateToActions, transition: transition };
  };


  var noiseProb = 0;
  var world = makeLine(noiseProb);

  // agent params
  var utility = function (world, state, action) {
    if (state.loc == 1) {return -4;}
    return state.loc;
  };

  var alpha = 100;
  var start = { loc:0, dead:false };

  var smallTrajectory = function () {
    var agent = makeHyperbolicDiscounter(utility, alpha, 0, 'naive', world);
    var trajectory = mdpSim(start, world, agent, 2, 2).trajectory;
    console.log('start, trajectory', JSON.stringify(start), JSON.stringify(trajectory) );
    assert.ok( _.isEqual([update(start, { timeLeft:2 }), update(start, { timeLeft:1, dead:true })],
                         trajectory), 'smallTraj test');
  };
  smallTrajectory();


  // test length==4 trajectories
  var mediumTrajectory = function (noiseProb) {
    var world = makeLine(noiseProb);
    map( function (sophisticatedOrNaive) {
      var perceivedTotalTime = 4;
      var actualTotalTime = 4;

      var runSim = function (discount) {
        var agent = makeHyperbolicDiscounter(utility, alpha, discount, sophisticatedOrNaive, world);
        return mdpSim(start, world, agent, actualTotalTime, perceivedTotalTime);
      };

      assert.ok( runSim(0).trajectory[1].loc == 1, 'no discount go to 1' + ' naiveOr:' + sophisticatedOrNaive );
      assert.ok( runSim(.1).trajectory[1].loc == 1, 'small discount go to 1' + ' naiveOr:' + sophisticatedOrNaive );
      assert.ok( runSim(1).trajectory[1].loc == 0, 'bigger discount stay' + ' naiveOr:' + sophisticatedOrNaive );
      assert.ok( runSim(2).trajectory[1].loc == 0, 'even bigger discount stay' + '  naiveOr:' + sophisticatedOrNaive);
    }, ['naive', 'sophisticated']);
  };

  map(mediumTrajectory, [0, .01]);
  console.log('Passed testLine');
};












// version of makeHyperbolic with two expectations
var makeHyperbolicDiscounterDoubleExpectation = function (utility, alpha, discount, sophisticatedOrNaive, world, priorBelief) {
  
  var _agent = dp.cache( 
    function(manifestState, currentBelief, observation, delay){

      var newBelief = updateBelief(manifestState, currentBelief, observation);      

      return Enumerate(function(){
        var action = uniformDraw( stateToActions(state) );
        var expectedUtility = expectation(
          Enumrate(function(){
            var state = buildState(manifestState, sample(newBelief));
            assert.ok(state.manifestState == manifestState, 'didnt build state correctly');
            return _expUtility(state, action, newBelief, delay);   
          }));

        factor(alpha * eu);
        return {action: action, belief:newBelief};
      });      
    });

  var _expUtility = dp.cache(
    function(state, action, currentBelief, delay){
      var u = 1.0/(1 + discount*delay) * utility(world, state, action);
      
      if (state.manifestState.dead){
        return u; 
      } else {                     
        return u + expectation( Enumerate(function(){
          var nextState = transition(state, action); 
          var perceivedDelay = { naive : delay + 1, sophisticated : 0}[sophisticatedOrNaive]; 
          var agentNext = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));
     
          return _expUtility(nextState, agentNext.action, agentNext.belief, delay+1);  
        }));
      }                      
    });
 
};



null

