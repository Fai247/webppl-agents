

// Helper functions for testing argument types (warning: heuristics only)
var isGreaterZero = function (x) {return _.isFinite(x) && x > 0;};
var isERP = function (x) {return x.hasOwnProperty('score') && x.hasOwnProperty('sample');};
var isState = function (x) {x.hasOwnProperty('latentState') && _.isFinite(x.manifestState.timeLeft);};
var isGridworld = function(world){return arraysEqual(world.actions, ['l', 'r', 'u', 'd']);};
var isWorld = function (x) { return x.hasOwnProperty('transition') && x.hasOwnProperty('manifestStateToActions') &&
                             x.hasOwnProperty('observe');};

var inSupport = function(x, erp){return _.isFinite( erp.score([], x) ); };


                    

// POMDP AGENT AND WORLD


// Helper function for building state from its components
var buildState = function (manifestState, latentState) {
  return { manifestState:manifestState, latentState:latentState };
};

var downToOne = function(n){
  if (n==0){return [];}
  else {return [n].concat(downToOne(n-1));}
};

var getLocs = function(trajectory){
  assert.ok(_.isArray(trajectory) && isState(trajectory[0]), 'getLocs args');
  return _.pluck( _.pluck(trajectory,'manifestState'), 'loc' );
};

var locsToManifestStates = function(locs){
  return map( function(locTime){ 
    var dead = locTime[1]==1;
    return {loc: locTime[0], timeLeft:locTime[1], dead: dead};
  }, zip( locs, downToOne(locs.length) ) );
};

var stateActionToFullStates = function(locAction, latentState){
  var locs = map(first,locAction);
  var manifestStates = locsToManifestStates(locs);
  var fullStates = map( function(manifest){
    return buildState(manifest,latentState);
    }, manifestStates);
  
  return zip( fullStates, map(second,locAction) );
};



var makeBeliefDelayAgent = function (agentParams, world){
  map(function(s){assert.ok(agentParams.hasOwnProperty(s),'makeBeliefDelayAgent args');}, 
      ['utility','alpha','discount','sophisticatedOrNaive', 'priorBelief']);
  assert.ok( isWorld(world), 'world argument lacks transition, stateToActions, or observe');

  var utility = agentParams.utility;
  
  var manifestStateToActions = world.manifestStateToActions;
  var transition = world.transition;
  var observe = world.observe;

  // Set all delays to zero to speed up runtime
  var getPerceivedDelay = function(currentDelay){
    return agentParams.noDelays ? 0 :
      {naive: currentDelay + 1, sophisticated: 0 }[agentParams.sophisticatedOrNaive];
  };
  
  var incrementDelay = function(currentDelay){
    return agentParams.noDelays ? 0 : currentDelay + 1;
  };

  // *currentBelief* is ERP on latent states, returns posterior ERP on latent states
  var updateBelief = dp.cache(
    function (manifestState, currentBelief, observation) {
      return Enumerate(function () {
        var latentState = sample(currentBelief);
        var state = buildState(manifestState, latentState);
        condition(_.isEqual(observe(state), observation));
        return latentState;
      });
    });

  var _agent = dp.cache(
    function (manifestState, currentBelief, observation, delay) {
      assert.ok(isGreaterZero(manifestState.timeLeft) && isERP(currentBelief) && _.isFinite(delay), '_agent args fail');
      
      var newBelief = updateBelief(manifestState, currentBelief, observation);
      return Enumerate(function () {
        var action = uniformDraw(manifestStateToActions(manifestState));
        var eu = expectedUtility(manifestState, newBelief, action, delay);
        factor(agentParams.alpha * eu);
        return { action: action, belief: newBelief };
      });
    });
  
  var agent = function (manifestState, currentBelief, observation) {
    return _agent(manifestState, currentBelief, observation, 0);
  };

  var expectedUtility = dp.cache(
    function (manifestState, currentBelief, action, delay) {
      return expectation(
        Enumerate(function () {
          var latentState = sample(currentBelief);
          var state = buildState(manifestState, latentState);
          var u = 1.0 / (1 + agentParams.discount * delay) * utility(state, action);
          if (state.manifestState.dead) {
            return u;
          } else {
            var nextState = transition(state, action);
            var perceivedDelay = getPerceivedDelay(delay);
            var nextAction = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));
            var futureU = expectedUtility(nextState.manifestState, nextAction.belief, nextAction.action, incrementDelay(delay));                                                                                                                
            return u + futureU;
          }
        }));
    });

  return { agent : agent, _agent:_agent, expectedUtility : expectedUtility, agentParams: agentParams};
};



var simulateBeliefDelayAgent = function (startState, world, agent, actualTotalTime, outputStatesOrActions) {
  var perceivedTotalTime = startState.manifestState.timeLeft;
  assert.ok( actualTotalTime <= perceivedTotalTime && isState(startState), 'simulate args');
  assert.ok( perceivedTotalTime  > 1, 'perceivedTime<=1. If=1 then should have state.dead, but then simulate wont work');

  var agentAction = agent.agent;
  var priorBelief = agent.agentParams.priorBelief;
  var transition = world.transition;
  var observe = world.observe;
  var outputTable = function(state, nextAction, currentBelief, observation, outputStatesOrActions){
    return {states:state, actions:nextAction.action, both:[state, nextAction.action], 
            stateBelief: [state, currentBelief],
            stateBeliefObservationAction: [{state:state, currentBelief:currentBelief, observation:observation},
                                           nextAction.action]
           }[outputStatesOrActions];
  };

  
  var cutoffCondition = isGridworld(world) ? function (actualTimeLeft, state) {return actualTimeLeft == 0 || state.dead;} :
      function (actualTimeLeft, state) {return actualTimeLeft == 0;};


  var sampleSequence = function(state, currentBelief, actualTimeLeft) {
    if (cutoffCondition(actualTimeLeft, state.manifestState) ) {
      return [];
    } else {
      var observation = observe(state);
      var nextAction = sample(agentAction(state.manifestState, currentBelief, observation));
      var nextState = transition(state, nextAction.action);

      var out = outputTable(state, nextAction, currentBelief, observation, outputStatesOrActions);
      
      return [out].concat( sampleSequence(nextState, nextAction.belief, actualTimeLeft - 1));
    }
  };
  return sampleSequence(startState, priorBelief, actualTotalTime);
};


// Run *simulate*. Then use *expectedUtility* to compute the expected
// utilities of each state along the trajectory. Do this from the
// perspective of each timestep. For discounting agents, the expected
// utilities for the same state/timeLeft will change depending on how
// close the agent is to the state (as measured by *delay*).

var getExpectedUtilitiesBeliefDelayAgent = function (startState, world, agent, actualTotalTime){
  assert.ok( !agent.agentParams.noDelays, 'Delays switched off. This function uses variation in delays.');
  
  var trajectory = simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'stateBelief');
  
  var expectedUtility = agent.expectedUtility;
  var stateToActions = world.manifestStateToActions;

  
  var getExpectedUtilityFromTimestep = function(t){
    var trajectoryAfterT = trajectory.slice(t, trajectory.length);
    var len = trajectoryAfterT.length;
    
    return map(function(stateBelief){
      var manifestState = stateBelief[0].manifestState;
      var belief = stateBelief[1];
      var timeLeft = manifestState.timeLeft;
      var delay = len - timeLeft;
      
      return [stateBelief[0], 
              map(function(a){
                return expectedUtility(manifestState, belief, a, delay);
              }, stateToActions(manifestState))
             ];
    }, trajectoryAfterT);
  };

  var expectedUtilities = map(getExpectedUtilityFromTimestep, _.range(trajectory.length));

  return {trajectory: trajectory, expectedUtilities: expectedUtilities};
  
};


// Helper functions to display outputs of *simulate* and *getExpectedUtilities*
var displayTrajectory = function ( trajectory ) {
  console.log('trajectory (locations only)',
              map( function (state) {return state.manifestState.loc;}, trajectory) );
};

var displayExpectedUtilities = function(timeToEUs){
  map( function(EUs){
    console.log('\n\n Next timestep: ');
    map(function(s){console.log(s[0].manifestState.loc, s[1]);}, EUs);
  }, timeToEUs);
};
  


// Multi-arm deterministic bandits.
// Rewards of arms are fixed by the latent state. 
var makeIRLBanditWorld = function (numArms) {
  var actions = _.range(numArms);
  
  var mdpTransition = function (state, action) {
    var dead = state.timeLeft - 1 == 1 ? true : false;
    return update(state, { loc:action, timeLeft:state.timeLeft - 1, dead:dead });
  };
  
  var manifestStateToActions = function (manifestState) {return actions;};

  var transition = function(state, action){
    return buildState( mdpTransition(state.manifestState, action), state.latentState);
  };

  var observe = function(state){ // agent observes the reward of arm when he chooses it
    if (state.manifestState.loc=='start'){return 'noObservation';}
    return state.latentState[state.manifestState.loc];
  };
  
  return {manifestStateToActions: manifestStateToActions, transition:transition, observe:observe};
};



// IRL-bandits

// We have a set of prizes ['a','b','c']. The agent is unsure of the prize
// for some of the arms. We (doing inference) are unsure of some of the
// utilities for prizes. 

// prizeToUtility = {a: 0, b: 5, ... }
// the true mapping from arms to prizes must be in agent's prior
// (for convenience, we can often let the true mapping be 0:a, 1:b, ...
// i.e. zip( _.range(numArms), ['a','b','c', ...].slice(...))

// form of latentState: {arm:prize}, usually, {0:'a', ....}

// we could implement stochastic version with stochastic utilities
// (where the utility is then baked into the observation). 


var noDiscountBaseAgentParams = { // doesn't contain priorBelief or utility
  alpha: 100,
  noDelays: true,
  discount: 0,
  sophisticatedOrNaive: 'naive'
};


var makeIRLBanditAgent = function(prizeToUtility, agentParams, worldAndStart){
  var priorBelief = agentParams.priorBelief;
  assert.ok( _.isFinite(priorBelief.score([],worldAndStart.startState.latentState)), 
             'makeIRLBanditAgent does not have true latent in support');
  
  var utility = function(state,action){
    var loc = state.manifestState.loc;
    if (loc=='start'){return 0;};
    return prizeToUtility[state.latentState[loc]];
  };
  return makeBeliefDelayAgent(update(agentParams,{utility:utility}), worldAndStart.world);
};

var makeIRLBanditWorldAndStart = function(numArms, armToPrize, perceivedTotalTime){
  map( function(i){assert.ok( _.isString(armToPrize[i]), 'makeIRLBanditWorld args' );},
       _.range(numArms));
  
  var actualTotalTime = perceivedTotalTime;
  var world = makeIRLBanditWorld(numArms);
  var startState = {manifestState:{loc:'start', timeLeft: perceivedTotalTime, dead:false}, 
                    latentState: armToPrize};

  return {world:world, startState: startState, actualTotalTime:actualTotalTime};
};


// Inference by sampling full trajectories or by doing 'off policy' inference

var inferIRLBandit = function(worldAndStart, baseAgentParams, prior, observedStateAction, trajectoryOrOffPolicy, numRejectionSamples){
  var world = worldAndStart.world;
  var startState = worldAndStart.startState;
  var actualTotalTime = worldAndStart.actualTotalTime;
  var observe = world.observe;

  var priorPrizeToUtility = prior.priorPrizeToUtility;
  var priorAgentPrior = prior.priorAgentPrior;

  assert.ok(isWorld(world) && isState(startState) && isERP(priorPrizeToUtility) && isERP(priorAgentPrior), 'inferirlbandit args');
  assert.ok(trajectoryOrOffPolicy == 'trajectory' || trajectoryOrOffPolicy=='offPolicy', 'trajectoryOrOffPolicy bad');
  if (actualTotalTime != observedStateAction.length){console.log( 'Warning: actualTotalTime is not observed trajectory length');}
  assert.ok( isState(observedStateAction[0][0]), 'fullstate in trajectory for inferirlbandit');

  
  return Enumerate(function(){
    // priors and makeAgent are specific to IRL Bandits
    var prizeToUtility = sample(priorPrizeToUtility);
    var priorBelief = sample(priorAgentPrior);
    
    var agent = makeIRLBanditAgent(prizeToUtility, update(baseAgentParams, {priorBelief:priorBelief}), worldAndStart);
    var agentAction = agent.agent;

    // Factor on whole sampled trajectory (SLOW IF NOT DETERMINISTIC AND NUM SAMPLES HIGH)
    var factorOnTrajectory = function(){
      var trajectoryERP = Rejection( function(){
        return simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'states')}, numRejectionSamples);
      factor( trajectoryERP.score( [], map(first, observedStateAction)) );
    };

    // Move agent through observed sequence 
    var conditionSequence = function(currentBelief, index){
      if (index >= observedStateAction.length){ 
        return []; // no-op
      } else {
        var state = observedStateAction[index][0];
        var observedAction = observedStateAction[index][1];
        var agentERP = agentAction(state.manifestState, currentBelief, observe(state));
        
        condition( _.isEqual( sample(agentERP).action, observedAction ) );
        
        conditionSequence( sample(agentERP).belief, index + 1);
      }
    };

    var doInfer = (trajectoryOrOffPolicy=='trajectory') ? factorOnTrajectory() : conditionSequence(priorBelief, 0);
    
    return {prizeToUtility: prizeToUtility, priorBelief:priorBelief};
  });
};


    


var testInferIRLBandit = function(){
  // Prizes are [a,b]. If agent chooses 0, then they prefer 'a'. 
  
  // world params
  var numArms = 2;
  var armToPrize = {0:'a', 1:'b'};
  var perceivedTotalTime = 3;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  
  // agent params
  var baseAgentParams = noDiscountBaseAgentParams;

  var prior = {
    priorPrizeToUtility: Enumerate(function(){
      return categorical( [.5, .5], [{a:0, b:1}, {a:1, b:0} ] );
    }),
    
    priorAgentPrior: Enumerate(function(){return deltaERP({0:'a', 1:'b'});})
  };

  // EXAMPLE 1
  var observedStateAction = [['start',1], [1,1], [1,1]]; 
  var latentState = armToPrize;
  var fullObservedStateAction = stateActionToFullStates(observedStateAction, latentState);
  
  // Test on two different inference functions
  var erp1 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10); 
  var erp2 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction,'offPolicy', 10);
  assert.ok( sample(erp1).prizeToUtility.a == 0 && sample(erp2).prizeToUtility.a == 0, 'testbandit infer 1');

  
  // EXAMPLE 2
  var observedStateAction = [['start',0], [0,0], [0,0]]; 
  var observedStates = map(first, observedStateAction);
  var fullObservedStateAction = stateActionToFullStates(observedStateAction, latentState);

  var erp1 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10); 
  var erp2 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction,'offPolicy', 10);
  assert.ok( sample(erp1).prizeToUtility.a == 1 && sample(erp2).prizeToUtility.a == 1, 'testbandit infer 2');

  console.log('passed easy testBanditInferBeliefOnly tests');
  
  
  // EXAMPLE 3 - INFER PRIOR AND UTILITY
  
  // Two arms: {0:a, 1:c}. Agent stays at 0. 
  // Explanation: u(a) high and prior that 1:b is low.

  // True armToPrize: 0:a, 1:c
  // True priorBelief:  0:a, 1:categorical([.05,.95],[b,c])
  // True utilities: {a:10, b:20, c:1}
  
  // world params
  var numArms = 2;
  var armToPrize = {0:'a', 1:'c'};
  var perceivedTotalTime = 5;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  
  // agent params
  var baseAgentParams = noDiscountBaseAgentParams;

  // Prior on agent's prizeToUtility
  var truePrizeToUtility = {a:10, b:20, c:1};
  var priorPrizeToUtility = Enumerate(function(){
    return {a: uniformDraw([0,3,10]), b:20, c:1};
  });
  
  // Prior on agent's prior
  var trueAgentPrior = Enumerate(function(){
    return {0:'a', 1: categorical([.05, .95], ['b','c']) };
  });
  var falseAgentPrior = Enumerate(function(){
    return {0:'a', 1: categorical([.5, .5], ['b','c']) };
  });

  var priorAgentPrior = Enumerate(function(){
    return flip() ? trueAgentPrior : falseAgentPrior;
  });
  
  var prior = {priorPrizeToUtility: priorPrizeToUtility, priorAgentPrior: priorAgentPrior};

  var latentState = armToPrize;
  var observedStateAction = [['start',0], [0,0], [0,0], [0,0], [0,0]]; 
  var fullObservedStateAction = stateActionToFullStates(observedStateAction, latentState);

  
  var out1 = timeit( function(){return inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10)});
  var out2 = timeit( function(){return inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'offPolicy', 10)});
  console.log('Time for Example 3, 2 arms and time 5: [from states, offpolicy]', out1.runtimeInMilliseconds, out2.runtimeInMilliseconds);
  var testERP = function(erp){
    var out = sample(erp);
    assert.ok( out.prizeToUtility.a==10  && _.isFinite( out.priorBelief.score([], armToPrize)),
               'testbandit inferbelief example 4' );
  };
  map(testERP,[out1.value,out2.value]);


  // He goes to 1 every time => u(a)==0=
  var observedStateAction = [['start',1], [1,1], [1,1], [1,1], [1,1]]; 
  var fullObservedStateAction = stateActionToFullStates(observedStateAction, latentState);

  var erp1 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction, 'trajectory', 10); 
  var erp2 = inferIRLBandit( worldAndStart, baseAgentParams, prior, fullObservedStateAction,'offPolicy', 10);

  var testERP = function(erp){
    var out = sample(erp);
    assert.ok( out.prizeToUtility.a == 0, 'testbandit inferbelief example 5' );
  };
  map(testERP,[erp1,erp2]);
 
  console.log('passed ALL testBanditInferBeliefOnly tests');
};
testInferIRLBandit();




var runIRLBanditExamples = function(){

  // Prizes are [a,b] and agent prefers c > a > b
  
  // world params
  var numArms = 2;
  var armToPrize = {0:'a', 1:'b'};
  var perceivedTotalTime = 3;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  var world = worldAndStart.world;
  var startState = worldAndStart.startState;

  // agent params
  var prizeToUtility = {a:10, b:5, c:100};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive'
  };

  var getTrajectory = function(priorBelief){
    var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
    var agent = makeIRLBanditAgent(prizeToUtility, agentParams, worldAndStart);
    return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');
  };

  // Agent knows armToPrize, picks arm with best prize
  var trajectory = getTrajectory( Enumerate(function(){return armToPrize;}));
  assert.ok( trajectory[1].manifestState.loc == 0, 'fail banditexample 1');

  // Agent has .5 chance on arm 1 having best prize c, and so tries 1 before switching to 0. 
  var trajectory = getTrajectory(Enumerate(
    function(){
      return categorical( [.5, .5], [armToPrize, {0:'a', 1:'c'}]);
    }));
  assert.ok( trajectory[1].manifestState.loc == 1 && trajectory[2].manifestState.loc == 0, 'fail banditexample 2');
  

  
  // --------------------------------
  // Example: Each arm independently either gives a or b. Since b is better, agent keeps exploring to try
  // to get it.
  
  // world params
  var numArms = 4;
  var armToPrize = {0:'a', 1:'a', 2:'a', 3:'a'};
  var perceivedTotalTime = 6;
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, perceivedTotalTime);
  var world = worldAndStart.world;
  var startState = worldAndStart.startState;

  // agent params
  var prizeToUtility = {a:5, b:10, c:-8};
  var baseAgentParams = {
    priorBelief: null,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive'
  };
  
  // Agent thinks each arm might offer c, and so tries them all (as c is so good -- and b is not that bad)  
  var priorBelief = Enumerate(
    function(){
      var dist = function(){return categorical([0.5, 0.5], ['a','b']);};
      return {0:dist(), 1:dist(), 2:dist(), 3:dist()};
    });
  
  var agentParams = update(baseAgentParams, {priorBelief: priorBelief});
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var trajectory = simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');

  var locs = getLocs(trajectory).slice(1,5);
  assert.ok( _.difference(_.range(4),locs).length == 0, 'fail bandit example 3');

  // ----------
  // Discounting example: agent thinks arms other than 0 could have b, with u=10, or
  // c, with u=-8. Not discounter will explore but discounter will just take 0, which
  // is known to have utility 5. (All arms still yield prize a). 

  var priorBelief = Enumerate(
    function(){
      var dist = function(){return categorical([.02, 0.49, 0.49], ['a', 'b','c']);};
      return {0:'a', 1:dist(), 2:dist(), 3:dist()};
    });
  
  // No discounting
  var agentParams = update(baseAgentParams, {priorBelief:priorBelief});
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var thunk = function(){return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');};
  var out = timeit(thunk);
  var locs = getLocs(out.value).slice(1,4);
  assert.ok(  _.difference([1,2,3],locs).length == 0, 'fail bandit example 4');
  
  console.log('\n No discounting: (locs, timeit) ', getLocs(out.value), out.runtimeInMilliseconds);

  // Discounting
  var replaceParams = {discount:4, noDelays:false, priorBelief:priorBelief};
  var agentParams = update(baseAgentParams, replaceParams);
  var agent = makeIRLBanditAgent(prizeToUtility, agentParams,  worldAndStart);
  var thunk = function(){return simulateBeliefDelayAgent(startState, world, agent, worldAndStart.actualTotalTime, 'states');};
  var out = timeit(thunk);
  var locs = getLocs(out.value).slice(1,4);
  assert.ok(  _.difference([0,0,0],locs).length == 0, 'fail bandit example 5');
  console.log( '\n Discounting: (locs, timeit) ', getLocs(out.value), out.runtimeInMilliseconds);
  console.log('runIRLBanditExamples passed');
};

runIRLBanditExamples();
ash();











null

