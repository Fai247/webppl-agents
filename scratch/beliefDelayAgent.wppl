
// TODO for beliefDelay agent
// 3. Add some inference examples and add agent to /src
// 4. Add myopia and bound VOI agents. 

// Helper functions for testing argument types (warning: heuristics only)
var isGreaterZero = function (x) {return _.isFinite(x) && x > 0;};
var isERP = function (x) {return x.hasOwnProperty('score') && x.hasOwnProperty('sample');};
var isState = function (x) {x.hasOwnProperty('latentState') && _.isFinite(x.manifestState.timeLeft);};
var isGridworld = function(world){return arraysEqual(world.actions, ['l', 'r', 'u', 'd']);};
var isWorld = function (x) { return x.hasOwnProperty('transition') && x.hasOwnProperty('manifestStateToActions') &&
                             x.hasOwnProperty('observe');};

                    

// POMDP AGENT AND WORLD

// Helper function for building state from its components
var buildState = function (manifestState, latentState) {
  return { manifestState:manifestState, latentState:latentState };
};


var makeBeliefDelayAgent = function (agentParams, world){
  map(function(s){assert.ok(agentParams.hasOwnProperty(s),'makeBeliefDelayAgent args');}, 
      ['utility','alpha','discount','sophisticatedOrNaive', 'priorBelief']);
  assert.ok( isWorld(world), 'world argument lacks transition, stateToActions, or observe');

  var utility = agentParams.utility;
  
  var manifestStateToActions = world.manifestStateToActions;
  var transition = world.transition;
  var observe = world.observe;

  // Set all delays to zero to speed up runtime
  var getPerceivedDelay = function(currentDelay){
    return agentParams.noDelays ? 0 :
      {naive: currentDelay + 1, sophisticated: 0 }[agentParams.sophisticatedOrNaive];
  };
  
  var incrementDelay = function(currentDelay){
    return agentParams.noDelays ? 0 : currentDelay + 1;
  };

  // *currentBelief* is ERP on latent states, returns posterior ERP on latent states
  var updateBelief = dp.cache(
    function (manifestState, currentBelief, observation) {
      return Enumerate(function () {
        var latentState = sample(currentBelief);
        var state = buildState(manifestState, latentState);
        condition(_.isEqual(observe(state), observation));
        return latentState;
      });
    });

  var _agent = dp.cache(
    function (manifestState, currentBelief, observation, delay) {
      assert.ok(isGreaterZero(manifestState.timeLeft) && isERP(currentBelief) && _.isFinite(delay), '_agent args fail');
      
      var newBelief = updateBelief(manifestState, currentBelief, observation);
      return Enumerate(function () {
        var action = uniformDraw(manifestStateToActions(manifestState));
        var eu = expectedUtility(manifestState, newBelief, action, delay);
        factor(agentParams.alpha * eu);
        return { action: action, belief: newBelief };
      });
    });
  
  var agent = function (manifestState, currentBelief, observation) {
    return _agent(manifestState, currentBelief, observation, 0);
  };

  var expectedUtility = dp.cache(
    function (manifestState, currentBelief, action, delay) {
      return expectation(
        Enumerate(function () {
          var latentState = sample(currentBelief);
          var state = buildState(manifestState, latentState);
          var u = 1.0 / (1 + agentParams.discount * delay) * utility(state, action);
          if (state.manifestState.dead) {
            return u;
          } else {
            var nextState = transition(state, action);
            var perceivedDelay = getPerceivedDelay(delay);
            var nextAction = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));
            var futureU = expectedUtility(nextState.manifestState, nextAction.belief, nextAction.action, incrementDelay(delay));                                                                                                                
            return u + futureU;
          }
        }));
    });

  return { agent : agent, _agent:_agent, expectedUtility : expectedUtility, agentParams: agentParams};
};



var simulateBeliefDelayAgent = function (startState, world, agent, actualTotalTime, outputStatesOrActions) {
  var perceivedTotalTime = startState.manifestState.timeLeft;
  assert.ok( actualTotalTime <= perceivedTotalTime && isState(startState), 'simulate args');
  assert.ok( perceivedTotalTime  > 1, 'perceivedTime<=1. If=1 then should have state.dead, but then simulate wont work');

  var agentAction = agent.agent;
  var priorBelief = agent.agentParams.priorBelief;
  var transition = world.transition;
  var observe = world.observe;
  var outputTable = function(state, nextAction, currentBelief, observation, outputStatesOrActions){
    return {states:state, actions:nextAction.action, both:[state, nextAction.action], 
            stateBelief: [state, currentBelief],
            stateBeliefObservationAction: [{state:state, currentBelief:currentBelief, observation:observation},
                                           nextAction.action]
           }[outputStatesOrActions];
  };

  
  var cutoffCondition = isGridworld(world) ? function (actualTimeLeft, state) {return actualTimeLeft == 0 || state.dead;} :
      function (actualTimeLeft, state) {return actualTimeLeft == 0;};


  var sampleSequence = function(state, currentBelief, actualTimeLeft) {
    if (cutoffCondition(actualTimeLeft, state.manifestState) ) {
      return [];
    } else {
      var observation = observe(state);
      var nextAction = sample(agentAction(state.manifestState, currentBelief, observation));
      var nextState = transition(state, nextAction.action);

      var out = outputTable(state, nextAction, currentBelief, observation, outputStatesOrActions);
      
      return [out].concat( sampleSequence(nextState, nextAction.belief, actualTimeLeft - 1));
    }
  };
  return sampleSequence(startState, priorBelief, actualTotalTime);
};


// Run *simulate*. Then use *expectedUtility* to compute the expected
// utilities of each state along the trajectory. Do this from the
// perspective of each timestep. For discounting agents, the expected
// utilities for the same state/timeLeft will change depending on how
// close the agent is to the state (as measured by *delay*).

var getExpectedUtilitiesBeliefDelayAgent = function (startState, world, agent, actualTotalTime){
  assert.ok( !agent.agentParams.noDelays, 'Delays switched off. This function uses variation in delays.');
  
  var trajectory = simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'stateBelief');
  
  var expectedUtility = agent.expectedUtility;
  var stateToActions = world.manifestStateToActions;

  
  var getExpectedUtilityFromTimestep = function(t){
    var trajectoryAfterT = trajectory.slice(t, trajectory.length);
    var len = trajectoryAfterT.length;
    
    return map(function(stateBelief){
      var manifestState = stateBelief[0].manifestState;
      var belief = stateBelief[1];
      var timeLeft = manifestState.timeLeft;
      var delay = len - timeLeft;
      
      return [stateBelief[0], 
              map(function(a){
                return expectedUtility(manifestState, belief, a, delay);
              }, stateToActions(manifestState))
             ];
    }, trajectoryAfterT);
  };

  var expectedUtilities = map(getExpectedUtilityFromTimestep, _.range(trajectory.length));

  return {trajectory: trajectory, expectedUtilities: expectedUtilities};
  
};


// Helper functions to display outputs of *simulate* and *getExpectedUtilities*
var displayTrajectory = function ( trajectory ) {
  console.log('trajectory (locations only)',
              map( function (state) {return state.manifestState.loc;}, trajectory) );
};

var displayExpectedUtilities = function(timeToEUs){
  map( function(EUs){
    console.log('\n\n Next timestep: ');
    map(function(s){console.log(s[0].manifestState.loc, s[1]);}, EUs);
  }, timeToEUs);
};
  



// Multi-arm deterministic bandits.
// Rewards of arms are fixed by the latent state. 
var makeBandits = function (numArms) {
  var actions = _.range(numArms);
  
  var mdpTransition = function (state, action) {
    var dead = state.timeLeft - 1 == 1 ? true : false;
    return update(state, { loc:action, timeLeft:state.timeLeft - 1, dead:dead });
  };
  
  var manifestStateToActions = function (manifestState) {return actions;};

  var transition = function(state, action){
    return buildState( mdpTransition(state.manifestState, action), state.latentState);
  };

  var observe = function(state){ // agent observes the reward of arm when he chooses it
    if (state.manifestState.loc=='start'){return 'noObservation';}
    return state.latentState[state.manifestState.loc];
  };
  
  return {manifestStateToActions: manifestStateToActions, transition:transition, observe:observe};
};


// Bandits where arm 0 has a string reward "prize0", which the agent assigns an unknown utility. 
// Other arms output numerical rewards as before. The latent state is the numerical reward
// of arms other than 0.

// Agent actions are in [0,1,...,numArms] and states are [0,1,...,numArms].
// Agent observes 'prize0' or the numerical reward for each state.
// For inference, we condition on a sequence of states or pairs of form [state,action].

// Example below:
// For inference, we don't know utility('prize0') for agent. Given a prior
// on this variable, we want to infer this from agent's actions. We know
// agent's prior on reward of arm 1. If the agent goes for arm 0 on first trial, then
// prize0 must have high utility, otherwise it has low utility. 

// We show simple+slow and complex+fast inference methods. 
var inferBanditsBelief = function(numArms, armToRewards, perceivedTotalTime, priorUtilityPrize0, observationUtilityPrize0) {


// BUILD AGENT AND BANDITS
  
  // Construct agent for this problem. Utilities are latent state values
  // expect for arm 0. Agent does NOT discount. 
  var makeAgent = function( utilityPrize0, priorBelief, armToRewards, world){
    assert.ok(_.isFinite(utilityPrize0) && _.isFinite(priorBelief.score([],armToRewards)), 'getAgent args');
    
    var utility = function(state,action){
      var loc = state.manifestState.loc;
      if (loc=='start'){return 0;};
      return loc==0 ? utilityPrize0 : state.latentState[loc];
    };
    
    var agentParams = {
      utility: utility,
      priorBelief: priorBelief,
      alpha: 100,
      noDelays: true,
      discount: 0,
      sophisticatedOrNaive: 'naive'
    };
    
    return makeBeliefDelayAgent(agentParams, world);
  };

  
  // Construct *world*, adding the special reward of prize0 for arm 0. 
  var actualTotalTime = perceivedTotalTime;
  var world = makeBandits(numArms);
  var armToRewards = update(armToRewards,{0:'prize0'});
  var startState = {manifestState:{loc:'start', timeLeft: perceivedTotalTime, dead:false}, 
                    latentState: armToRewards};
  
  map( function(n){assert.ok(!_.isUndefined(armToRewards[n]),'check armToRewards')}, _.range(numArms) );

  // Agent knows the rewards of all states
  var priorBelief = Enumerate(function(){return armToRewards;});

  
// BUILD OBSERVATIONS
  
  // Compute [state,action] trajectory for agent who has the observed trajectory
  var observedAgent = makeAgent( observationUtilityPrize0, priorBelief, armToRewards, world);
  var observedStateAction = simulateBeliefDelayAgent(startState, world, observedAgent, actualTotalTime, 'both');
  

// INFERENCE IN THIS MODEL
  
  // Simple and slow inference. Generate trajectory ERP with rejection sampling
  // and condition on state sequence. 
  var inferFromStates = function(observedStates, priorUtilityPrize0){
    return Enumerate(function(){
      var utilityPrize0 = priorUtilityPrize0();
      var agent = makeAgent(utilityPrize0, priorBelief, armToRewards, world);

      var trajectoryERP = Rejection( function(){
        return simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'states');
      }, 100);

      factor( trajectoryERP.score( [], observedStates) )

      return {utilityPrize0: utilityPrize0};
    });
  };
  
  var erpInferFromStates = inferFromStates( map(first, observedStateAction), priorUtilityPrize0);


  // Fast inference. Given trajectory of [state,action], run hypothetical
  // agent on each state, condition on action and then continue on to
  // next state in trajectory. Need to pass through agent's belief and
  // feed the agent a new observation at each iteration.

  // Input needs to have full states (not just .loc). 
  
  var inferOffPolicy = function(observedStateAction, priorUtilityPrize0){
    return Enumerate(function(){
      var utilityPrize0 = priorUtilityPrize0();
      var agent = makeAgent(utilityPrize0, priorBelief, armToRewards, world);
      var agentAction = agent.agent;
      
      var observe = world.observe;

      var factorSequence = function(currentBelief, index){
        if (index >= observedStateAction.length){ 
          return []; // no-op
        } else {
          var state = observedStateAction[index][0];
          var observedAction = observedStateAction[index][1];
          var agentERP = agentAction(state.manifestState, currentBelief, observe(state));

          condition( _.isEqual( sample(agentERP).action, observedAction ) );
          
          factorSequence( sample(agentERP).belief, index + 1);
        }
      };

      factorSequence(priorBelief, 0);
      return {utilityPrize0: utilityPrize0};
    });
  };
  
  var erpInferOffPolicy = inferOffPolicy( observedStateAction, priorUtilityPrize0 );


  // Fast inference by sampling an action given the state, belief, observation.
  // WARNING: not working. 
  
  // *stateAction* has form [  [ {state:, currentBelief:, observation: }, action] ]
  var inferFromStateBeliefs = function(priorUtilityPrize0, observationUtilityPrize0){
    
    var agent = makeAgent( observationUtilityPrize0, priorBelief, armToRewards, world);
    var observedStateActions = simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 
                                                        'stateBeliefObservationAction');
    var observedStateActions = getObservationStates(observationUtilityPrize0);
    
    return Enumerate(function(){
      var utilityPrize0 = priorUtilityPrize0();
      var _agent = makeAgent(utilityPrize0, priorBelief, armToRewards, world)._agent;

      map( function( stateAction ){
        var state = stateAction[0];
        var observedAction = stateAction[1];
 
        var agentERP = _agent(state.state.manifestState, state.currentBelief, state.observation, 0);
        var actionERP = Enumerate(function(){return sample(agentERP).action;});
        
        factor( actionERP.score( [], observedAction ));
      }, observedStateActions);
      
      return {utilityPrize0: utilityPrize0};
    });  
  };

  return {erpInferOffPolicy:erpInferOffPolicy, erpInferFromStates: erpInferFromStates};
};

// world params
var armToRewards = {1:5, 2:10};
var numArms = 3;
var perceivedTotalTime = 3;

// Prior on the agent's utility for prize0
var priorUtilityPrize0 = function(){return uniformDraw([0,5,20]);};

// Condition agent on a trajectory generated by assuming prize0 utility is 1
// (So agent will go to arm 2, which has reward 10)
var observationUtilityPrize0 = 20;

var out = inferBanditsBelief(numArms, armToRewards, perceivedTotalTime, priorUtilityPrize0, observationUtilityPrize0);
printERP(out.erpInferOffPolicy);
printERP(out.erpInferFromStates);
ash();



// var f = cache( function(x,y){return x+5;} );
// var c = f.cache;


// IRL-bandits

// We have a set of prizes ['a','b','c']. The agent is unsure of the prize
// for some of the arms. We (doing inference) are unsure of some of the
// utilities for prizes. 

// prizeToUtility = {a: 0, b: 5, ... }
// the true mapping from arms to prizes must be in agent's prior
// (for convenience, we can often let the true mapping be 0:a, 1:b, ...
// i.e. zip( _.range(numArms), ['a','b','c', ...].slice(...))

// form of latentState: {arm:prize}, usually, {0:'a', ....}

// we could implement stochastic version with stochastic utilities
// (where the utility is then baked into the observation). 

var makeBanditAgent = function(prizeToUtility, agentParams, armToPrizes, world){
  var priorBelief = agentParams.priorBelief;
  assert.ok( _.isFinite(priorBelief.score([],armToPrizes)), 'makeBanditAgent has true latent in support');
  
  var utility = function(state,action){
    var loc = state.manifestState.loc;
    if (loc=='start'){return 0;};
    return prizeToUtility( state.latentState[loc] );
  };
  
  return makeBeliefDelayAgent( update(agentParams, {utility:utility}), world);
};

var makeBanditWorldAndStart = function(numArms, armToPrizes, perceivedTotalTime){
  map( function(i){assert.ok( _.isString(armToPrizes(i)), 'makeBanditWorld args' );}, _.range(numArms));
  
  var actualTotalTime = perceivedTotalTime;
  var world = makeBandits(numArms);

  var startState = {manifestState:{loc:'start', timeLeft: perceivedTotalTime, dead:false}, 
                    latentState: armToPrizes};

  return {world:world, startState: startState, actualTotalTime:actualTotalTime};
};

var runIRLBandit = function(){
  var numArms = 2;
  var armToPrizes = {0:'a', 1:'b'};
  var perceivedTotalTime = 3;
  var worldAndStart = makeBanditWorldAndStart(numArms, armToPrizes, perceivedTotalTime);
  var world = worldAndStart.world;

  var prizeToUtility = {a:10, b:5};
  var priorBelief = Enumerate(function(){return armToPrizes;});
  var agentParams = {
    priorBelief: priorBelief,
    alpha: 100,
    noDelays: true,
    discount: 0,
    sophisticatedOrNaive: 'naive'
  };
   
  var agent = makeBanditAgent(prizeToUtility, agentParams, armToPrizes, world);

  var trajectory = simulateBeliefDelayAgent(worldAndStart.startState, 
                                            world, agent, worldAndStart.actualTotalTime, 'states');

  return trajectory;
};

console.log(runIRLBandit());




// *armToRewards* is the actual rewards for each arm (true latentState)
// *priorBelief* is agent's belief about rewards, which must have true latentState in support
var runBandits = function (numArms, armToRewards, priorBelief, discount, noDelays, perceivedTotalTime){
  
  map( function(n){assert.ok(_.isFinite(armToRewards[n]),'check armTo')}, _.range(numArms) );
  var world = makeBandits(numArms);

  // agent params 
  assert.ok( _.isFinite(priorBelief.score([],armToRewards)), "actual latent not in prior's support" )
  
  var agentParams = { 
    utility : function (state,action) {return state.latentState[state.manifestState.loc];}, // utility == reward
    alpha: 100,
    discount: discount,
    sophisticatedOrNaive: 'naive',
    priorBelief: priorBelief,
    noDelays: noDelays
  };

  var agent = makeBeliefDelayAgent(agentParams, world);

  var actualTotalTime = perceivedTotalTime;
  var startState = {manifestState:{loc:'start', timeLeft: perceivedTotalTime, dead:false}, 
                    latentState: armToRewards};

  // Display expected utilities as an informal test
  if (!agentParams.noDelays){
    var out = getExpectedUtilitiesBeliefDelayAgent( startState, world, agent, actualTotalTime);
    console.log('\n\n runBandits: display trajectory');
    displayTrajectory( map(first,(out.trajectory) ) );
    console.log('\n runBandits: display expected utilities');
    displayExpectedUtilities(out.expectedUtilities);
  }

  var out = simulateBeliefDelayAgent(startState, world, agent, actualTotalTime, 'states');
  //console.log(agent._agent.cache);
  ash();
};


var testBandits = function () {

  // Agent thinks 0 is likely better. It is better and so agent stays
  var numArms = 2;
  var armToRewards = {'start':0, 0:10, 1:5}; 
  var priorBelief = Enumerate(function(){
    return flip(.8) ? armToRewards : update(armToRewards,{1:15});
  });
  var discount = 0;
  var noDelays = true;
  var perceivedTotalTime = 3;
  var trajectory = runBandits(numArms, armToRewards, priorBelief, discount, noDelays, perceivedTotalTime);
  
  map( function(index){assert.ok( trajectory[index].manifestState.loc == 0);}, [1,2] );

  // Discounting agent will explore a risky arm less
  var testDiscount = function(discount){
    var numArms = 2;
    var armToRewards = {'start':0, 0:1, 1:5};
    
    var priorBelief = Enumerate(function(){
      var utility1 = uniformDraw([-10, 5]);
      return {'start':0, 0:1, 1:utility1};
    });
    var noDelays = false;
    var perceivedTotalTime = 5;
   
    return last(runBandits(numArms, armToRewards, priorBelief, discount, noDelays, perceivedTotalTime)).manifestState.loc;
  };
  assert.ok( testDiscount(0) == 1, 'testdiscount');
  assert.ok( testDiscount(2) == 0, 'testdiscount');

  console.log('passed testbandits');  
};
  
testBandits();                                                       
ash();





// var testLine = function () {

//   var makeLine = function (noiseProb) {
//     var detTransition = function (state, action) {
//       var newLoc = (state.loc == 0 & action == -1) ? 0 : state.loc + action;
//       var dead = state.timeLeft - 1 == 1 ? true : false;
//       assert.ok(state.timeLeft != 0, 'detTransition for makeLine');
//       return update(state, { loc:newLoc, timeLeft:state.timeLeft - 1, dead:dead });
//     };

//     var stochasticTransition = function (state, action) {
//       return flip(noiseProb) ? detTransition(state, uniformDraw([-1, 1])) : detTransition(state, action);
//     };

//     var transition = noiseProb > 0 ? stochasticTransition : detTransition;
//     var stateToActions = function (state) {return [-1, 1];};

//     return { stateToActions: stateToActions, transition: transition };
//   };


//   var noiseProb = 0;
//   var world = makeLine(noiseProb);

//   // agent params
//   var utility = function (world, state, action) {
//     if (state.loc == 1) {return -4;}
//     return state.loc;
//   };

//   var alpha = 100;
//   var start = { loc:0, dead:false };

//   var smallTrajectory = function () {
//     var agent = makeHyperbolicDiscounter(utility, alpha, 0, 'naive', world);
//     var trajectory = mdpSim(start, world, agent, 2, 2).trajectory;
//     console.log('start, trajectory', JSON.stringify(start), JSON.stringify(trajectory) );
//     assert.ok( _.isEqual([update(start, { timeLeft:2 }), update(start, { timeLeft:1, dead:true })],
//                          trajectory), 'smallTraj test');
//   };
//   smallTrajectory();


//   // test length==4 trajectories
//   var mediumTrajectory = function (noiseProb) {
//     var world = makeLine(noiseProb);
//     map( function (sophisticatedOrNaive) {
//       var perceivedTotalTime = 4;
//       var actualTotalTime = 4;

//       var runSim = function (discount) {
//         var agent = makeHyperbolicDiscounter(utility, alpha, discount, sophisticatedOrNaive, world);
//         return mdpSim(start, world, agent, actualTotalTime, perceivedTotalTime);
//       };

//       assert.ok( runSim(0).trajectory[1].loc == 1, 'no discount go to 1' + ' naiveOr:' + sophisticatedOrNaive );
//       assert.ok( runSim(.1).trajectory[1].loc == 1, 'small discount go to 1' + ' naiveOr:' + sophisticatedOrNaive );
//       assert.ok( runSim(1).trajectory[1].loc == 0, 'bigger discount stay' + ' naiveOr:' + sophisticatedOrNaive );
//       assert.ok( runSim(2).trajectory[1].loc == 0, 'even bigger discount stay' + '  naiveOr:' + sophisticatedOrNaive);
//     }, ['naive', 'sophisticated']);
//   };

//   map(mediumTrajectory, [0, .01]);
//   console.log('Passed testLine');
// };



// // version of makeHyperbolic with two expectations
// var makeHyperbolicDiscounterDoubleExpectation = function (utility, alpha, discount, sophisticatedOrNaive, world, priorBelief) {
  
//   var _agent = dp.cache( 
//     function(manifestState, currentBelief, observation, delay){

//       var newBelief = updateBelief(manifestState, currentBelief, observation);      

//       return Enumerate(function(){
//         var action = uniformDraw( stateToActions(state) );
//         var expectedUtility = expectation(
//           Enumrate(function(){
//             var state = buildState(manifestState, sample(newBelief));
//             assert.ok(state.manifestState == manifestState, 'didnt build state correctly');
//             return _expUtility(state, action, newBelief, delay);   
//           }));

//         factor(alpha * eu);
//         return {action: action, belief:newBelief};
//       });      
//     });

//   var _expUtility = dp.cache(
//     function(state, action, currentBelief, delay){
//       var u = 1.0/(1 + discount*delay) * utility(world, state, action);
      
//       if (state.manifestState.dead){
//         return u; 
//       } else {                     
//         return u + expectation( Enumerate(function(){
//           var nextState = transition(state, action); 
//           var perceivedDelay = { naive : delay + 1, sophisticated : 0}[sophisticatedOrNaive]; 
//           var agentNext = sample(_agent(nextState.manifestState, currentBelief, observe(nextState), perceivedDelay));
     
//           return _expUtility(nextState, agentNext.action, agentNext.belief, delay+1);  
//         }));
//       }                      
//     });
 
// };



null

