// this should perform inverse reinforcement learning: take multiple trajectories
// and infer the utilities of each terminal state. the inference function will condition on a simulated
// agent making the same choices in the same states as appears in the trajectories (rather than
// conditioning on a simulated agent recreating the same trajectories).

// note: this uses dp.cache, from the package webppl-dp. some testing options use timeit
// from webppl-timeit.

// TODO: optimisation, extension

// first: some helper functions

var rangeBy = function(start, end, stepSize){
    if (start > end){return [];}
    else {return [start].concat(rangeBy(start + stepSize, end, stepSize));}
};

var _arrayIndexOf = function(x, xs){
    if (arraysEqual(x, xs[0])) {return 0;}
    else {return 1 + _arrayIndexOf(x, rest(xs));}
};

var arrayIndexOf = function(x, xs){
    if (!stateInArray(x, xs)) {return undefined;}
    else {return _arrayIndexOf(x, xs);}
};

var numberOf = function(x, xs) {
    return sum(map(function(y){
	if (x===y) {return 1;}
	else {return 0;}
    }, xs));
};

var update = function(base, ext){
  return _.extend({}, base, ext);
};

var unifDiscrete = function(){
    return uniformDraw(rangeBy(-1, 1, 0.1));
};

var flippy01 = function(){
    return (flip() ? 1 : 0);
};

var flippypm = function(){
    return (flip() ? 1 : -1);
};

// these functions will take a list of trajectories and make a 'histogram array', whose elements
// are arrays with three elements:
// 0. a state that was visited in the trajectories
// 1. the time remaining for the agent when the state was visited. states visited with different
//    times remaining get different entries. we assume that each trajectory represents the whole
//    lifetime of the agent.
// 3. [number of times the agent selected 'l' in this situation,
//     number of times the agent selected 'r' in this situation,
//     number of times the agent selected 'u' in this situation,
//     number of times the agent selected 'd' in this situation]

// this function takes an action and returns a 'mini-histogram'
var miniHist = function(action) {
    if (action === 'l') {return [1, 0, 0, 0];}
    if (action === 'r') {return [0, 1, 0, 0];}
    if (action === 'u') {return [0, 0, 1, 0];}
    if (action === 'd') {return [0, 0, 0, 1];}
    else {return undefined;}
};

// this function takes a single trajectory and turns it into a histogram array
var trajToHistArray = function(trajectory) {
    var lifetime = trajectory.length;
    if (lifetime == 0) {
	return [];
    } else {
	return cons([trajectory[0][0], lifetime, miniHist(trajectory[0][1])],
		    trajToHistArray(rest(trajectory)));
    }
};

// this function checks if two entries of a histogram array have the same first two elements
var entriesMatch = function(entry1, entry2) {
    return (arraysEqual(entry1[0], entry2[0]) && entry1[1] === entry2[1]);
};

// this function checks if an entry is in a histogram array
var entryInArray = function(entry, array) {
    return any(function(entry2){return entriesMatch(entry, entry2);}, array);
};

// this function adds an entry to a histogram array
var addEntryToHistArray = function(entry, array) {
    // if the first two parts of the entry are in the array, add the minihists
    // if not, just put the entry in at the start
    if (array.length == 0) {return [entry];}
    else if (entryInArray(entry, array)) {
	return map(function(entry2){
	    if (entriesMatch(entry, entry2)) {
		return [entry[0], entry[1], map2(plus, entry[2], entry2[2])];
	    } else {
		return entry2;
	    }
	}, array);
    } else {
	return cons(entry, array);
    }
};

// this function merges two histogram arrays (by adding all the entries of the first to the second)
var mergeTwoHistArrays = function(array1, array2) {
    if (array1.length == 0) {return array2;}
    if (array2.length == 0) {return array1;}
    else {
	var newHistArray = addEntryToHistArray(array1[0], array2);
	return mergeTwoHistArrays(rest(array1), newHistArray);
    }
};

// this function takes an array of trajectories and returns a histogram array
var trajectoriesToHistArray = function(trajectories) {
    var histArrays = map(trajToHistArray, trajectories);
    return reduce(mergeTwoHistArrays, [], histArrays);
};

// this function takes an observed histogram array (obsHistArray), and parameters for the
// agent and MDP, and returns an ERP over the agent's actions in the (state, timeleft) pairs
// represented in obsHistArray. the function simulates as many actions of the agent in each
// (state, timeleft) pair as are present in obsHistArray.
// if the state is terminal, simulateAgent assumes that the agent will perform the actions
// seen in obsHistArray, since the agent's actions in these states don't provide any information
// anyway.
var simulateAgent = function(obsHistArray, params) {
// var simulateAgent = function(obsHistArray, params, startState, n) {

    var isTerminal = function(state){
	return stateInArray(state, params.terminals);
    };
    
    // defining a softMax agent
    var agent = dp.cache(
	function(state, timeLeft){
	    return Enumerate(function(){
		var action = uniformDraw(params.actions);
		var eu = expUtility(state, action, timeLeft);
		factor(params.alpha * eu);
		return action;
	    });
	}
    );

    var expUtility = dp.cache(
	function(state, action, timeLeft){
	    var utility = params.utility;
	    var u = utility(state, action);

	    if (timeLeft - 1 == 0 || isTerminal(state)){
		return u;
	    } else {
		return u + expectation( Enumerate(function(){
		    var transition = params.transition;
		    var nextState = transition(state, action);
		    var nextAction = sample(agent(nextState, timeLeft - 1));
		    return expUtility(nextState, nextAction, timeLeft - 1);
		}));
	    }
	}
    );

    // this function takes an element of observedHistArray, and returns what actions the
    // agent would take with the new utility function in the same format.
    var stateSim = function(array) {
	var state = array[0];
	if (isTerminal(state)) {
	    return array;
	} else {
	    var timeLeft = array[1];
	    var numActs = sum(array[2]);
	    var sampleAgent = function() {
		return sample(agent(state, timeLeft));
	    };
	    var newActs = repeat(numActs, sampleAgent);
	    var newActsHist =  [numberOf('l', newActs), numberOf('r', newActs),
				numberOf('u', newActs), numberOf('d', newActs)];
	    return [state, timeLeft, newActsHist];
	}
    };

    var sampleHistArray = function(){
	return map(stateSim, obsHistArray);
    };

    return Rejection(sampleHistArray, 100);
    // return sample(agent(startState, n));
};

// this function generates some number of trajectories, all of the same length and starting
// state, of an agent in an MDP.
var agentTrajectories = function(startState, length, numTrajectories, params) {

    var isTerminal = function(state){
	return stateInArray(state, params.terminals);
    };
    
    // defining a softMax agent
    var agent = dp.cache(
	function(state, timeLeft){
	    return Enumerate(function(){
		var action = uniformDraw(params.actions);
		var eu = expUtility(state, action, timeLeft);
		factor(params.alpha * eu);
		return action;
	    });
	}
    );

    var expUtility = dp.cache(
	function(state, action, timeLeft){
	    var utility = params.utility;
	    var u = utility(state, action);

	    if (timeLeft - 1 == 0 || isTerminal(state)){
		return u;
	    } else {
		return u + expectation( Enumerate(function(){
		    var transition = params.transition;
		    var nextState = transition(state, action);
		    var nextAction = sample(agent(nextState, timeLeft - 1));
		    return expUtility(nextState, nextAction, timeLeft - 1);
		}));
	    }
	}
    );

    var makeTraj = function(startState, length) {
	if (length === 0) {
	    return [];
	} else {
	    var action = sample(agent(startState, length));
	    var transition = params.transition;
	    var nextState = transition(startState, action);
	    return cons([startState, action], makeTraj(nextState, length - 1));
	}
    };

    var _makeTraj = function() {
	return makeTraj(startState, length);
    };

    return repeat(numTrajectories, _makeTraj);
    
};

// this function takes a list of trajectories, parameters for the MDP and agent, a prior thunk for utilities of
// terminal states, and options for inference, and returns an ERP over the list of utilities for each terminal state.
// the prior thunk should be a function whose outputs are distributed according to the prior over utilities of each
// terminal state. it is assumed that utilities of terminal states are independent.
// inference on single agent actions is done by enumeration, inference on the collection of actions that the agent
// takes is done by rejection sampling with 100 samples, and inference on the utilities of terminal states is done
// by MCMC
var inferUtilities = function(trajectories, params, utilPrior, options){

    var obsHistArray = trajectoriesToHistArray(trajectories);

    var terminals = params.terminals;
    
    // this is the thing that will be enumerated over to get the ERP of what the utilities are.
    // it returns an array of utilities for each terminal state (in the order they are listed in in params.terminals)
    var utilityProcess = function() {

	// generating utility function

	var uArray = repeat(terminals.length, utilPrior);
	
	// console.log(uArray);

	// we assume that the penalty for not being in a terminal state is -0.1
	var utilityFun = function(state, action){
	    if (stateInArray(state, terminals)) {
		return uArray[arrayIndexOf(state, terminals)];
	    } else if (isPostTerminal(state)) {
		return 0;
	    } else {
		return -0.1;
	    }
	};

	var newParams = update(params, {utility: utilityFun});

	// ERP over the agent's behaviour in the generated environment

	var agentERP = simulateAgent(obsHistArray, newParams);

	// testing options:
	// var agentERP = simulateAgent(obsHistArray, newParams, trajectories[0][0][0], trajectories[0].length);
	// var agentAct = simulateAgent(obsHistArray, newParams, trajectories[0][0][0], trajectories[0].length);
	
	// console.log(printERP(agentERP));
	
	// now, we do the inference part.
	
	factor(agentERP.score([], obsHistArray));

	// testing options
	
	// factor(agentAct === trajectories[0][0][1] ? 0 : -Infinity);
	// factor(agentERP.score([], 'u'));
	// factor(arraysEqual(newHist, obsHistArray) ? 0 : -Infinity);
	
	return uArray;
    };

    // now we need to enumerate over utilityProcess to get the posterior over the utilities
    // var utilityPosterior = MCMC(utilityProcess, options);
    var utilityPosterior = Enumerate(utilityProcess);

    return utilityPosterior;
};

// this is me testing stuff

// var params43 = make43(0, 10);
// var trajs43 = agentTrajectories([1,0], 5, 1, params43);
// var hist43 = trajectoriesToHistArray(trajs43);

// timeit(function(){return agentTrajectories([1,0], 5, 1, params43)})
// timeit(function(){return inferUtilities(trajs43, params43, flippypm)})

// var makeSquare = function(noiseProb, alpha, lim, n){
//     var u = function(state, action){
// 	if (gridEqual(state, [0, n])) {
// 	    return 1;
// 	} else if (gridEqual(state, [n, 0])) {
// 	    return -1;
// 	} else if (isPostTerminal(state)) {
// 	    return 0;
// 	} else {
// 	    return -0.1;
// 	}
//     };
//     var blockedStates = [];
//     var terminals = [[0,n], [n,0]];
//     return makeBlockedGridParams(lim, lim, blockedStates, terminals, u, noiseProb, alpha);
// };

// var doubleFlippy = function(){
//     if (flip()) {
// 	return flip() ? -1 : -0.5;
//     } else {
// 	return flip() ? 0.5 : 1;
//     }
// };

// var myTimer = function(n) {
//     console.log(n);
//     var squareParams = makeSquare(0.1, 4, 6, n - 1);
//     var squareTrajs = agentTrajectories([0,0], n, 1, squareParams);
//     var squareHist = trajectoriesToHistArray(squareTrajs);
    
//     var f = function() {
// 	console.log("..");
// 	return printERP(inferUtilities(squareTrajs, squareParams, flippypm));
// 	// return simulateAgent(squareHist, squareParams);
//     };

//     var g = function() {return timeit(f).runtimeInMilliseconds;};

//     return listMean(repeat(3, g));
// };

// map(myTimer, [2,3,4,5,6])

// var squareParams = makeSquare(0.1, 4, 5, 4);
// var squareTrajs = agentTrajectories([0,0], 7, 1, squareParams);
// var squareHist = trajectoriesToHistArray(squareTrajs);

// printERP(inferUtilities(squareTrajs, squareParams, flippypm))

// var myERP = simulateAgent(squareHist, squareParams);

// var f = function() {
//     return sample(myERP);
// };

// var g = function() {
//     return timeit(f).runtimeInMilliseconds;
// };

// listMean(repeat(5,g))

// printERP(simulateAgent(squareHist, squareParams, [0,0], 2))

// var donutParams = makeDonut(0.1, 1);
// var donutTrajs = agentTrajectories([2,0], 8, 5, donutParams);

// var donutPrior = function(){
//     if (flip(0.3)) {
// 	return 4;
//     } else if (flip()) {
// 	return 1;
//     } else {
// 	return -1;
//     }
// };

// var squareParams = makeSquare(0.1, 10, 20, 5);
// var squareTrajs = agentTrajectories([0,0], 6, 5, squareParams);

// var f = function() {
//     // console.log("..");
//     // return printERP(inferUtilities(squareTrajs, squareParams, flippypm));
//     return agentTrajectories([0,0], 6, 5, squareParams);
// };

// var g = function() {return timeit(f).runtimeInMilliseconds;};

// repeat(5, g)

// JSON.stringify(squareTrajs)

// printERP(inferUtilities(donutTrajs, donutParams, donutPrior, {verbose: true}))

// JSON.stringify(donutTrajs)

// donutTrajs[0].length

// printERP(simulateAgent(hist43, params43))
