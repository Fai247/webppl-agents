// note: this assumes that the probability distribution over agentParams is
// basically factorisable.

// arguments: *observedTrajectories* an array of trajectories,
// *paramsToUtilityFunction* a function which takes parameters for a utility function
// and returns the associated utility function,
// *world* the POMDP that the trajectories were generated in,
// and *priors* an object containing:
// *utilityParamsPrior* an ERP over parameters for a utility function,
// *beliefPrior* an ERP over the agent's prior over the latent state,
// *alphaPrior* an ERP over the agent's softmax noise,
// *discountPrior* an ERP over the agent's discount parameter (if zero, run with noDelays
// unless agent is myopic or has bounded VOI),
// *probSophisticated* the probability that the agent is sophisticated (conditioned on no myopia
// or bounded VOI),
// *probMyopic* the probability that the agent is myopic,
// *myopiaBoundPrior* the prior over the myopia bound of the agent (conditioned on myopia),
// *probBoundVOI* the probability that the agent has bounded VOI (conditioned on no myopia),
// and *voiBoundPrior* the prior over the VOI bound of the agent (conditioned on bounded VOI)

var inferBeliefDelayAgent = function(observedTrajectories,
				     paramsToUtilityFunction, world, priors) {
  var numTrajectories = observedTrajectories.length;

  var transition = world.transition;
  var observe = world.observe;
  
  var utilityParamsPrior = priors.utilityParamsPrior;
  var beliefPrior = priors.beliefPrior;
  var alphaPrior = priors.alphaPrior;
  var discountPrior = priors.discountPrior;
  var probSophisticated = priors.probSophisticated;
  var probMyopic = priors.probMyopic;
  var myopiaBoundPrior = priors.myopiaBoundPrior;
  var probBoundVOI = priors.probBoundVOI;
  var voiBoundPrior = priors.voiBoundPrior;
  
  var sampleAlongTrajectory = function(index, trajectory, agent, belief){
    if (index >= trajectory.length) {
      return [];
    } else {

      var agentAct = agent.act;
      var agentUpdateBelief = agent.updateBelief;
      
      var state = trajectory[index][0];
      var observedAction = trajectory[index][1];

      var observation = observe(state);
      var nextBelief = agentUpdateBelief(state.manifestState, belief, observation, 0);
      var nextActionERP = agentAct(state.manifestState, nextBelief, 0);
      
      factor(nextActionERP.score(observedAction));
      
      return sampleAlongTrajectory(index + 1, trajectory, agent, nextBelief);
    }
  };

  return Infer({ method: 'enumerate' }, function(){
    var alpha = sample(alphaPrior);
    
    var utilityParams = sample(utilityParamsPrior);
    var utility = paramsToUtilityFunction(utilityParams);
    var agentPrior = sample(beliefPrior);

    var isMyopic = flip(probMyopic);
    var myopiaBound = isMyopic ? sample(myopiaBoundPrior) : 0;

    var hasBoundVOI = isMyopic ? false : flip(probBoundVOI);
    var voiBound = hasBoundVOI ? sample(voiBoundPrior) : 0;

    var discount = sample(discountPrior);
    var noDelays = (discountPrior === 0) && !isMyopic && !hasBoundVOI;

    var sophisticatedOrNaive = (flip(probSophisticated) && !isMyopic && !hasBoundVOI)
	  ? 'sophisticated' : 'naive';

    var params = {
      alpha: alpha,
      utility: utility,
      noDelays: noDelays,
      discount: discount,
      sophisticatedOrNaive: sophisticatedOrNaive,
      myopia: {on: isMyopic, bound: myopiaBound},
      priorBelief: agentPrior,
      boundVOI: {on: hasBoundVOI, bound: voiBound}
    };
    
    var agent = makeBeliefDelayAgent(params, world);

    var factorTrajectory = function(trajectory) {
      return sampleAlongTrajectory(0, trajectory, agent, agentPrior);
    };

    map(factorTrajectory, observedTrajectories);

    return update( _.omit(params, utility), {utilityParams: utilityParams});
  });
};
