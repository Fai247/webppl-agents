var numArms = 2;
var bandit = makeDeterministicBandit(numArms);
var trueLatent = {start: 0, rewards: [10, 1]};
var falseLatent = {start: 0, rewards: [10, 20]};

var agentBelief = Enumerate(function(){
  if (flip(0.8)) {
    return falseLatent;
  } else {
    return trueLatent;
  }
});

var informedBelief = Enumerate(function(){
  return trueLatent;
});

var observedTrajectory = runBandit(numArms, trueLatent, agentBelief, 2);

// note: change console.log to print in codeboxes
console.log('observed trajectory', map( function (state) {
  return state.manifestState.loc;
}, observedTrajectory) );

// infer what the agent knows and whether they like rewards.
// doesn't work, because they could either think that 1 has high rewards and
// like rewards, or they could think that 1 has low rewards but hate rewards.
// without seeing what they do once they know the rewards for 1, no way to tell.
var inferredAgent = Enumerate(function(){
  var priorBelief = uniformDraw([agentBelief, informedBelief]);
  var utilitySign = uniformDraw([-1, 1]);

  var agentUtility = function (state,action) {
    return state.manifestState.loc === 'start' ? 0 :
      utilitySign * state.latentState.rewards[state.manifestState.loc];
  };

  var alpha = 100;
  
  var agent = makeBeliefAgent({utility: agentUtility,
			       alpha: alpha,
			       priorBelief: priorBelief},
			      bandit);
  var newTrajectory = simulateBeliefAgent(observedTrajectory[0], bandit, agent,
					  observedTrajectory.length, 'states');
  condition(_.isEqual(newTrajectory, observedTrajectory));

  return {utilitySign: utilitySign, priorBelief: priorBelief};
});


printERP(inferredAgent);

