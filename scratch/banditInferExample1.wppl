// here, we infer the agent's beliefs about the latent state. our agent will
// pull arm 1, and then arm 0. From this, we can infer that the agent thought
// that arm 1 had higher reward.

// here, we set up the bandit, and define some convenient latent states. note
// that in the true latent state, arm 1 has reward 1, while arm 0 has reward 10
var numArms = 2;
var bandit = makeDeterministicBandit(numArms);
var trueLatent = {start: 0, rewards: [10, 1]};
var falseLatent = {start: 0, rewards: [10, 20]};

// this is the agent's prior: they think that arm 1 probably has reward 20, but
// may only have reward 1.
var agentBelief = Enumerate(function(){
  if (flip(0.8)) {
    return falseLatent;
  } else {
    return trueLatent;
  }
});

// one could imagine that the agent actually knew the true latent state.
var informedBelief = deltaERP(trueLatent);

// here, we generate the trajectory of our agent, which we then print.
var observedTrajectory = runBandit(numArms, trueLatent, agentBelief, 3);
console.log('daniel is cool');
// note: change console.log to print in codeboxes
console.log('observed trajectory', map( function (state) {
  return state.manifestState.loc;
}, observedTrajectory) );


timeit(function(){return 8 + 15;});
ash();

// now, we infer what the agent knows, starting from a 50/50 prior that they
// might start out with informedBelief or agentBelief. we also time the
// inference.
var inferredAgent = function(){
  return printERP(Enumerate(function(){
    var priorBelief = uniformDraw([agentBelief, informedBelief]);

    var agentUtility = function (state,action) {
      return state.manifestState.loc === 'start' ? 0 :
	state.latentState.rewards[state.manifestState.loc];
    };
    
    var alpha = 100;
    
    var agent = makeBeliefAgent({utility: agentUtility,
				 alpha: alpha,
				 priorBelief: priorBelief},
				bandit);
    var newTrajectory = simulateBeliefAgent(observedTrajectory[0], bandit, agent,
					    observedTrajectory.length, 'states');
    condition(_.isEqual(newTrajectory, observedTrajectory));

    return priorBelief;
  }));
};

//timeit(inferredAgent);
