
// Helpers
var utilityToPOMDP = function(utility) {
  return function (state) { 
    return utility(state.manifestState);
  };
};

var utilityTableToFunction = function(utilityTable,world){
  return utilityToPOMDP(makeRestaurantUtilityMDP(world,utilityTable));
};

var nameToPath = restaurantNameToPath;
var pathToName = restaurantPathToName;



// WORLD PARAMS
var gridworldMDP = makeDonutWorld2({big: true, maxTimeAtRestaurant : 2});
var gridworldMDP = restaurantChoiceMDP;
var world = makeGridworldPOMDP(gridworldMDP);

var trueLatentState = {'Donut N': true,
		       'Donut S': true,
		       'Veg': true,
		       'Noodle': true};

var start = {manifestState: restaurantChoiceStart, //timeLeft: 11
	     latentState: trueLatentState};

                          
// the agent's prior - agent knows starting manifest, maybe not latent

var agentPrior = getPriorBeliefGridworld( start.manifestState,
					  function(){
					    return trueLatentState;
					  });

var baseParams = {
  priorBelief : agentPrior,
  alpha : 1000,
  noDelays: false,
  myopia: {on: false, bound: 0},
  boundVOI: {on: false, bound: 0}
};

var restaurantBelief = function(){
  
  var getObservations = function(world, start, pathName){ 
    var temptationUtility = {
      'Donut N' : [10, -10],
      'Donut S' : [10, -10],
      'Veg'   : [-10, 20],
      'Noodle': [-5, -5],
      'timeCost': -.01,
    };
    
    var discount = pathName=='vegDirect' ? 0 : 1;
    var sophisticatedOrNaive = pathName=='naive' ? 'naive' : 'sophisticated';
    
    var params = update(baseParams,{
      utility: utilityTableToFunction(temptationUtility,world),
      discount: discount,
      sophisticatedOrNaive: sophisticatedOrNaive
    });
    
    var agent = makeBeliefDelayAgent(params, world);
    var observedStateAction = simulateBeliefDelayAgent(start, world, agent, 'stateAction');
    var observedLocs = trajectoryToLocations( _.map(observedStateAction,0));
    
    assert.ok( _.isEqual( observedLocs, restaurantNameToPath[pathName]), 
               'generated matches stored');
    return observedStateAction;
  };


  var conditionOnPath = function (agent, world, start, observedPath) {
    var agentAct = agent.act;
    var agentUpdateBelief = agent.updateBelief;
    var priorBelief = agent.params.priorBelief;
    var transition = world.transition;
    var worldObserve = world.observe;
    var observe = getFullObserve(worldObserve);

    var shouldTerminate = function (state) {
      return state.manifestState.terminateAfterAction;
    };

    // TODO why conditon on states and not actions?
    var _conditionOnPath = function(state, priorBelief, action, i) {
      var observation = observe(state);
      var delay = 0;
      var belief = agentUpdateBelief(priorBelief, observation, action, delay);
      var newAction = sample(agentAct(belief, delay));

      if (shouldTerminate(state) || i >= observedPath.length) {
        return 0;
      } else {   
        var nextState = transition(state, newAction);
        condition(_.isEqual(nextState.manifestState.loc, observedPath[i]));
        return _conditionOnPath(nextState, belief, newAction, i+1);
      }
    };
    return _conditionOnPath(start, priorBelief, 'noAction', 1); // is 1 the right start index?
  };

  var getPosterior = function(priorUtilityTable, priorDiscounting, priorAlpha, observedStateAction){
    return Enumerate(function () { 
      var utilityTable = priorUtilityTable();
      var discounting = priorDiscounting();
      var alpha = priorAlpha();
      
      var params = update(baseParams,{
        utility: utilityTableToFunction(utilityTable,world),
        discount: discounting.discount,
        sophisticatedOrNaive: discounting.sophisticatedOrNaive,
        noDelays: discount===0
      });
      
      var agent = makeBeliefDelayAgent(params, world);
      
      conditionOnPath(agent, observedStateAction, world, start);

      var vegMinusDonut = sum(utilityTable['Veg']) - sum(utilityTable['Donut N']);
      var donutNWins = sum(utilityTable['Donut S']) < sum(utilityTable['Donut N']);
      
      return {utility: utilityTable, 
              discount: discounting.discount, 
              alpha: alpha,
              sophisticatedOrNaive: discounting.sophisticatedOrNaive,
              vegMinusDonut: vegMinusDonut,
              donutNWins: donutNWins
             };
    });
  };

  return {getObservations: getObservations, getPosterior: getPosterior};
};
  


var restaurantBelief = restaurantBelief()
var getObservations = restaurantBelief.getObservations;
var obs = getObservations(world,start,'sophisticated');
console.log('obs', _.map(obs,1));
ash();


var timePosterior = function( observedPath ){
  var thunk = function(){return posterior(observedPath)};
  var out = timeit( thunk );
  printERP(out.value);
  console.log( 'runtime in s:', .001 * out.runtimeInMilliseconds);
};

//getPosterior( nameToPath.naive);
getPosterior( nameToPath.sophisticated);

