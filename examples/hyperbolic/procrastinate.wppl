
// discounter_procrastinates

// procrastinates, because it thinks that it will do it later

var world = makeProcrastinationMDP2();

var utilityTable = {reward: 10,
		    waitCost: -0.1,
		    workCost: -1};
var utility = makeProcrastinationUtility2(utilityTable);

var startState = {loc: "wait_state",
		  waitSteps: 0,
		  timeLeft: 10,
		  terminateAfterAction: false};

var params = {utility: utility,
	      alpha: 100,
	      discount: 5,
	      sophisticatedOrNaive: 'naive'};
  
var agent = makeHyperbolicDiscounter(params, world);
var trajectory = simulateHyperbolic(startState, world, agent, 'stateAction');
map(function(stateAction){return [stateAction[0].loc, stateAction[1]];},
    trajectory);


// Construct Procrastinate Problem world 
var deadline = 10;
var world = makeProcrastinationMDP2(deadline);

// Agent's params
var utilityTable = {reward: 10,
		    waitCost: -0.1,
		    workCost: -1};

var params = {utility: makeProcrastinationUtility2(utilityTable),
	      alpha: 1000,
	      discount: null,
	      sophisticatedOrNaive: 'naive'};

var simulatePro = function(discount){
  var agent = makeHyperbolicDiscounter(update(params, {discount: discount}), world);
  var stateActions = simulateHyperbolic(world.startState, world, agent);
  var states = map(first,stateActions);
  return [last(states).loc, stateActions.length];
};

var discounts = range(11);
var lastActionsAndTimes = map(simulatePro, discounts);

// console.log('\n\n Discounts: ' + discounts + '\nLast actions and lengths of trajectories:'
//       + JSON.stringify(lastActionsAndTimes));



// infer_procrastination

// give it prefixes of the actual sequence, see how expected reward and logAlpha varies
var observedStateAction = procrastinateUntilEnd102;
var lastChanceState = secondLast(observedStateAction)[0];


var posterior = function(observedStateAction, optimalModel) {

  var world = makeProcrastinationMDP2();

  
  return Enumerate(function(){
   
    var utilityTable = {reward: uniformDraw([0.5, 2, 3, 4, 5, 6, 7, 8]),
			waitCost: -0.1,
			workCost: -1};
    
    var params = {
      utility: makeProcrastinationUtility2(utilityTable),
      alpha: categorical([0.1, 0.2, 0.2, 0.2, 0.3], [0.1, 1, 10, 100, 1000]),
      discount: optimalModel ? uniformDraw([0, .5, .1, 2, 4]) : 0,
      sophisticatedOrNaive: 'naive'
    };
    
    var agent = makeHyperbolicDiscounter(params, world);
    var act = agent.act;
    
    map(function(stateAction){
      var state = stateAction[0];
      var action = stateAction[1];
      factor( act(state, 0).score([], action) )
    }, observedStateAction);


    return {reward: utilityTable.reward, 
            alpha: params.alpha, 
            discount: params.discount, 
            lastAction: sample( act(lastChanceState, 0) )};
  });
};

var feature = ['reward', 'lastAction', 'alpha', 'discount'];

var inferToTimeIndex = function(timeIndex){
  var optimalPosterior = posterior(observedStateAction.slice(0,timeIndex), false);
  var hyperbolicPosterior = posterior(observedStateAction.slice(0,timeIndex), true);

  var expectations = function(erp){
    return map(function(feature){
      return expectation(getMarginal(erp,feature));
    }, features)
  };
  
  return [expectations(optimalPosterior), expectations(hyperbolicPosterior)];
};

var observedTimeIndexes = range(observedStateAction.length);

var indexToExpectations = map(inferToTimeIndex, observedTimeIndexes);


var expectedRewardsOptimal = map(function(object){return object.optimalReward;},
				 expectations);
var expectedAlphasOptimal = map(function(object){return object.optimalAlpha;},
				expectations);
var expectedRewardsHyperbolic = map(function(object){return object.hyperbolicReward;},
				    expectations);
var expectedAlphasHyperbolic = map(function(object){return object.hyperbolicAlpha;},
				   expectations);
var expectedDiscountsHyperbolic = map(function(object){return object.hyperbolicDiscount;},
                                      expectations);

console.log('Expected reward vs state action pairs observed, optimal');
console.log(observedTimesteps, expectedRewardsOptimal);

console.log('Expected alpha vs state action pairs observed, optimal');
console.log(observedTimesteps, expectedAlphasOptimal);

console.log('Expected reward vs state action pairs observed, hyperbolic');
console.log(observedTimesteps, expectedRewardsHyperbolic);

console.log('Expected alpha vs state action pairs observed, hyperbolic');
console.log(observedTimesteps, expectedAlphasHyperbolic);

console.log('Expected discount vs state action pairs observed, hyperbolic');
console.log(observedTimesteps, expectedDiscountsHyperbolic);
