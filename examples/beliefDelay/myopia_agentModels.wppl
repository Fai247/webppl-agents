// infer_utility_from_no_exploration


var getMarginal = function(dist, key){
  return Infer({ model() {
    return sample(dist)[key];
  }});
};

var getPosterior = function(timeLeft, useOptimalModel) {
  var numArms = 2;
  var armToPrize = {0: 'chocolate',
		    1: 'nothing'};
  var worldAndStart = makeIRLBanditWorldAndStart(numArms, armToPrize, timeLeft);
  var startState = worldAndStart.startState;
  var alternativeLatent = extend(armToPrize, {1: 'champagne'});
  var alternativeStartState = extend(startState, {latentState:
						  alternativeLatent});

  var priorAgentPrior = Delta({ v: Categorical({ ps: [0.7, 0.3],
						 vs: [startState, alternativeStartState] }) });
  
  var priorPrizeToUtility = Infer({ method: 'enumerate' }, function(){
    return {chocolate: uniformDraw(_.range(20).concat(25)),
	    nothing: 0,
	    champagne: 20};
  });
  
  var priorMyopia =  useOptimalModel ? deltaDist({on:false, bound:0}) :
      Infer({ method: 'enumerate' }, function(){
        return {on: true, 
                bound: categorical([.4, .2, .1, .1, .1, .1], 
                                   [1, 2, 3, 4, 6, 10])};
      });
  
  var prior = {priorAgentPrior: priorAgentPrior,
	       priorPrizeToUtility: priorPrizeToUtility,
               priorMyopia: priorMyopia};

  var baseAgentParams = {alpha: 1000,
			 myopia: {on: false, bound:0},
			 boundVOI: {on: false, bound: 0},
			 sophisticatedOrNaive: 'naive',
			 discount: 0, //agentType === 'hyperbolic' ? 1 : 0,
			 noDelays: useOptimalModel};                      

  var observedStateAction = [[startState, 0]];

  var outputDist = inferIRLBandit(worldAndStart, baseAgentParams, prior,
				 observedStateAction, 'offPolicy', 0, 'beliefDelay');
  
  var marginalChocolate = Infer({ method: 'enumerate' }, function(){
    return sample(outputDist).prizeToUtility.chocolate;
  });
  
  return [expectation(marginalChocolate), 
          expectation(getMarginal(outputDist,'myopiaBound'))]
};

var timeHorizonValues = _.range(10).slice(2);

var optimalExpectations = map(function(t){return getPosterior(t, true);},
			      timeHorizonValues);
var possiblyMyopicExpectations = map(function(t){return getPosterior(t, false);},
			             timeHorizonValues);

var print = function(x){return console.log(x);};

print('Prior expected utility for arm0 (chocolate): ' + listMean(_.range(20).concat(25)) );

print('Inferred Utility for arm0 (chocolate) for Optimal Model as timeHorizon increases');
console.log(timeHorizonValues, map(first, optimalExpectations));

print('Inferred Utility for arm0 (chocolate) for Possibly Greedy Model as timeHorizon increases');
console.log(timeHorizonValues, map(first, possiblyMyopicExpectations));

print('Inferred Myopic Bound for Possibly Greedy Model as timeHorizon increases');
console.log(timeHorizonValues, map(second, possiblyMyopicExpectations));
