
// Helper functions for Bandits:
///fold:

// HELPERS FOR CONSTRUCTING AGENT

var prizeToUtility = {start: 0, 0: 0, 1: 1}; // Utility linear in the reward
var utility = makeStochasticBanditUtility(prizeToUtility);

var baseParams = {
  utility: utility,
  alpha: 1000,
  fastUpdateBelief: false,
  noDelays: false,
  sophisticatedOrNaive: 'naive',
  myopia: {on: false, bound: 0},
  discount: 0
};

var getParams = function(agentPrior, options){
  var params = update(baseParams, {priorBelief: agentPrior});
  return update(params, options);
};

var getAgentPrior = function(totalTime, priorArm0, priorArm1){
  return Infer({ method: 'enumerate' }, function(){
    var latentState = {0: priorArm0(), 1: priorArm1()};
    return buildStochasticBanditStartState(totalTime, latentState);
  });
};

// HELPERS FOR CONSTRUCTING WORLD

// Possible distributions for arms
var probably1ERP = Categorical({ ps: [0.4, 0.6], vs: ['0', '1'] });
var probably0ERP = Categorical({ ps: [0.6, 0.4], vs: ['0', '1'] });


// Construct Bandit POMDP
var getBanditWorld = function(totalTime){
  var numberArms = 2;
  var world = makeStochasticBanditWorld(numberArms);
  
  // Arm to distribution
  var trueLatent = {0: probably0ERP,
		    1: probably1ERP};
  
  var start = buildStochasticBanditStartState(totalTime, trueLatent);

  return {world:world, start:start};
};

// Get score for a single episode of bandits
var score = function(out){
  var total =  sum( map(function(x){return prizeToUtility[x.observation.manifestState.loc];}, out) );
  return total / out.length;
};
///


// Agent prior on arm rewards

// Possible distributions for arms
var probably0ERP = Categorical({ ps: [0.6, 0.4], vs: ['0', '1'] });
var probably1ERP = Categorical({ ps: [0.4, 0.6], vs: ['0', '1'] });

// True latentState:
// arm0 is probably0ERP, arm1 is probably1ERP (and so is better)

// Agent prior on arms: arm1 (better arm) has higher EV
var priorArm0 = function(){
  return categorical([0.5, 0.5], [probably1ERP, probably0ERP])
};
var priorArm1 = function(){
  return categorical([0.6, 0.4], [probably1ERP, probably0ERP])
};


var runAgents = function(totalTime){
  
  // Construct world and agents
  var banditWorld = getBanditWorld(totalTime);
  var world = banditWorld.world;
  var start = banditWorld.start;
  
  var agentPrior = getAgentPrior(totalTime, priorArm0, priorArm1);
  var optimalAgent = makeBeliefAgent(getParams(agentPrior), world);

  var myopicParams = getParams(agentPrior, {boundVOI: {on:true, bound:1}});
  var myopicAgent = makeBeliefDelayAgent(myopicParams, world);

  // Get average score across totalTime for both agents
  var runOptimal = function(){
    return score(simulateBeliefAgent(start, world, optimalAgent, 
                                     'stateObservationAction'));
  };
  
  var runMyopic = function(){
    return score(simulateBeliefDelayAgent(start, world, myopicAgent, 
                                          'stateObservationAction'));
  };

  return [ timeit(runOptimal), timeit(runMyopic) ];
};


// Show performance is similar for Optimal and Myopic
var runAgents5 = function(){
  var out = runAgents(5);
  return [out[0].value, out[1].value];
};

var repeat10 = repeat(10, runAgents5);
var optimalScore = map(first, repeat10);
var myopicScore = map(second, repeat10);

map( function(name){
  var score = name=='Optimal' ? optimalScore : myopicScore;
  console.log( name + ' Agent mean score for each of 10 episodes of Bandits: \n' + 
               JSON.stringify(score) + '\n Overall Mean: ' + listMean(score) );
}, ['Optimal', 'Myopic'] );



// Show scaling in number of trials is better for Myopic

var totalTimeValues = range(9).slice(2);
var allTimes = map( runAgents, totalTimeValues )
var optimalTimes = map( function(time){
  return (1/1000)*time[0].runtimeInMilliseconds;
}, allTimes );
var myopicTimes = map( function(time){
  return (1/1000)*time[1].runtimeInMilliseconds;
}, allTimes );

console.log('\n Optimal Agent runtime in sec:  ' + totalTimeValues + '\n' + optimalTimes);
//viz.line(totalTimeValues, optimalTimes);

console.log('Myopic Agent runtime in sec:  ' + totalTimeValues + '\n' + myopicTimes);
//viz.line(totalTimeValues, myopicTimes);

ash();




// more rigorous test that optimal and myopic agent are equivalent for 2-arm

var testEquivalence = function(){

  var worldObserve = world.observe;
  var observe = getFullObserve(worldObserve);
  var transition = world.transition;


  var nearlyEqualActionERPs = function(erp1, erp2) {
    var nearlyEqual = function(float1, float2) {
      return Math.abs(float1 - float2) < 0.05;
    };
    return nearlyEqual(erp1.score(0), erp2.score(0))
      && nearlyEqual(erp1.score(1), erp2.score(1));
  };

  // it's important that we simulate the two agents such that they get the same
  // prizes when pulling the same arms, so that we can check if their
  // actions are the same. We could not ensure this by simply simulating one agent
  // and then the other.
  var sampleTwoSequences = function(states, priorBeliefs, actions) {
    ///fold:
    var optimalState = states[0];
    var optimalPriorBelief = priorBeliefs[0];
    var optimalAction = actions[0];

    var optimalAct = optimalAgent.act;
    var optimalUpdateBelief = optimalAgent.updateBelief;
    
    var myopicState = states[1];
    var myopicPriorBelief = priorBeliefs[1];
    var myopicAction = actions[1];
    
    var myopicAct = myopicAgent.act;
    var myopicUpdateBelief = myopicAgent.updateBelief;

    var optimalObservation = observe(optimalState);
    var myopicObservation = observe(myopicState);
    
    var delay = 0;
    var newMyopicBelief = myopicUpdateBelief(myopicPriorBelief, myopicObservation,
					     myopicAction, delay);
    var newOptimalBelief = optimalUpdateBelief(optimalPriorBelief,
					       optimalObservation, optimalAction);

    var newMyopicActionERP = myopicAct(newMyopicBelief, delay);
    var newOptimalActionERP = optimalAct(newOptimalBelief);

    var newMyopicAction = sample(newMyopicActionERP);
    // if ERPs over actions are almost the same, have the agents pick the same
    // action
    var newOptimalAction = nearlyEqualActionERPs(newMyopicActionERP,
					         newOptimalActionERP)
	? newMyopicAction : sample(newOptimalActionERP);

    var optimalLocAction = [optimalState.manifestState.loc, newOptimalAction];
    var myopicLocAction = [myopicState.manifestState.loc, newMyopicAction];

    var output = [optimalLocAction, myopicLocAction];

    if (optimalState.manifestState.terminateAfterAction) {
      return output;
    } else {
      var nextPriorBeliefs = [newOptimalBelief, newMyopicBelief];
      var nextActions = [newOptimalAction, newMyopicAction];
      if (_.isEqual(optimalState, myopicState) && _.isEqual(newOptimalAction,
							    newMyopicAction)) {
        // if actions are the same, transition to the same state
        var nextState = transition(optimalState, newOptimalAction);
        var nextStates = [nextState, nextState];
        var recurse = sampleTwoSequences(nextStates, nextPriorBeliefs,
			                 nextActions);
        return [optimalLocAction.concat(recurse[0]),
	        myopicLocAction.concat(recurse[1])];
      } else {
        var nextOptimalState = transition(optimalState, newOptimalAction);
        var nextMyopicState = transition(myopicState, newMyopicAction);
        var nextStates = [nextOptimalState, nextMyopicState];
        var recurse = sampleTwoSequences(nextStates, nextPriorBeliefs,
			                 nextActions);
        return [optimalLocAction.concat(recurse[0]),
	        myopicLocAction.concat(recurse[1])];
      }
    }
    ///	
  };

  var startAction = 'noAction';

  var trajectories = sampleTwoSequences([startState, startState], [prior, prior],
                                        [startAction, startAction]);
  var length = trajectories[0].length;


  console.log('Trajectory of optimal agent: ' + trajectories[0].slice(1, length - 1));
  console.log('Trajectory of myopic agent: ' + trajectories[1].slice(1, length - 1));
};


null;
