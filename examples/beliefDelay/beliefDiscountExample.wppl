// We know that the agent prefers noodle to veg to donut, but are unsure about
// its belief about the latent state and its discounting behaviour. We see the
// agent go straight to veg. We infer that the agent must think that noodle is
// closed and that the agent probably has a low discount (although the agent
// could have a very high discount and be subject to softmax noise), but cannot
// infer anything about the agent's sophistication.

// WORLD PARAMS

var world = getBigDonutWorld();
var feature = world.feature;

// possible latent states

var noodleClosedLatent = {'Donut N': true,
			  'Donut S': true,
			  'Veg': true,
			  'Noodle': false};

var everythingOpenLatent = {'Donut N': true,
			    'Donut S': true,
			    'Veg': true,
			    'Noodle': true};

var trueLatentState = everythingOpenLatent;

// start state
var startState = {manifestState: {loc: [3,1],
				  terminateAfterAction: false,
				  timeLeft: 10},
		  latentState: trueLatentState};

// POTENTIAL AGENT PARAMS

// agent's utility function
var agentUtilityTable = {'Donut N': 1,
			 'Donut S': 1,
			 Veg: 2,
			 Noodle: 3,
			 timeCost : -0.1};

var agentUtility = tableToUtilityFunction(agentUtilityTable, feature);

// possible priors that the agent could have
var uninformedLatentStateSampler = function(){
  return flip(.9) ? noodleClosedLatent : trueLatentState;
};

// make this library function? is getPriorBeliefGridworld actually specialised
// to gridworld?
var makeInformedBelief = function(state) {
  return getPriorBeliefGridworld( state.manifestState,
				  function(){return state.latentState;});
};

var uninformedPriorBelief = getPriorBeliefGridworld( startState.manifestState,
					       uninformedLatentStateSampler);

var informedPriorBelief = makeInformedBelief(startState);

// priors over agent params

var agentProbNoodleOpenSampler = function() {
  return uniformDraw([0.1, 1]);
};

var agentDiscountSampler = function() {
  return uniformDraw([0, 1, 100]);
};

var agentSophisticationSampler = function() {
  return uniformDraw(['sophisticated', 'naive']);
};

// see what the agent does with the parameters we want to infer to check if we're right

// var agent = makeBeliefDelayAgent({
//   priorBelief: uninformedPriorBelief,
//   utility : agentUtility,
//   // utility : pomdpUtilityFromManifestUtility(makeDonutUtility(world, donutUtilityTable)),
//   alpha : 100,
//   noDelays: false,
//   discount: 0,
//   sophisticatedOrNaive: 'sophisticated',
//   myopia: {on: false, bound: 0},
//   boundVOI: {on: false, bound: 0}
// }, world);


// var trajectory = simulateBeliefDelayAgent(startState, world, agent, 'states');

// var locs = map(function(state) {return state.manifestState.loc;}, trajectory);

// console.log(locs);
// ash();


// PATH WE CONDITION ON

var path = [[3,1], [3,2], [3,3], [3,4], [3,5], [3,6], [4,6], [4,7]];

// INFERENCE

var conditionOnPath = function (agent, path, world, startState) {
  var agentAct = agent.act;
  var agentUpdateBelief = agent.updateBelief;
  var priorBelief = agent.params.priorBelief;
  var transition = world.transition;
  var worldObserve = world.observe;
  var observe = getFullObserve(worldObserve);

  var shouldTerminate = function (manifestState) {
    return manifestState.terminateAfterAction;
  };
  
  var _conditionOnPath = function(state, priorBelief, action, i) {
    var observation = observe(state);

    var delay = 0;

    var belief = agentUpdateBelief(priorBelief, observation, action, delay);

    var newAction = sample(agentAct(belief, delay));

    if (shouldTerminate(state.manifestState) || i >= path.length) {
      return 0;
    } else {   
      var nextState = transition(state, newAction);

      condition(_.isEqual(nextState.manifestState.loc, path[i]));

      return _conditionOnPath(nextState, belief, newAction, i+1);
    }
  };

  var startAction = 'noAction';

  return _conditionOnPath(startState, priorBelief, startAction, 1);
};

var posterior = function() {

  var posteriorDist = Infer({ method: 'enumerate' }, function () { 

    var agentProbNoodleOpen = agentProbNoodleOpenSampler();
    var agentPrior = agentProbNoodleOpen === 1 ? informedPriorBelief
	  : uninformedPriorBelief;
    var discount = agentDiscountSampler();
    var sophistication = agentSophisticationSampler();
    var agent = makeBeliefDelayAgent({
      priorBelief : agentPrior,
      utility: agentUtility,
      alpha : 100,
      noDelays: discount===0,
      discount: discount,
      sophisticatedOrNaive: sophistication,
      myopia: {on: false, bound: 0},
      boundVOI: {on: false, bound: 0}
    }, world);

    conditionOnPath(agent, path, world, startState);

    return {
      probNoodleOpen: agentProbNoodleOpen,
      discount: discount,
      sophistication: sophistication
    };

  });

  printDist(posteriorDist);
};

// printDist(posterior);

timeit(posterior);

